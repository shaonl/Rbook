
@article{mlinaric_dealing_2017,
	title = {Dealing with the positive publication bias: {Why} you should really publish your negative results},
	volume = {27},
	issn = {1330-0962},
	shorttitle = {Dealing with the positive publication bias},
	url = {https://hrcak.srce.hr/index.php?show=clanak&id_clanak_jezik=276521},
	doi = {10.11613/BM.2017.030201},
	abstract = {Studies with positive results are greatly more represented in literature than studies with negative results, producing so-called publication bias. This review aims to discuss occurring problems around negative results and to emphasize...},
	language = {en},
	number = {3},
	urldate = {2020-12-15},
	journal = {Biochemia Medica},
	author = {Mlinarić, Ana and Horvat, Martina and Šupak Smolčić, Vesna},
	month = oct,
	year = {2017},
	note = {Publisher: Medicinska naklada},
	pages = {447--452},
	file = {Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\Z68FG3R5\\Mlinarić et al. - 2017 - Dealing with the positive publication bias Why yo.pdf:application/pdf;Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\E3HBM6HM\\Mlinarić et al. - 2017 - Dealing with the positive publication bias Why yo.pdf:application/pdf;Snapshot:C\:\\Users\\shaonl\\Zotero\\storage\\9RGDVIKP\\index.html:text/html;Snapshot:C\:\\Users\\shaonl\\Zotero\\storage\\MF7ZN467\\index.html:text/html},
}

@article{ozier_replication_2019,
	title = {Replication {Redux}: {The} {Reproducibility} {Crisis} and the {Case} of {Deworming}},
	volume = {8835},
	url = {http://documents.worldbank.org/curated/en/118271556632669793/pdf/Replication-Redux-The-Reproducibility-Crisis-and-the-Case-of-Deworming.pdf},
	urldate = {2019-08-08},
	journal = {Policy  Research  Working  Paper},
	author = {Ozier, Owen},
	month = apr,
	year = {2019},
}

@book{cooper_handbook_2019,
	title = {The {Handbook} of {Research} {Synthesis} and {Meta}-{Analysis}, {Second} {Edition} {\textbar} {RSF}},
	url = {https://www.russellsage.org/publications/handbook-research-synthesis-and-meta-analysis-second-edition},
	urldate = {2019-08-03},
	author = {Cooper, Harris and Hedges, Larry V. and Valentine, Jeffrey},
	month = aug,
	year = {2019},
}

@article{simera_transparent_2010,
	title = {Transparent and accurate reporting increases reliability, utility, and impact of your research: reporting guidelines and the {EQUATOR} {Network}},
	volume = {8},
	issn = {1741-7015},
	shorttitle = {Transparent and accurate reporting increases reliability, utility, and impact of your research},
	url = {https://doi.org/10.1186/1741-7015-8-24},
	doi = {10.1186/1741-7015-8-24},
	abstract = {Although current electronic methods of scientific publishing offer increased opportunities for publishing all research studies and describing them in sufficient detail, health research literature still suffers from many shortcomings. These shortcomings seriously undermine the value and utility of the literature and waste scarce resources invested in the research. In recent years there have been several positive steps aimed at improving this situation, such as a strengthening of journals' policies on research publication and the wide requirement to register clinical trials.},
	number = {1},
	urldate = {2019-08-03},
	journal = {BMC Medicine},
	author = {Simera, Iveta and Moher, David and Hirst, Allison and Hoey, John and Schulz, Kenneth F. and Altman, Douglas G.},
	month = apr,
	year = {2010},
	note = {Number: 1},
	pages = {24},
}

@misc{grant_proposed_2017,
	title = {Proposed {Reporting} {Items} for {Protocols} of {Social} {Intervention} {Trials}},
	url = {https://osf.io/x6mkb/},
	publisher = {Open Society Framework},
	author = {Grant, Sean},
	year = {2017},
	keywords = {Reporting guidelines, RT2},
}

@article{grant_consort-spi_2018,
	title = {{CONSORT}-{SPI} 2018 {Explanation} and {Elaboration}: guidance for reporting social and psychological intervention trials},
	volume = {19},
	issn = {1745-6215},
	shorttitle = {{CONSORT}-{SPI} 2018 {Explanation} and {Elaboration}},
	url = {https://doi.org/10.1186/s13063-018-2735-z},
	doi = {10.1186/s13063-018-2735-z},
	abstract = {The CONSORT (Consolidated Standards of Reporting Trials) Statement was developed to help biomedical researchers report randomised controlled trials (RCTs) transparently. We have developed an extension to the CONSORT 2010 Statement for social and psychological interventions (CONSORT-SPI 2018) to help behavioural and social scientists report these studies transparently.},
	number = {1},
	urldate = {2019-08-19},
	journal = {Trials},
	author = {Grant, Sean and Mayo-Wilson, Evan and Montgomery, Paul and Macdonald, Geraldine and Michie, Susan and Hopewell, Sally and Moher, David and Aber, J. Lawrence and Altman, Doug and Bhui, Kamaldeep and Booth, Andrew and Clark, David and Craig, Peter and Eisner, Manuel and Fraser, Mark W. and Gardner, Frances and Grant, Sean and Hedges, Larry and Hollon, Steve and Hopewell, Sally and Kaplan, Robert and Kaufmann, Peter and Konstantopoulos, Spyros and Macdonald, Geraldine and Mayo-Wilson, Evan and McLeroy, Kenneth and Michie, Susan and Mittman, Brian and Moher, David and Montgomery, Paul and Nezu, Arthur and Sherman, Lawrence and Sonuga-Barke, Edmund and Thomas, James and VandenBos, Gary and Waters, Elizabeth and West, Robert and Yaffe, Joanne and , on behalf of the CONSORT-SPI Group},
	month = jul,
	year = {2018},
	note = {Number: 1},
	pages = {406},
}

@article{burlig_improving_2017,
	title = {Improving transparency in observational social science research: {A} pre-analysis plan approach},
	shorttitle = {Improving transparency in observational social science research},
	url = {https://osf.io/preprints/bitss/qemkz/},
	doi = {10.17605/OSF.IO/QEMKZ},
	abstract = {Social science research has undergone a credibility revolution, but these gains are at risk due to problematic research practices. Existing research on transparency has centered around randomized controlled trials, which constitute only a small fraction of research in economics. In this paper, I discuss three scenarios in which study preregistration can be credibly applied in non-experimental settings: cases where researchers collect their own data; prospective studies; and research using restricted-access data. Finally, I outline suggested contents for observational pre-analysis plans, and highlight where these plans should deviate from those designed for experimental research.},
	urldate = {2017-11-21},
	journal = {MetaArXiv},
	author = {Burlig, Fiona},
	month = oct,
	year = {2017},
}

@article{king_replication_1995,
	title = {Replication, {Replication}},
	volume = {28},
	url = {https://gking.harvard.edu/files/gking/files/replication.pdf},
	journal = {PS: Political Science and Politics},
	author = {King, Gary},
	year = {1995},
	pages = {444--452},
}

@article{chang_is_2018,
	title = {Is {Economics} {Research} {Replicable}? {Sixty} {Published} {Papers} {From} {Thirteen} {Journals} {Say} “{Often} {Not}”},
	volume = {7},
	issn = {2164-5744, 2164-5760},
	shorttitle = {Is {Economics} {Research} {Replicable}?},
	url = {https://www.nowpublishers.com/article/Details/CFR-0053},
	doi = {10.1561/104.00000053},
	abstract = {Is Economics Research Replicable? Sixty Published Papers From Thirteen Journals Say “Often Not”},
	language = {English},
	urldate = {2020-02-24},
	journal = {Critical Finance Review},
	author = {Chang, Andrew C. and Li, Phillip},
	month = oct,
	year = {2018},
}

@article{plemmons_randomized_2020,
	title = {A randomized trial of a lab-embedded discourse intervention to improve research ethics},
	volume = {117},
	copyright = {Copyright © 2020 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by-nc-nd/4.0/This open access article is distributed under Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND).},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/117/3/1389},
	doi = {10.1073/pnas.1917848117},
	abstract = {We report a randomized trial of a research ethics training intervention designed to enhance ethics communication in university science and engineering laboratories, focusing specifically on authorship and data management. The intervention is a project-based research ethics curriculum that was designed to enhance the ability of science and engineering research laboratory members to engage in reason giving and interpersonal communication necessary for ethical practice. The randomized trial was fielded in active faculty-led laboratories at two US research-intensive institutions. Here, we show that laboratory members perceived improvements in the quality of discourse on research ethics within their laboratories and enhanced awareness of the relevance and reasons for that discourse for their work as measured by a survey administered over 4 mo after the intervention. This training represents a paradigm shift compared with more typical module-based or classroom ethics instruction that is divorced from the everyday workflow and practices within laboratories and is designed to cultivate a campus culture of ethical science and engineering research in the very work settings where laboratory members interact.},
	language = {en},
	number = {3},
	urldate = {2020-02-26},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Plemmons, Dena K. and Baranski, Erica N. and Harp, Kyle and Lo, David D. and Soderberg, Courtney K. and Errington, Timothy M. and Nosek, Brian A. and Esterling, Kevin M.},
	month = jan,
	year = {2020},
	pmid = {31919283},
	note = {Number: 3},
	keywords = {randomized trial, authorship, data management, research ethics},
	pages = {1389--1394},
}

@article{munafo_manifesto_2017,
	title = {A manifesto for reproducible science},
	volume = {1},
	copyright = {2017 Macmillan Publishers Limited},
	issn = {2397-3374},
	url = {http://www.nature.com/articles/s41562-016-0021},
	doi = {10.1038/s41562-016-0021},
	abstract = {Improving the reliability and efficiency of scientific research will increase the credibility of the published scientific literature and accelerate discovery. Here we argue for the adoption of measures to optimize key elements of the scientific process: methods, reporting and dissemination, reproducibility, evaluation and incentives. There is some evidence from both simulations and empirical studies supporting the likely effectiveness of these measures, but their broad adoption by researchers, institutions, funders and journals will require iterative evaluation and improvement. We discuss the goals of these measures, and how they can be implemented, in the hope that this will facilitate action toward improving the transparency, reproducibility and efficiency of scientific research.},
	language = {en},
	number = {1},
	urldate = {2020-11-02},
	journal = {Nature Human Behaviour},
	author = {Munafò, Marcus R. and Nosek, Brian A. and Bishop, Dorothy V. M. and Button, Katherine S. and Chambers, Christopher D. and Percie du Sert, Nathalie and Simonsohn, Uri and Wagenmakers, Eric-Jan and Ware, Jennifer J. and Ioannidis, John P. A.},
	month = jan,
	year = {2017},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {1--9},
	file = {Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\ICQBCWTG\\Munafò et al. - 2017 - A manifesto for reproducible science.pdf:application/pdf;Snapshot:C\:\\Users\\shaonl\\Zotero\\storage\\H3UTNSQ9\\s41562-016-0021.html:text/html},
}

@article{francis_too_2012,
	title = {Too good to be true: {Publication} bias in two prominent studies from experimental psychology},
	volume = {19},
	issn = {1531-5320},
	shorttitle = {Too good to be true},
	url = {https://doi.org/10.3758/s13423-012-0227-9},
	doi = {10.3758/s13423-012-0227-9},
	abstract = {Empirical replication has long been considered the final arbiter of phenomena in science, but replication is undermined when there is evidence for publication bias. Evidence for publication bias in a set of experiments can be found when the observed number of rejections of the null hypothesis exceeds the expected number of rejections. Application of this test reveals evidence of publication bias in two prominent investigations from experimental psychology that have purported to reveal evidence of extrasensory perception and to indicate severe limitations of the scientific method. The presence of publication bias suggests that those investigations cannot be taken as proper scientific studies of such phenomena, because critical data are not available to the field. Publication bias could partly be avoided if experimental psychologists started using Bayesian data analysis techniques.},
	language = {en},
	number = {2},
	urldate = {2020-10-16},
	journal = {Psychonomic Bulletin \& Review},
	author = {Francis, Gregory},
	month = apr,
	year = {2012},
	pages = {151--156},
	file = {Springer Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\MEU8M8RS\\Francis - 2012 - Too good to be true Publication bias in two promi.pdf:application/pdf},
}

@article{simonsohn_p-curve_2013,
	title = {P-{Curve}: {A} {Key} to the {File} {Drawer}},
	shorttitle = {P-{Curve}},
	url = {https://papers.ssrn.com/abstract=2256237},
	abstract = {Because scientists tend to report only studies (publication bias) or analyses (p-hacking) that “work”, readers must ask, “Are these effects true, or do they merely reflect selective reporting?” We introduce p-curve as a way to answer this question. P-curve is the distribution of statistically significant p-values for a set of studies (ps {\textless} .05). Because only true effects are expected to generate right-skewed p-curves – containing more low (.01s) than high (.04s) significant p-values – only right-skewed p-curves are diagnostic of evidential value. By telling us whether we can rule out selective reporting as the sole explanation for a set of findings, p-curve offers a solution to the age-old inferential problems caused by file-drawers of failed studies and analyses.},
	language = {en},
	urldate = {2019-08-02},
	journal = {Journal of Experimental Psychology},
	author = {Simonsohn, Uri and Nelson, Leif D. and Simmons, Joseph P.},
	month = apr,
	year = {2013},
	keywords = {Psychology, decision making, publication bias, file drawer, Hypothesis Testing, judgment, p-hacking, science, Scientific Communication, statistics, Statistics},
}

@article{klein_investigating_2014,
	title = {Investigating {Variation} in {Replicability}},
	volume = {45},
	issn = {1864-9335},
	url = {https://econtent.hogrefe.com/doi/10.1027/1864-9335/a000178},
	doi = {10.1027/1864-9335/a000178},
	abstract = {Although replication is a central tenet of science, direct replications are rare in psychology. This research tested variation in the replicability of 13 classic and contemporary effects across 36 independent samples totaling 6,344 participants. In the aggregate, 10 effects replicated consistently. One effect – imagined contact reducing prejudice – showed weak support for replicability. And two effects – flag priming influencing conservatism and currency priming influencing system justification – did not replicate. We compared whether the conditions such as lab versus online or US versus international sample predicted effect magnitudes. By and large they did not. The results of this small sample of effects suggest that replicability is more dependent on the effect itself than on the sample and setting used to investigate the effect.},
	number = {3},
	urldate = {2020-09-18},
	journal = {Social Psychology},
	author = {Klein, Richard A. and Ratliff, Kate A. and Vianello, Michelangelo and Adams, Reginald B. and Bahník, Štěpán and Bernstein, Michael J. and Bocian, Konrad and Brandt, Mark J. and Brooks, Beach and Brumbaugh, Claudia Chloe and Cemalcilar, Zeynep and Chandler, Jesse and Cheong, Winnee and Davis, William E. and Devos, Thierry and Eisner, Matthew and Frankowska, Natalia and Furrow, David and Galliani, Elisa Maria and Hasselman, Fred and Hicks, Joshua A. and Hovermale, James F. and Hunt, S. Jane and Huntsinger, Jeffrey R. and IJzerman, Hans and John, Melissa-Sue and Joy-Gaba, Jennifer A. and Barry Kappes, Heather and Krueger, Lacy E. and Kurtz, Jaime and Levitan, Carmel A. and Mallett, Robyn K. and Morris, Wendy L. and Nelson, Anthony J. and Nier, Jason A. and Packard, Grant and Pilati, Ronaldo and Rutchick, Abraham M. and Schmidt, Kathleen and Skorinko, Jeanine L. and Smith, Robert and Steiner, Troy G. and Storbeck, Justin and Van Swol, Lyn M. and Thompson, Donna and van ‘t Veer, A. E. and Ann Vaughn, Leigh and Vranka, Marek and Wichman, Aaron L. and Woodzicka, Julie A. and Nosek, Brian A.},
	month = jan,
	year = {2014},
	note = {Publisher: Hogrefe Publishing},
	pages = {142--152},
	file = {Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\IZFPQ3RW\\Klein et al. - 2014 - Investigating Variation in Replicability.pdf:application/pdf;Snapshot:C\:\\Users\\shaonl\\Zotero\\storage\\JRM6774W\\a000178.html:text/html},
}

@article{ioannidis_why_2005,
	title = {Why {Most} {Published} {Research} {Findings} {Are} {False}},
	volume = {2},
	issn = {1549-1676},
	url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124},
	doi = {10.1371/journal.pmed.0020124},
	abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
	language = {en},
	number = {8},
	urldate = {2020-09-18},
	journal = {PLOS Medicine},
	author = {Ioannidis, John P. A.},
	month = aug,
	year = {2005},
	note = {Publisher: Public Library of Science},
	keywords = {Schizophrenia, Meta-analysis, Randomized controlled trials, Cancer risk factors, Clinical research design, Finance, Genetic epidemiology, Genetics of disease, Metaanalysis, Research design},
	pages = {e124},
	file = {Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\9NS8NJXD\\Ioannidis - 2005 - Why Most Published Research Findings Are False.pdf:application/pdf;Snapshot:C\:\\Users\\shaonl\\Zotero\\storage\\LBZDLL49\\article.html:text/html},
}

@article{goodman_what_2016,
	title = {What does research reproducibility mean?},
	volume = {8},
	copyright = {Copyright © 2016, American Association for the Advancement of Science},
	issn = {1946-6234, 1946-6242},
	url = {https://stm.sciencemag.org/content/8/341/341ps12},
	doi = {10.1126/scitranslmed.aaf5027},
	abstract = {The language and conceptual framework of “research reproducibility” are nonstandard and unsettled across the sciences. In this Perspective, we review an array of explicit and implicit definitions of reproducibility and related terminology, and discuss how to avoid potential misunderstandings when these terms are used as a surrogate for “truth.”
The language and conceptual framework of “research reproducibility” are nonstandard and unsettled across the sciences.
The language and conceptual framework of “research reproducibility” are nonstandard and unsettled across the sciences.},
	language = {en},
	number = {341},
	urldate = {2020-09-18},
	journal = {Science Translational Medicine},
	author = {Goodman, Steven N. and Fanelli, Daniele and Ioannidis, John P. A.},
	month = jun,
	year = {2016},
	pmid = {27252173},
	note = {Publisher: American Association for the Advancement of Science
Section: Perspective},
	pages = {341ps12--341ps12},
	file = {Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\76TLAYG9\\Goodman et al. - 2016 - What does research reproducibility mean.pdf:application/pdf;Snapshot:C\:\\Users\\shaonl\\Zotero\\storage\\IVIR2CYS\\341ps12.html:text/html},
}

@article{casey_reshaping_2012,
	title = {Reshaping {Institutions}: {Evidence} on {Aid} {Impacts} {Using} a {Preanalysis} {Plan}*},
	volume = {127},
	issn = {0033-5533},
	shorttitle = {Reshaping {Institutions}},
	url = {https://www.povertyactionlab.org/sites/default/files/publications/45_reshaping%20institutions%20QJE.pdf},
	doi = {10.1093/qje/qje027},
	abstract = {Abstract.   Despite their importance, there is limited evidence on how institutions can be strengthened. Evaluating the effects of specific reforms is complicat},
	language = {en},
	number = {4},
	urldate = {2019-08-03},
	journal = {The Quarterly Journal of Economics},
	author = {Casey, Katherine and Glennerster, Rachel and Miguel, Edward},
	month = nov,
	year = {2012},
	note = {Number: 4},
	pages = {1755--1812},
}

@article{benjamin_redefine_2018,
	title = {Redefine statistical significance},
	volume = {2},
	copyright = {2017 The Author(s)},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-017-0189-z},
	doi = {10.1038/s41562-017-0189-z},
	abstract = {We propose to change the default P-value threshold for statistical significance from 0.05 to 0.005 for claims of new discoveries.},
	language = {en},
	number = {1},
	urldate = {2020-09-18},
	journal = {Nature Human Behaviour},
	author = {Benjamin, Daniel J. and Berger, James O. and Johannesson, Magnus and Nosek, Brian A. and Wagenmakers, E.-J. and Berk, Richard and Bollen, Kenneth A. and Brembs, Björn and Brown, Lawrence and Camerer, Colin and Cesarini, David and Chambers, Christopher D. and Clyde, Merlise and Cook, Thomas D. and De Boeck, Paul and Dienes, Zoltan and Dreber, Anna and Easwaran, Kenny and Efferson, Charles and Fehr, Ernst and Fidler, Fiona and Field, Andy P. and Forster, Malcolm and George, Edward I. and Gonzalez, Richard and Goodman, Steven and Green, Edwin and Green, Donald P. and Greenwald, Anthony G. and Hadfield, Jarrod D. and Hedges, Larry V. and Held, Leonhard and Hua Ho, Teck and Hoijtink, Herbert and Hruschka, Daniel J. and Imai, Kosuke and Imbens, Guido and Ioannidis, John P. A. and Jeon, Minjeong and Jones, James Holland and Kirchler, Michael and Laibson, David and List, John and Little, Roderick and Lupia, Arthur and Machery, Edouard and Maxwell, Scott E. and McCarthy, Michael and Moore, Don A. and Morgan, Stephen L. and Munafó, Marcus and Nakagawa, Shinichi and Nyhan, Brendan and Parker, Timothy H. and Pericchi, Luis and Perugini, Marco and Rouder, Jeff and Rousseau, Judith and Savalei, Victoria and Schönbrodt, Felix D. and Sellke, Thomas and Sinclair, Betsy and Tingley, Dustin and Van Zandt, Trisha and Vazire, Simine and Watts, Duncan J. and Winship, Christopher and Wolpert, Robert L. and Xie, Yu and Young, Cristobal and Zinman, Jonathan and Johnson, Valen E.},
	month = jan,
	year = {2018},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {6--10},
	file = {Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\H7NCT6LB\\Benjamin et al. - 2018 - Redefine statistical significance.pdf:application/pdf;Snapshot:C\:\\Users\\shaonl\\Zotero\\storage\\BQ8LLZYW\\s41562-017-0189-z.html:text/html},
}

@article{rosenthal_file_1979,
	title = {The file drawer problem and tolerance for null results},
	volume = {86},
	issn = {1939-1455(ELECTRONIC),0033-2909(PRINT)},
	doi = {10.1037/0033-2909.86.3.638},
	abstract = {For any given research area, one cannot tell how many studies have been conducted but never reported. The extreme view of the "file drawer problem" is that journals are filled with the 5\% of the studies that show Type I errors, while the file drawers are filled with the 95\% of the studies that show nonsignificant results. Quantitative procedures for computing the tolerance for filed and future null results are reported and illustrated, and the implications are discussed. (15 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {3},
	journal = {Psychological Bulletin},
	author = {Rosenthal, Robert},
	year = {1979},
	note = {Number: 3},
	keywords = {Experimentation, Statistical Probability, Scientific Communication, Statistical Tests, Type I Errors},
	pages = {638--641},
}

@article{christensen_transparency_2018,
	title = {Transparency, {Reproducibility}, and the {Credibility} of {Economics} {Research}},
	volume = {56},
	issn = {0022-0515},
	url = {https://www.aeaweb.org/articles?id=10.1257/jel.20171350},
	doi = {10.1257/jel.20171350},
	abstract = {There is growing interest in enhancing research transparency and reproducibility in economics and other scientific fields. We survey existing work on these topics within economics, and discuss the evidence suggesting that publication bias, inability to replicate, and specification searching remain widespread in the discipline. We next discuss recent progress in this area, including through improved research design, study registration and pre-analysis plans, disclosure standards, and open sharing of data and materials, drawing on experiences in both economics and other social sciences. We discuss areas where consensus is emerging on new practices, as well as approaches that remain controversial, and speculate about the most effective ways to make economics research more credible in the future.},
	language = {en},
	number = {3},
	urldate = {2019-08-02},
	journal = {Journal of Economic Literature},
	author = {Christensen, Garret and Miguel, Edward},
	month = sep,
	year = {2018},
	note = {Number: 3},
	keywords = {Market for Economists, Methodological Issues: General, Higher Education, Research Institutions, Role of Economics, Role of Economists},
	pages = {920--980},
}

@article{akerlof_persistence_2018,
	title = {Persistence of false paradigms in low-power sciences},
	volume = {115},
	copyright = {Copyright © 2018 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by-nc-nd/4.0/This open access article is distributed under Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND).},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/115/52/13228},
	doi = {10.1073/pnas.1816454115},
	abstract = {We develop a model describing how false paradigms may persist, hindering scientific progress. The model features two paradigms, one describing reality better than the other. Tenured scientists display homophily: They favor tenure candidates who adhere to their paradigm. As in statistics, power is the probability (absent any bias) of denying tenure to scientists adhering to the false paradigm. The model shows that because of homophily, when power is low, the false paradigm may prevail. Then, only an increase in power can ignite convergence to the true paradigm. Historical case studies suggest that low power comes either from lack of empirical evidence or from reluctance to base tenure decisions on available evidence.},
	language = {en},
	number = {52},
	urldate = {2019-08-02},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Akerlof, George A. and Michaillat, Pascal},
	month = dec,
	year = {2018},
	pmid = {30523117},
	note = {Number: 52},
	keywords = {homophily, paradigms, power, scientific progress, tenure},
	pages = {13228--13233},
}

@article{leamer_lets_1983,
	title = {Let’s {Take} the {Con} {Out} of {Econometrics}},
	volume = {73},
	url = {http://www.econ.ucla.edu/workingpapers/wp239.pdf},
	urldate = {2019-08-02},
	journal = {The American Economic Review},
	author = {Leamer, Edward},
	year = {1983},
	pages = {31--43},
}

@article{miguel_promoting_2014,
	title = {Promoting {Transparency} in {Social} {Science} {Research}},
	volume = {343},
	copyright = {Copyright © 2014, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {https://www.gsb.stanford.edu/sites/gsb/files/publication-pdf/Science-2014-Miguel-30-1.pdf},
	doi = {10.1126/science.1245317},
	abstract = {Social scientists should adopt higher transparency standards to improve the quality and credibility of research.
Social scientists should adopt higher transparency standards to improve the quality and credibility of research.},
	language = {en},
	number = {6166},
	urldate = {2017-11-27},
	journal = {Science},
	author = {Miguel, E. and Camerer, C. and Casey, K. and Cohen, J. and Esterling, K. M. and Gerber, A. and Glennerster, R. and Green, D. P. and Humphreys, M. and Imbens, G. and Laitin, D. and Madon, T. and Nelson, L. and Nosek, B. A. and Petersen, M. and Sedlmayr, R. and Simmons, J. P. and Simonsohn, U. and Laan, M. Van der},
	month = jan,
	year = {2014},
	pmid = {24385620},
	note = {Number: 6166},
	pages = {30--31},
}

@article{nosek_promoting_2015,
	title = {Promoting an open research culture},
	volume = {348},
	copyright = {Copyright © 2015, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/348/6242/1422},
	doi = {10.1126/science.aab2374},
	abstract = {Author guidelines for journals could help to promote transparency, openness, and reproducibility
Author guidelines for journals could help to promote transparency, openness, and reproducibility},
	language = {en},
	number = {6242},
	urldate = {2017-12-21},
	journal = {Science},
	author = {Nosek, B. A. and Alter, G. and Banks, G. C. and Borsboom, D. and Bowman, S. D. and Breckler, S. J. and Buck, S. and Chambers, C. D. and Chin, G. and Christensen, G. and Contestabile, M. and Dafoe, A. and Eich, E. and Freese, J. and Glennerster, R. and Goroff, D. and Green, D. P. and Hesse, B. and Humphreys, M. and Ishiyama, J. and Karlan, D. and Kraut, A. and Lupia, A. and Mabry, P. and Madon, T. and Malhotra, N. and Mayo-Wilson, E. and McNutt, M. and Miguel, E. and Paluck, E. Levy and Simonsohn, U. and Soderberg, C. and Spellman, B. A. and Turitto, J. and VandenBos, G. and Vazire, S. and Wagenmakers, E. J. and Wilson, R. and Yarkoni, T.},
	month = jun,
	year = {2015},
	pmid = {26113702},
	note = {Number: 6242},
	pages = {1422--1425},
}

@article{open_science_collaboration_estimating_2015,
	title = {Estimating the reproducibility of psychological science},
	volume = {349},
	copyright = {Copyright © 2015, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/349/6251/aac4716},
	doi = {10.1126/science.aac4716},
	abstract = {Empirically analyzing empirical evidence
One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.
Science, this issue 10.1126/science.aac4716
Structured Abstract
INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.
RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.
RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P {\textless} .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.
CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that “we already know this” belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know. {\textless}img class="fragment-image" aria-describedby="F1-caption" src="https://science.sciencemag.org/content/sci/349/6251/aac4716/F1.medium.gif"/{\textgreater} Download high-res image Open in new tab Download Powerpoint Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.
Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.
A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.
A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.},
	language = {en},
	number = {6251},
	urldate = {2019-08-02},
	journal = {Science},
	author = {{Open Science Collaboration}},
	month = aug,
	year = {2015},
	pmid = {26315443},
	note = {Number: 6251},
	pages = {aac4716},
}

@article{stodden_enhancing_2016,
	title = {Enhancing reproducibility for computational methods},
	volume = {354},
	copyright = {Copyright © 2016, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {http://web.stanford.edu/~vcs/papers/ERCM2016-STODDEN.pdf},
	doi = {10.1126/science.aah6168},
	abstract = {Data, code, and workflows should be available and cited
Data, code, and workflows should be available and cited},
	language = {en},
	number = {6317},
	urldate = {2019-08-02},
	journal = {Science},
	author = {Stodden, Victoria and McNutt, Marcia and Bailey, David H. and Deelman, Ewa and Gil, Yolanda and Hanson, Brooks and Heroux, Michael A. and Ioannidis, John P. A. and Taufer, Michela},
	month = dec,
	year = {2016},
	pmid = {27940837},
	note = {Number: 6317},
	pages = {1240--1241},
}

@book{national_academies_of_sciences_fostering_2017,
	title = {Fostering {Integrity} in {Research}},
	isbn = {978-0-309-39125-2},
	url = {https://www.nap.edu/catalog/21896/fostering-integrity-in-research},
	abstract = {Download a PDF of "Fostering Integrity in Research" by the National Academies of Sciences, Engineering, and Medicine for free.},
	language = {en},
	urldate = {2019-08-02},
	author = {National Academies of Sciences, Engineering},
	month = apr,
	year = {2017},
	doi = {10.17226/21896},
}

@book{national_academies_of_sciences_reproducibility_2019,
	title = {Reproducibility and {Replicability} in {Science}},
	isbn = {978-0-309-48613-2},
	url = {https://www.nap.edu/catalog/25303/reproducibility-and-replicability-in-science},
	abstract = {Download a PDF of "Reproducibility and Replicability in Science" by the National Academies of Sciences, Engineering, and Medicine for free.},
	language = {en},
	urldate = {2019-08-02},
	author = {National Academies of Sciences, Engineering},
	month = may,
	year = {2019},
	doi = {10.17226/25303},
}

@article{gerber_publication_2008,
	title = {Publication {Bias} in {Empirical} {Sociological} {Research}: {Do} {Arbitrary} {Significance} {Levels} {Distort} {Published} {Results}?},
	volume = {37},
	issn = {0049-1241},
	shorttitle = {Publication {Bias} in {Empirical} {Sociological} {Research}},
	url = {https://doi.org/10.1177/0049124108318973},
	doi = {10.1177/0049124108318973},
	abstract = {Despite great attention to the quality of research methods in individual studies, if publication decisions of journals are a function of the statistical significance of research findings, the published literature as a whole may not produce accurate measures of true effects. This article examines the two most prominent sociology journals (the American Sociological Review and the American Journal of Sociology) and another important though less influential journal (The Sociological Quarterly) for evidence of publication bias. The effect of the .05 significance level on the pattern of published findings is examined using a ``caliper'' test, and the hypothesis of no publication bias can be rejected at approximately the 1 in 10 million level. Findings suggest that some of the results reported in leading sociology journals may be misleading and inaccurate due to publication bias. Some reasons for publication bias and proposed reforms to reduce its impact on research are also discussed.},
	language = {en},
	number = {1},
	urldate = {2019-08-02},
	journal = {Sociological Methods \& Research},
	author = {Gerber, Alan S. and Malhotra, Neil},
	month = aug,
	year = {2008},
	note = {Number: 1},
	pages = {3--30},
}

@article{lenz_achieving_2017,
	title = {Achieving {Statistical} {Significance} with {Covariates} and without {Transparency}},
	url = {https://osf.io/preprints/metaarxiv/s42ba/},
	doi = {10.31222/osf.io/s42ba},
	abstract = {How often do articles depend on suppression effects for their findings? How often do they disclose this fact? By suppression effects, we mean covariate-induced increases in effect sizes. Researchers generally scrutinize suppression effects as they want reassurance that researchers have a strong explanation for effect size increases, especially when the statistical significance of the key finding depends on them. In this research note, we report a startling finding: Almost 40\% of observational articles in a leading journal depend on suppression effects for statistical significance and almost 0\% disclose this fact. These findings may point to a hole in the review process: journals are accepting articles that depend on suppression effects without readers, reviewers, or editors being made aware.},
	urldate = {2019-08-02},
	author = {Lenz, Gabriel and Sahn, Alexander},
	month = apr,
	year = {2017},
}

@article{gelman_garden_2019,
	title = {The {Garden} of {Forking} {Paths}: {Why} {Multiple} {Comparisons} {Can} {Be} a {Problem}, {Even} {When} {There} {Is} {No} ‘{Fishing} {Expedition}’ or ‘p-{Hacking}’ and the {Research} {Hypothesis} {Was} {Posited} {Ahead} of {Time}},
	url = {http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf},
	urldate = {2019-08-02},
	journal = {Unpublished},
	author = {Gelman, Andrew and Loken, Eric},
	month = aug,
	year = {2019},
}

@article{button_power_2013,
	title = {Power failure: why small sample size undermines the reliability of neuroscience},
	volume = {14},
	copyright = {2013 Nature Publishing Group},
	issn = {1471-0048},
	shorttitle = {Power failure},
	url = {https://www.nature.com/articles/nrn3475},
	doi = {10.1038/nrn3475},
	abstract = {A study with low statistical power has a reduced chance of detecting a true effect, but it is less well appreciated that low power also reduces the likelihood that a statistically significant result reflects a true effect. Here, we show that the average statistical power of studies in the neurosciences is very low. The consequences of this include overestimates of effect size and low reproducibility of results. There are also ethical dimensions to this problem, as unreliable research is inefficient and wasteful. Improving reproducibility in neuroscience is a key priority and requires attention to well-established but often ignored methodological principles.},
	language = {en},
	number = {5},
	urldate = {2019-08-02},
	journal = {Nature Reviews Neuroscience},
	author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munafò, Marcus R.},
	month = may,
	year = {2013},
	note = {Number: 5},
	pages = {365--376},
}

@article{coville_how_2017,
	title = {How {Often} {Should} {We} {Believe} {Positive} {Results}? {Assessing} the {Credibility} of {Research} {Findings} in {Development} {Economics}},
	shorttitle = {How {Often} {Should} {We} {Believe} {Positive} {Results}?},
	url = {https://osf.io/5nsh3},
	doi = {https://doi.org/10.31222/osf.io/5nsh3},
	abstract = {Under-powered studies combined with low prior beliefs about intervention effects increase the chances that a positive result is overstated. We collect prior beliefs about intervention impacts from 125 experts to estimate the false positive and false negative report probabilities (FPRP and FNRP) as well as Type S (sign) and Type M (magnitude) errors for studies in development economics. We find that the large majority of studies in our sample are generally credible. We discuss how more systematic collection and use of prior expectations could help improve the literature.},
	urldate = {2019-08-02},
	journal = {MetaArXiv},
	author = {Coville, Aidan and Vivalt, Eva},
	month = aug,
	year = {2017},
	doi = {10.31222/osf.io/5nsh3},
}

@article{burlig_panel_2017,
	title = {Panel {Data} and {Experimental} {Design}},
	url = {https://osf.io/d5eud},
	doi = {https://doi.org/10.31222/osf.io/d5eud},
	abstract = {How should researchers design experiments to detect treatment effects with panel data? In this paper, we derive analytical expressions for the variance of panel estimators under non-i.i.d. error structures, which inform power calculations in panel data settings. Using Monte Carlo simulation, we demonstrate that, with correlated errors, traditional methods for experimental design result in experiments that are incorrectly powered with proper inference. Failing to account for serial correlation yields overpowered experiments in short panels and underpowered experiments in long panels. Using both data from a randomized experiment in China and a high-frequency dataset of U.S. electricity consumption, we show that these results hold in real-world settings. Our theoretical results enable us to achieve correctly powered experiments in both simulated and real data. This paper provides researchers with the tools to design well-powered experiments in panel data settings.},
	urldate = {2019-08-02},
	journal = {MetaArXiv},
	author = {Burlig, Fiona and Preonas, Louis and Woerman, Matt},
	month = mar,
	year = {2017},
	doi = {10.31222/osf.io/d5eud},
}

@book{gandrud_reproducible_2015,
	title = {Reproducible {Research} with {R} and {R} {Studio} {Second} {Edition}},
	url = {https://englianhu.files.wordpress.com/2016/01/reproducible-research-with-r-and-studio-2nd-edition.pdf},
	publisher = {CRC Press},
	author = {Gandrud, Christopher},
	year = {2015},
}

@article{bowers_six_2019,
	title = {Six  steps  to  a  better  relationship  with your future self},
	volume = {18},
	url = {http://www.jakebowers.org/PAPERS/tpm_v18_n2.pdf},
	number = {2},
	urldate = {2019-08-02},
	journal = {The Political Methodologist},
	author = {Bowers, Jake},
	month = aug,
	year = {2019},
	note = {Number: 2},
}

@article{wood_push_2018,
	title = {Push button replication: {Is} impact evaluation evidence for international development verifiable?},
	volume = {13},
	issn = {1932-6203},
	shorttitle = {Push button replication},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0209416},
	doi = {10.1371/journal.pone.0209416},
	abstract = {Objective Empirical research that cannot be reproduced using the original dataset and software code (replication files) creates a credibility challenge, as it means those published findings are not verifiable. This study reports the results of a research audit exercise, known as the push button replication project, that tested a sample of studies that use similar empirical methods but span a variety of academic fields. Methods We developed and piloted a detailed protocol for conducting push button replication and determining the level of comparability of these replication findings to original findings. We drew a sample of articles from the ten journals that published the most impact evaluations from low- and middle-income countries from 2010 through 2012. This set includes health, economics, and development journals. We then selected all articles in these journals published in 2014 that meet the same inclusion criteria and implemented the protocol on the sample. Results Of the 109 articles in our sample, only 27 are push button replicable, meaning the provided code run on the provided dataset produces comparable findings for the key results in the published article. The authors of 59 of the articles refused to provide replication files. Thirty of these 59 articles were published in journals that had replication file requirements in 2014, meaning these articles are non-compliant with their journal requirements. For the remaining 23 of the 109 articles, we confirmed that three had proprietary data, we received incomplete replication files for 15, and we found minor differences in the replication results for five. Conclusion The findings presented here reveal that many economics, development, and public health researchers are a long way from adopting the norm of open research. Journals do not appear to be playing a strong role in ensuring the availability of replication files.},
	language = {en},
	number = {12},
	urldate = {2019-08-02},
	journal = {PLOS ONE},
	author = {Wood, Benjamin D. K. and Müller, Rui and Brown, Annette N.},
	month = dec,
	year = {2018},
	note = {Number: 12},
	keywords = {Economics, Development economics, Economic development, Health economics, Open data, Public and occupational health, Replication studies, Scientific publishing},
	pages = {e0209416},
}

@unpublished{dellavigna_predicting_2016,
	type = {Working {Paper}},
	title = {Predicting {Experimental} {Results}: {Who} {Knows} {What}?},
	shorttitle = {Predicting {Experimental} {Results}},
	url = {http://www.nber.org/papers/w22566},
	abstract = {Academic experts frequently recommend policies and treatments. But how well do they anticipate the impact of different treatments? And how do their predictions compare to the predictions of non-experts? We analyze how 208 experts forecast the results of 15 treatments involving monetary and non-monetary motivators in a real-effort task. We compare these forecasts to those made by PhD students and non-experts: undergraduates, MBAs, and an online sample. We document seven main results. First, the average forecast of experts predicts quite well the experimental results. Second, there is a strong wisdom-of-crowds effect: the average forecast outperforms 96 percent of individual forecasts. Third, correlates of expertise—citations, academic rank, field, and contextual experience–do not improve forecasting accuracy. Fourth, experts as a group do better than non-experts, but not if accuracy is defined as rank ordering treatments. Fifth, measures of effort, confidence, and revealed ability are predictive of forecast accuracy to some extent, especially for non-experts. Sixth, using these measures we identify `superforecasters' among the non-experts who outperform the experts out of sample. Seventh, we document that these results on forecasting accuracy surprise the forecasters themselves. We present a simple model that organizes several of these results and we stress the implications for the collection of forecasts of future experimental results.},
	urldate = {2019-08-02},
	author = {DellaVigna, Stefano and Pope, Devin},
	month = aug,
	year = {2016},
	doi = {10.3386/w22566},
}

@article{lin_standard_2016,
	title = {Standard {Operating} {Procedures}: {A} {Safety} {Net} for {Pre}-{Analysis} {Plans}},
	volume = {49},
	issn = {1049-0965, 1537-5935},
	shorttitle = {Standard {Operating} {Procedures}},
	url = {https://www.stat.berkeley.edu/~winston/sop-safety-net.pdf},
	doi = {10.1017/S1049096516000810},
	abstract = {Across the social sciences, growing concerns about research transparency have led to calls for pre-analysis plans (PAPs) that specify in advance how researchers intend to analyze the data they are about to gather. PAPs promote transparency and credibility by helping readers distinguish between exploratory and confirmatory analyses. However, PAPs are time-consuming to write and may fail to anticipate contingencies that arise in the course of data collection. This article proposes the use of “standard operating procedures” (SOPs)—default practices to guide decisions when issues arise that were not anticipated in the PAP. We offer an example of an SOP that can be adapted by other researchers seeking a safety net to support their PAPs.},
	language = {en},
	number = {3},
	urldate = {2019-08-03},
	journal = {PS: Political Science \& Politics},
	author = {Lin, Winston and Green, Donald P.},
	month = jul,
	year = {2016},
	note = {Number: 3},
	pages = {495--500},
}

@article{hamermesh_viewpoint_2007,
	title = {Viewpoint: {Replication} in economics},
	volume = {40},
	issn = {1540-5982},
	shorttitle = {Viewpoint},
	url = {https://www.semanticscholar.org/paper/Viewpoint%3A-Replication-in-Economics-Hamermesh/7f81fa275aca9d397ee124b9da910f7891554fa8},
	doi = {10.1111/j.1365-2966.2007.00428.x},
	abstract = {Abstract. This examination of the role and potential for replication in economics points out the paucity of both pure replication – checking on others' published papers using their data – and scientific replication – using data representing different populations in one's own work or in a comment. Several controversies in empirical economics are used to illustrate how and how not to behave when replicating others' work. The incentives for replication are examined, and proposals aimed at journal editors and authors are advanced that might stimulate an activity that most economists applaud but few perform.},
	language = {en},
	number = {3},
	urldate = {2019-08-02},
	journal = {Canadian Journal of Economics/Revue canadienne d'économique},
	author = {Hamermesh, Daniel S.},
	year = {2007},
	note = {Number: 3},
	keywords = {A14, B41, C59},
	pages = {715--733},
}

@article{dafoe_science_2014,
	title = {Science {Deserves} {Better}: {The} {Imperative} to {Share} {Complete} {Replication} {Files}},
	volume = {47},
	issn = {1049-0965, 1537-5935},
	shorttitle = {Science {Deserves} {Better}},
	url = {https://www.cambridge.org/core/journals/ps-political-science-and-politics/article/science-deserves-better-the-imperative-to-share-complete-replication-files/C19AE087642810DD9C0C83BF8D0908A9},
	doi = {10.1017/S104909651300173X},
	abstract = {In April 2013, a controversy arose when a working paper (Herndon, Ash, and Pollin 2013) claimed to show serious errors in a highly cited and influential economics paper by Carmen Reinhart and Kenneth Rogoff (2010). The Reinhart and Rogoff paper had come to serve as authoritative evidence in elite conversations (Krugman 2013) that high levels of debt, especially above the “90 percent [debt/GDP] threshold” (Reinhart and Rogoff 2010, 577), posed a risk to economic growth. Much of the coverage of this controversy focused on an error that was a “perfect made-for-TV mistake” (Stevenson and Wolfers 2013) involving a simple error in the formula used in their Excel calculations. The real story here, however, is that it took three years for this error and other issues to be discovered because replication files were not publicly available, nor were they provided to scholars when asked. If professional norms or the American Economic Review had required that authors publish replication files, this debate would be advanced by three years and discussions about austerity policies would have been based on a more clear-sighted appraisal of the evidence.},
	language = {en},
	number = {1},
	urldate = {2019-08-02},
	journal = {PS: Political Science \& Politics},
	author = {Dafoe, Allan},
	month = jan,
	year = {2014},
	note = {Number: 1},
	pages = {60--66},
}

@article{gertler_how_2018,
	title = {How to make replication the norm},
	volume = {554},
	copyright = {2018 Nature},
	url = {http://www.nature.com/articles/d41586-018-02108-9},
	doi = {10.1038/d41586-018-02108-9},
	abstract = {The publishing system builds in resistance to replication. Paul Gertler, Sebastian Galiani and Mauricio Romero surveyed economics journals to find out how to fix it.},
	language = {EN},
	number = {7693},
	urldate = {2019-08-02},
	journal = {Nature},
	author = {Gertler, Paul and Galiani, Sebastian and Romero, Mauricio},
	month = feb,
	year = {2018},
	note = {Number: 7693},
	pages = {417--419},
}

@article{olken_promises_2015,
	title = {Promises and {Perils} of {Pre}-analysis {Plans}},
	volume = {29},
	issn = {0895-3309},
	url = {https://economics.mit.edu/files/10654},
	doi = {10.1257/jep.29.3.61},
	abstract = {The purpose of this paper is to help think through the advantages and costs of rigorous pre-specification of statistical analysis plans in economics. 
A pre-analysis plan pre-specifies in a precise way the analysis to be run before examining the data. 
A researcher can specify variables, data cleaning procedures, regression specifications, and so on. 
If the regressions are pre-specified in advance and researchers are required to report all the results they pre-specify, data-mining problems are greatly reduced. 
I begin by laying out the basics of what a statistical analysis plan actually contains so those researchers unfamiliar with it can better understand how it is done. 
In so doing, I have drawn both on standards used in clinical trials, which are clearly specified by the Food and Drug Administration, as well as my own practical experience from writing these plans in economics contexts. 
I then lay out some of the advantages of pre-specified analysis plans, both for the scientific community as a whole and also for the researcher. 
I also explore some of the limitations and costs of such plans. 
I then review a few pieces of evidence that suggest that, in many contexts, the benefits of using pre-specified analysis plans may not be as high as one might have expected initially. 
For the most part, I will focus on the relatively narrow issue of pre-analysis for randomized controlled trials.},
	language = {en},
	number = {3},
	urldate = {2019-08-03},
	journal = {Journal of Economic Perspectives},
	author = {Olken, Benjamin A.},
	month = sep,
	year = {2015},
	note = {Number: 3},
	keywords = {Cluster Analysis, Factor Models, Multiple or Simultaneous Equation Models: Classification Methods, Principal Components},
	pages = {61--80},
}

@misc{zandbergen_ensuring_2014,
	type = {Research article},
	title = {Ensuring {Confidentiality} of {Geocoded} {Health} {Data}: {Assessing} {Geographic} {Masking} {Strategies} for {Individual}-{Level} {Data}},
	shorttitle = {Ensuring {Confidentiality} of {Geocoded} {Health} {Data}},
	url = {https://www.hindawi.com/journals/amed/2014/567049/},
	abstract = {Public health datasets increasingly use geographic identifiers such as an individual’s address. Geocoding these addresses often provides new insights since it becomes possible to examine spatial patterns and associations. Address information is typically considered confidential and is therefore not released or shared with others. Publishing maps with the locations of individuals, however, may also breach confidentiality since addresses and associated identities can be discovered through reverse geocoding. One commonly used technique to protect confidentiality when releasing individual-level geocoded data is geographic masking. This typically consists of applying a certain amount of random perturbation in a systematic manner to reduce the risk of reidentification. A number of geographic masking techniques have been developed as well as methods to quantity the risk of reidentification associated with a particular masking method. This paper presents a review of the current state-of-the-art in geographic masking, summarizing the various methods and their strengths and weaknesses. Despite recent progress, no universally accepted or endorsed geographic masking technique has emerged. Researchers on the other hand are publishing maps using geographic masking of confidential locations. Any researcher publishing such maps is advised to become familiar with the different masking techniques available and their associated reidentification risks.},
	language = {en},
	urldate = {2019-08-03},
	journal = {Advances in Medicine},
	author = {Zandbergen, Paul A.},
	year = {2014},
	doi = {10.1155/2014/567049},
}

@article{sturdy_opening_2017,
	title = {Opening {Up} {Evaluation} {Microdata}: {Balancing} {Risks} and {Benefits} of {Research} {Transparency}},
	shorttitle = {Opening {Up} {Evaluation} {Microdata}},
	url = {https://osf.io/s67my},
	doi = {10.31222/osf.io/s67my},
	abstract = {The Millennium Challenge Corporation (MCC), an independent U.S. Government foreign aid agency, uses a data-based approach to select country partners, and design, monitor, and evaluate programs. With a commitment to transparency built into the MCC model as a means for accountability and learning, the agency has grappled with the ethical, legal, and practical issues related to public release of its evaluation microdata. In 2013, MCC identified key objectives for public release of its evaluation microdata: facilitating verification of evaluation results, maximizing data usability, and minimizing risk to survey respondents. The agency implemented a series of actions, including establishing a microdata dissemination mechanism; data protection principles; an internal Disclosure Review Board; procedures for restricted-access to low-risk data; and allocating staffing and resources to implement the data protection principles. The paper discusses MCC’s experience and concludes with a set of lessons learned about finding balance between benefits and risks when releasing evaluation microdata.},
	urldate = {2019-08-03},
	journal = {MetaArXiv},
	author = {Sturdy, Jennifer and Burch, Stephanie and Hanson, Heather and Molyneaux, Jack},
	month = mar,
	year = {2017},
}

@article{playford_administrative_2016,
	title = {Administrative social science data: {The} challenge of reproducible research:},
	shorttitle = {Administrative social science data},
	url = {https://journals.sagepub.com/stoken/default+domain/10.1177/2053951716684143/full?utm_source=Adestra&utm_medium=email&utm_content=Administrative+Social+Science+Data%3A+The+Challenge+of+Reproducible+Research&utm_campaign=Methods17aug&utm_term=},
	doi = {10.1177/2053951716684143},
	abstract = {Powerful new social science data resources are emerging. One particularly important source is administrative data, which were originally collected for organisat...},
	language = {en},
	urldate = {2019-08-03},
	journal = {Big Data \& Society},
	author = {Playford, Christopher J. and Gayle, Vernon and Connelly, Roxanne and Gray, Alasdair JG},
	month = dec,
	year = {2016},
}

@article{goodman_ten_2014,
	title = {Ten {Simple} {Rules} for the {Care} and {Feeding} of {Scientific} {Data}},
	volume = {10},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003542},
	doi = {10.1371/journal.pcbi.1003542},
	language = {en},
	number = {4},
	urldate = {2019-08-03},
	journal = {PLOS Computational Biology},
	author = {Goodman, Alyssa and Pepe, Alberto and Blocker, Alexander W. and Borgman, Christine L. and Cranmer, Kyle and Crosas, Merce and Stefano, Rosanne Di and Gil, Yolanda and Groth, Paul and Hedstrom, Margaret and Hogg, David W. and Kashyap, Vinay and Mahabal, Ashish and Siemiginowska, Aneta and Slavkovic, Aleksandra},
	month = apr,
	year = {2014},
	note = {Number: 4},
	keywords = {Data management, Archives, Data visualization, Moons, Open source software, Scientists, Software tools, Telescopes},
	pages = {e1003542},
}

@article{kaiser_protecting_2009,
	title = {Protecting {Respondent} {Confidentiality} in {Qualitative} {Research}},
	volume = {19},
	issn = {1049-7323},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2805454/},
	doi = {10.1177/1049732309350879},
	abstract = {For qualitative researchers, maintaining respondent confidentiality while presenting rich, detailed accounts of social life presents unique challenges. These challenges are not adequately addressed in the literature on research ethics and research methods. Using an example from a study of breast cancer survivors, I argue that by carefully considering the audience for one’s research and by re-envisioning the informed consent process, qualitative researchers can avoid confidentiality dilemmas that might otherwise lead them not to report rich, detailed data.},
	number = {11},
	urldate = {2019-08-03},
	journal = {Qualitative health research},
	author = {Kaiser, Karen},
	month = nov,
	year = {2009},
	pmid = {19843971},
	pmcid = {PMC2805454},
	note = {Number: 11},
	pages = {1632--1641},
}

@article{ioannidis_meta-research_2015,
	title = {Meta-research: {Evaluation} and {Improvement} of {Research} {Methods} and {Practices}},
	volume = {13},
	issn = {1545-7885},
	shorttitle = {Meta-research},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002264},
	doi = {10.1371/journal.pbio.1002264},
	abstract = {As the scientific enterprise has grown in size and diversity, we need empirical evidence on the research process to test and apply interventions that make it more efficient and its results more reliable. Meta-research is an evolving scientific discipline that aims to evaluate and improve research practices. It includes thematic areas of methods, reporting, reproducibility, evaluation, and incentives (how to do, report, verify, correct, and reward science). Much work is already done in this growing field, but efforts to-date are fragmented. We provide a map of ongoing efforts and discuss plans for connecting the multiple meta-research efforts across science worldwide.},
	language = {en},
	number = {10},
	urldate = {2019-08-03},
	journal = {PLOS Biology},
	author = {Ioannidis, John P. A. and Fanelli, Daniele and Dunne, Debbie Drake and Goodman, Steven N.},
	month = oct,
	year = {2015},
	note = {Number: 10},
	keywords = {Research design, Publication ethics, Research funding, Research integrity, Research monitoring, Research quality assessment, Research validity, Social research},
	pages = {e1002264},
}

@article{russo_how_2007,
	title = {How to {Review} a {Meta}-analysis},
	volume = {3},
	issn = {1554-7914},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3099299/},
	abstract = {Meta-analysis is a systematic review of a focused topic in the literature that provides a quantitative estimate for the effect of a treatment intervention or exposure. The key to designing a high quality meta-analysis is to identify an area where the effect of the treatment or exposure is uncertain and where a relatively homogenous body of literature exists. The techniques used in meta-analysis provide a structured and standardized approach for analyzing prior findings in a specific topic in the literature. Meta-analysis findings may not only be quantitative but also may be qualitative and reveal the biases, strengths, and weaknesses of existing studies. The results of a meta-analysis can be used to form treatment recommendations or to provide guidance in the design of future clinical trials.},
	number = {8},
	urldate = {2019-08-03},
	journal = {Gastroenterology \& Hepatology},
	author = {Russo, Mark W.},
	month = aug,
	year = {2007},
	pmid = {21960873},
	pmcid = {PMC3099299},
	note = {Number: 8},
	pages = {637--642},
}

@article{kepes_avoiding_2014,
	title = {Avoiding bias in publication bias research: {The} value of “null” findings},
	volume = {29},
	issn = {0889-3268},
	number = {2},
	journal = {Journal of Business and Psychology},
	author = {Kepes, Sven and Banks, George C and Oh, In-Sue},
	year = {2014},
	note = {Publisher: Springer},
	pages = {183--203},
}

@article{franco_publication_2014,
	title = {Publication bias in the social sciences: {Unlocking} the file drawer},
	volume = {345},
	issn = {0036-8075},
	number = {6203},
	journal = {Science},
	author = {Franco, Annie and Malhotra, Neil and Simonovits, Gabor},
	year = {2014},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {1502--1505},
}

@article{wicherts_degrees_2016,
	title = {Degrees of {Freedom} in {Planning}, {Running}, {Analyzing}, and {Reporting} {Psychological} {Studies}: {A} {Checklist} to {Avoid} p-{Hacking}},
	volume = {7},
	issn = {1664-1078},
	shorttitle = {Degrees of {Freedom} in {Planning}, {Running}, {Analyzing}, and {Reporting} {Psychological} {Studies}},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2016.01832/full},
	doi = {10.3389/fpsyg.2016.01832},
	abstract = {The designing, collecting, analyzing, and reporting of psychological studies entail many choices that are often arbitrary. The opportunistic use of these so-called researcher degrees of freedom aimed at obtaining statistically significant results is problematic because it enhances the chances of false positive results and may inflate effect size estimates. In this review article, we present an extensive list of 34 degrees of freedom that researchers have in formulating hypotheses, and in designing, running, analyzing, and reporting of psychological research. The list can be used in research methods education, and as a checklist to assess the quality of preregistrations and to determine the potential for bias due to (arbitrary) choices in unregistered studies.},
	language = {English},
	urldate = {2020-09-18},
	journal = {Frontiers in Psychology},
	author = {Wicherts, Jelte M. and Veldkamp, Coosje L. S. and Augusteijn, Hilde E. M. and Bakker, Marjan and van Aert, Robbie C. M. and van Assen, Marcel A. L. M.},
	year = {2016},
	note = {Publisher: Frontiers},
	keywords = {p-hacking, Bias, Experimental design (study designs), questionable research practices, Research methods education, significance chasing, Significance testing},
	file = {Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\NH67UCZJ\\Wicherts et al. - 2016 - Degrees of Freedom in Planning, Running, Analyzing.pdf:application/pdf;PubMed Central Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\ID3JIVHC\\Wicherts et al. - 2016 - Degrees of Freedom in Planning, Running, Analyzing.pdf:application/pdf},
}

@article{riveros_timing_2013,
	title = {Timing and {Completeness} of {Trial} {Results} {Posted} at {ClinicalTrials}.gov and {Published} in {Journals}},
	volume = {10},
	issn = {1549-1676},
	url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1001566},
	doi = {10.1371/journal.pmed.1001566},
	abstract = {Agnes Dechartres and colleagues searched ClinicalTrials.gov for completed drug RCTs with results reported and then searched for corresponding studies in PubMed to evaluate timeliness and completeness of reporting. Please see later in the article for the Editors' Summary},
	language = {en},
	number = {12},
	urldate = {2021-04-17},
	journal = {PLOS Medicine},
	author = {Riveros, Carolina and Dechartres, Agnes and Perrodeau, Elodie and Haneef, Romana and Boutron, Isabelle and Ravaud, Philippe},
	month = dec,
	year = {2013},
	note = {Publisher: Public Library of Science},
	keywords = {Randomized controlled trials, Scientific publishing, Clinical trials, Adverse events, Drug administration, Medical journals, Research reporting guidelines, Statistical data},
	pages = {e1001566},
	file = {Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\YY7INBA7\\Riveros et al. - 2013 - Timing and Completeness of Trial Results Posted at.pdf:application/pdf;Snapshot:C\:\\Users\\shaonl\\Zotero\\storage\\UUUPWW5W\\article.html:text/html},
}

@article{paez_gray_2017,
	title = {Gray literature: {An} important resource in systematic reviews},
	volume = {10},
	copyright = {© 2017 Chinese Cochrane Center, West China Hospital of Sichuan University and John Wiley \& Sons Australia, Ltd},
	issn = {1756-5391},
	shorttitle = {Gray literature},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jebm.12266},
	doi = {https://doi.org/10.1111/jebm.12266},
	abstract = {Systematic reviews aide the analysis and dissemination of evidence, using rigorous and transparent methods to generate empirically attained answers to focused research questions. Identifying all evidence relevant to the research questions is an essential component, and challenge, of systematic reviews. Gray literature, or evidence not published in commercial publications, can make important contributions to a systematic review. Gray literature can include academic papers, including theses and dissertations, research and committee reports, government reports, conference papers, and ongoing research, among others. It may provide data not found within commercially published literature, providing an important forum for disseminating studies with null or negative results that might not otherwise be disseminated. Gray literature may thusly reduce publication bias, increase reviews’ comprehensiveness and timeliness, and foster a balanced picture of available evidence. Gray literature's diverse formats and audiences can present a significant challenge in a systematic search for evidence. However, the benefits of including gray literature may far outweigh the cost in time and resource needed to search for it, and it is important for it to be included in a systematic review or review of evidence. A carefully thought out gray literature search strategy may be an invaluable component of a systematic review. This narrative review provides guidance about the benefits of including gray literature in a systematic review, and sources for searching through gray literature. An illustrative example of a search for evidence within gray literature sources is presented to highlight the potential contributions of such a search to a systematic review. Benefits and challenges of gray literature search methods are discussed, and recommendations made.},
	language = {en},
	number = {3},
	urldate = {2021-04-17},
	journal = {Journal of Evidence-Based Medicine},
	author = {Paez, Arsenio},
	year = {2017},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/jebm.12266},
	keywords = {evidence, publication bias, search, systematic review, gray literature},
	pages = {233--240},
	file = {Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\F3NZWKBY\\Paez - 2017 - Gray literature An important resource in systemat.pdf:application/pdf;Snapshot:C\:\\Users\\shaonl\\Zotero\\storage\\MTXWQQ6D\\jebm.html:text/html},
}

@article{adams_searching_2016,
	title = {Searching and synthesising ‘grey literature’ and ‘grey information’ in public health: critical reflections on three case studies},
	volume = {5},
	issn = {2046-4053},
	shorttitle = {Searching and synthesising ‘grey literature’ and ‘grey information’ in public health},
	url = {https://doi.org/10.1186/s13643-016-0337-y},
	doi = {10.1186/s13643-016-0337-y},
	abstract = {Grey literature includes a range of documents not controlled by commercial publishing organisations. This means that grey literature can be difficult to search and retrieve for evidence synthesis. Much knowledge and evidence in public health, and other fields, accumulates from innovation in practice. This knowledge may not even be of sufficient formality to meet the definition of grey literature. We term this knowledge ‘grey information’. Grey information may be even harder to search for and retrieve than grey literature.},
	number = {1},
	urldate = {2021-04-17},
	journal = {Systematic Reviews},
	author = {Adams, Jean and Hillier-Brown, Frances C. and Moore, Helen J. and Lake, Amelia A. and Araujo-Soares, Vera and White, Martin and Summerbell, Carolyn},
	month = sep,
	year = {2016},
	keywords = {Public health, Interventions, Systematic review, Evidence synthesis, Grey information, Grey literature},
	pages = {164},
	file = {Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\NBWFAJTB\\Adams et al. - 2016 - Searching and synthesising ‘grey literature’ and ‘.pdf:application/pdf;Snapshot:C\:\\Users\\shaonl\\Zotero\\storage\\B8L8WF94\\s13643-016-0337-y.html:text/html},
}

@article{rothstein_publication_2005,
	title = {Publication bias in meta-analysis},
	journal = {Publication bias in meta-analysis: Prevention, assessment and adjustments},
	author = {Rothstein, Hannah R and Sutton, Alexander J and Borenstein, Michael},
	year = {2005},
	note = {Publisher: Wiley Online Library},
	pages = {1--7},
}

@article{simons_constraints_2017,
	title = {Constraints on {Generality} ({COG}): {A} {Proposed} {Addition} to {All} {Empirical} {Papers}},
	volume = {12},
	issn = {1745-6916},
	shorttitle = {Constraints on {Generality} ({COG})},
	url = {https://doi.org/10.1177/1745691617708630},
	doi = {10.1177/1745691617708630},
	abstract = {Psychological scientists draw inferences about populations based on samples—of people, situations, and stimuli—from those populations. Yet, few papers identify their target populations, and even fewer justify how or why the tested samples are representative of broader populations. A cumulative science depends on accurately characterizing the generality of findings, but current publishing standards do not require authors to constrain their inferences, leaving readers to assume the broadest possible generalizations. We propose that the discussion section of all primary research articles specify Constraints on Generality (i.e., a “COG” statement) that identify and justify target populations for the reported findings. Explicitly defining the target populations will help other researchers to sample from the same populations when conducting a direct replication, and it could encourage follow-up studies that test the boundary conditions of the original finding. Universal adoption of COG statements would change publishing incentives to favor a more cumulative science.},
	language = {en},
	number = {6},
	urldate = {2023-06-06},
	journal = {Perspectives on Psychological Science},
	author = {Simons, Daniel J. and Shoda, Yuichi and Lindsay, D. Stephen},
	month = nov,
	year = {2017},
	note = {Publisher: SAGE Publications Inc},
	pages = {1123--1128},
	file = {SAGE PDF Full Text:C\:\\Users\\shaonl\\Zotero\\storage\\M47Z56WE\\Simons et al. - 2017 - Constraints on Generality (COG) A Proposed Additi.pdf:application/pdf},
}

@book{moravcsik_transparency_2020,
	title = {Transparency in qualitative research},
	isbn = {1-5264-2103-8},
	publisher = {SAGE Publications Limited},
	author = {Moravcsik, Andrew},
	year = {2020},
}

@incollection{steinle_stability_2016,
	address = {New York, UNITED STATES},
	title = {Stability and {Replication} of {Experimental} {Results}: {A} {Historical} {Perspective}},
	isbn = {978-1-118-86523-1},
	url = {http://ebookcentral.proquest.com/lib/upenn-ebooks/detail.action?docID=4547409},
	booktitle = {Reproducibility : {Principles}, {Problems}, {Practices}, and {Prospects}},
	publisher = {John Wiley \& Sons, Incorporated},
	author = {Steinle, Friedrich},
	year = {2016},
	keywords = {Observation (Scientific method)},
}

@book{glick_routledge_2017,
	title = {Routledge {Revivals}: {Medieval} {Science}, {Technology} and {Medicine} (2006): {An} {Encyclopedia}},
	isbn = {978-1-351-67617-5},
	shorttitle = {Routledge {Revivals}},
	abstract = {First published in 2005, this encyclopedia demonstrates that the millennium from the fall of the Roman Empire to the Renaissance was a period of great intellectual and practical achievement and innovation. In Europe, the Islamic world, South and East Asia, and the Americas, individuals built on earlier achievements, introduced sometimes radical refinements and laid the foundations for modern development. Medieval Science, Technology, and Medicine details the whole scope of scientific knowledge in the medieval period in more than 300 A to Z entries. This comprehensive resource discusses the research, application of knowledge, cultural and technology exchanges, experimentation, and achievements in the many disciplines related to science and technology. It also looks at the relationship between medieval science and the traditions it supplanted. Written by a select group of international scholars, this reference work will be of great use to scholars, students, and general readers researching topics in many fields, including medieval studies, world history, history of science, history of technology, history of medicine, and cultural studies.},
	language = {en},
	publisher = {Taylor \& Francis},
	author = {Glick, Thomas F. and Livesey, Steven J. and Wallis, Faith},
	month = jul,
	year = {2017},
	note = {Google-Books-ID: zTQrDwAAQBAJ},
	keywords = {History / Europe / General, History / Europe / Medieval, History / General, History / Middle East / General},
}

@article{zampieri_andreas_2015,
	title = {Andreas {Vesalius}: {Celebrating} 500 years of dissecting nature},
	volume = {2015},
	issn = {2305-7823},
	shorttitle = {Andreas {Vesalius}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4762440/},
	doi = {10.5339/gcsp.2015.66},
	number = {5},
	urldate = {2023-06-13},
	journal = {Global Cardiology Science \& Practice},
	author = {Zampieri, Fabio and ElMaghawry, Mohamed and Zanatta, Alberto and Thiene, Gaetano},
	month = dec,
	year = {2015},
	pmid = {28127546},
	pmcid = {PMC4762440},
	pages = {66},
	file = {PubMed Central Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\4JH3HH8A\\Zampieri et al. - 2015 - Andreas Vesalius Celebrating 500 years of dissect.pdf:application/pdf},
}

@misc{cartwright_middle-range_2020,
	type = {Published {Article} or {Volume}},
	title = {Middle-range theory: {Without} it what could anyone do?},
	copyright = {cc\_by\_nc\_nd\_4},
	shorttitle = {Middle-range theory},
	url = {https://ojs.ehu.eus/index.php/THEORIA/article/view/21479},
	abstract = {Philosophers of science have had little to say about 'middle-range theory' although much of what is done in science and of what drives its successes falls under that label. These lectures aim to spark an interest in the topic and to lay groundwork for further research on it. 'Middle' in 'middle range' is with respect to the level both of abstraction and generality. Much middle-range theory is about things that come under the label 'mechanism'. The lectures explore three different kinds of mechanism: structural mechanisms or underlying systems that afford causal pathways; causal-chain mechanisms that are represented in what in policy contexts are called 'theories of change' and for which I give an extended account following the causal process theory of Wesley Salmon; and middle-range-law mechanisms like those discussed by Jon Elster, which I claim are – and rightly are –  rampant throughout the social sciences. The theory of the democratic peace, that democracies do not go to war with democracies, serves as a running example. The discussions build up to the start of, first, an argument that reliability in social (and natural) science depends not so much on evidence than it does on the support of a virtuous tangle of practices (without which there couldn't even be evidence), and second, a defence of a community-practice centred instrumentalist understanding of many of the central basic principles that we use (often successfully) in social (and in natural) science for explanation, prediction and evaluation.},
	language = {en},
	urldate = {2023-06-13},
	journal = {THEORIA. An International Journal for Theory, History and Foundations of Science},
	author = {Cartwright, Nancy},
	month = sep,
	year = {2020},
	note = {ISSN: 2171-679X
Issue: 3
Number: 3
Pages: 269-323
Publisher: Euskal Herriko Unibertsitatea / Universidad del País Vasco
Volume: 35},
	file = {Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\JI8NEI9D\\Cartwright - 2020 - Middle-range theory Without it what could anyone .pdf:application/pdf;Snapshot:C\:\\Users\\shaonl\\Zotero\\storage\\JE43TSJK\\18280.html:text/html},
}

@article{merton_science_1942,
	title = {Science and technology in a democratic order, reprinted as {The} normative structure of science},
	url = {https://sciencepolicy.colorado.edu/students/envs_5110/merton_sociology_science.pdf},
	author = {Merton, Robert K},
	year = {1942},
	note = {Publisher: The sociology of science. Chicago: University of Chicago Press},
}

@article{huckvale_assessment_2019,
	title = {Assessment of the {Data} {Sharing} and {Privacy} {Practices} of {Smartphone} {Apps} for {Depression} and {Smoking} {Cessation}},
	volume = {2},
	issn = {2574-3805},
	url = {https://doi.org/10.1001/jamanetworkopen.2019.2542},
	doi = {10.1001/jamanetworkopen.2019.2542},
	abstract = {Inadequate privacy disclosures have repeatedly been identified by cross-sectional surveys of health applications (apps), including apps for mental health and behavior change. However, few studies have assessed directly the correspondence between privacy disclosures and how apps handle personal data. Understanding the scope of this discrepancy is particularly important in mental health, given enhanced privacy concerns relating to stigma and negative impacts of inadvertent disclosure. Because most health apps fall outside government regulation, up-to-date technical scrutiny is essential for informed decision making by consumers and health care professionals wishing to prescribe health apps.To provide a contemporary assessment of the privacy practices of popular apps for depression and smoking cessation by critically evaluating privacy policy content and, specifically, comparing disclosures regarding third-party data transmission to actual behavior.Cross-sectional assessment of 36 top-ranked (by app store search result ordering in January 2018) apps for depression and smoking cessation for Android and iOS in the United States and Australia. Privacy policy content was evaluated with prespecified criteria. Technical assessment of encrypted and unencrypted data transmission was performed. Analysis took place between April and June 2018.Correspondence between policies and transmission behavior observed by intercepting sent data.Twenty-five of 36 apps (69\%) incorporated a privacy policy. Twenty-two of 25 apps with a policy (88\%) provided information about primary uses of collected data, while only 16 (64\%) described secondary uses. While 23 of 25 apps with a privacy policy (92\%) stated in a policy that data would be transmitted to a third party, transmission was detected in 33 of all 36 apps (92\%). Twenty-nine of 36 apps (81\%) transmitted data for advertising and marketing purposes or analytics to just 2 commercial entities, Google and Facebook, but only 12 of 28 (43\%) transmitting data to Google and 6 of 12 (50\%) transmitting data to Facebook disclosed this.Data sharing with third parties that includes linkable identifiers is prevalent and focused on services provided by Google and Facebook. Despite this, most apps offer users no way to anticipate that data will be shared in this way. As a result, users are denied an informed choice about whether such sharing is acceptable to them. Privacy assessments that rely solely on disclosures made in policies, or are not regularly updated, are unlikely to uncover these evolving issues. This may limit their ability to offer effective guidance to consumers and health care professionals.},
	number = {4},
	urldate = {2023-06-14},
	journal = {JAMA Network Open},
	author = {Huckvale, Kit and Torous, John and Larsen, Mark E.},
	month = apr,
	year = {2019},
	pages = {e192542},
	file = {Full Text:C\:\\Users\\shaonl\\Zotero\\storage\\ENRQAGM9\\Huckvale et al. - 2019 - Assessment of the Data Sharing and Privacy Practic.pdf:application/pdf;Snapshot:C\:\\Users\\shaonl\\Zotero\\storage\\3PYTEU56\\2730782.html:text/html},
}

@article{de_saint-exupery_petit_1943,
	title = {Le petit prince [{The} little prince]},
	journal = {Verenigde State van Amerika: Reynal \& Hitchkock (US), Gallimard (FR)},
	author = {de Saint-Exupéry, Antoine},
	year = {1943},
}

@article{mitroff_norms_1974,
	title = {Norms and {Counter}-{Norms} in a {Select} {Group} of the {Apollo} {Moon} {Scientists}: {A} {Case} {Study} of the {Ambivalence} of {Scientists}},
	volume = {39},
	issn = {0003-1224},
	shorttitle = {Norms and {Counter}-{Norms} in a {Select} {Group} of the {Apollo} {Moon} {Scientists}},
	url = {https://www.jstor.org/stable/2094423},
	doi = {10.2307/2094423},
	abstract = {This paper describes a three and a half year study conducted over the course of the Apollo lunar missions with forty-two of the most prestigious scientists who studied the lunar rocks. The paper supports the Merton-E. Barber concept of sociological ambivalence, that social institutions reflect potentially conflicting sets of norms. The paper offers a set of counter-norms for science, arguing that if the norm of universalism is rooted in the impersonal character of science, an opposing counter-norm is rooted in the personal character of science. The paper also argues that not only is sociological ambivalence a characteristic of science, but it seems necessary for the existence and ultimate rationality of science.},
	number = {4},
	urldate = {2023-06-20},
	journal = {American Sociological Review},
	author = {Mitroff, Ian I.},
	year = {1974},
	note = {Publisher: [American Sociological Association, Sage Publications, Inc.]},
	pages = {579--595},
	file = {JSTOR Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\J7A3JE3Y\\Mitroff - 1974 - Norms and Counter-Norms in a Select Group of the A.pdf:application/pdf},
}

@article{cattau_david_2010,
	title = {David {Blackwell}, '{Superstar}'},
	language = {en},
	journal = {The University of Illinois Alumni Association – Illinois Alumni Magazine},
	author = {Cattau, Daniel},
	year = {2010},
	file = {Blackwell - The University of Illinois Alumni Association – Il.pdf:C\:\\Users\\shaonl\\Zotero\\storage\\77LDVFRV\\Blackwell - The University of Illinois Alumni Association – Il.pdf:application/pdf},
}

@article{brin_anatomy_1998,
	title = {The {Anatomy} of a {Large}-{Scale} {Hypertextual} {Web} {Search} {Engine}},
	volume = {30},
	url = {http://www-db.stanford.edu/~backrub/google.html},
	urldate = {2023-06-21},
	journal = {Computer Networks},
	author = {Brin, Sergey and Page, Lawrence},
	year = {1998},
	pages = {107--117},
	file = {Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\SW6C4Q6F\\Brin and Page - 1998 - The Anatomy of a Large-Scale Hypertextual Web Sear.pdf:application/pdf},
}

@misc{zaveri_googles_2020,
	title = {Google's {Revenue} {By} {Segment} (2016-2023)},
	url = {https://businessquant.com/google-revenue-by-segment},
	abstract = {The statistic highlights Google's Revenue By Segment, split across its Google Cloud, Network, Search, Other Bets and Youtube Ads segments.},
	language = {en-US},
	urldate = {2023-06-21},
	journal = {Business Quant},
	author = {Zaveri, Bhavya},
	month = feb,
	year = {2020},
	file = {Snapshot:C\:\\Users\\shaonl\\Zotero\\storage\\6TB5RE6Q\\google-revenue-by-segment.html:text/html},
}

@misc{flaherty_revolt_2018,
	title = {Revolt {Over} an {Editor}},
	url = {https://www.insidehighered.com/news/2018/04/30/prominent-psychologist-resigns-journal-editor-over-allegations-over-self-citation},
	abstract = {Prominent psychologist apologizes and resigns as journal editor over allegations of aggressive self-citation and more.},
	language = {en},
	urldate = {2023-06-21},
	journal = {Inside Higher Ed},
	author = {Flaherty, Colleen},
	year = {2018},
	keywords = {premium},
	file = {Snapshot:C\:\\Users\\shaonl\\Zotero\\storage\\PJ93F6Q6\\prominent-psychologist-resigns-journal-editor-over-allegations-over-self-citation.html:text/html},
}

@article{easterbrook_publication_1991,
	title = {Publication bias in clinical research},
	volume = {337},
	issn = {0140-6736},
	number = {8746},
	journal = {The Lancet},
	author = {Easterbrook, Phillipa J and Gopalan, Ramana and Berlin, JA and Matthews, David R},
	year = {1991},
	note = {Publisher: Elsevier},
	pages = {867--872},
}

@article{sternberg_am_2016,
	title = {"{Am} {I} {Famous} {Yet}?" {Judging} {Scholarly} {Merit} in {Psychological} {Science}: {An} {Introduction}},
	volume = {11},
	issn = {1745-6924},
	shorttitle = {"{Am} {I} {Famous} {Yet}?},
	doi = {10.1177/1745691616661777},
	abstract = {The purpose of this symposium is to consider new ways of judging merit in academia, especially with respect to research in psychological science. First, I discuss the importance of merit-based evaluation and the purpose of this symposium. Next, I review some previous ideas about judging merit-especially creative merit-and I describe some of the main criteria used by institutions today for judging the quality of research in psychological science. Finally, I suggest a new criterion that institutions and individuals might use and draw some conclusions.},
	language = {eng},
	number = {6},
	journal = {Perspectives on Psychological Science: A Journal of the Association for Psychological Science},
	author = {Sternberg, Robert J.},
	month = nov,
	year = {2016},
	pmid = {27899729},
	keywords = {Humans, Psychology, Achievement, Awards and Prizes, Universities, Attention, Research, Peer Review, Research, Periodicals as Topic, Creativity, Judgment, diagnosis, assessment, Congresses as Topic, application: policy, Authorship, creativity, imagination},
	pages = {877--881},
}

@incollection{brown_dogmatism_2012,
	address = {Boston, MA},
	title = {Dogmatism},
	isbn = {978-1-4419-1428-6},
	url = {https://doi.org/10.1007/978-1-4419-1428-6_856},
	language = {en},
	urldate = {2023-06-21},
	booktitle = {Encyclopedia of the {Sciences} of {Learning}},
	publisher = {Springer US},
	author = {Brown, Adam},
	editor = {Seel, Norbert M.},
	year = {2012},
	doi = {10.1007/978-1-4419-1428-6_856},
	pages = {1031--1032},
	file = {Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\486XEABU\\Brown - 2012 - Dogmatism.pdf:application/pdf},
}

@article{rokeach_nature_1954,
	title = {The nature and meaning of dogmatism.},
	issn = {1939-1471},
	author = {Rokeach, Milton},
	year = {1954},
	note = {Publisher: American Psychological Association},
}

@misc{resnick_intellectual_2019,
	title = {Intellectual humility: the importance of knowing you might be wrong},
	shorttitle = {Intellectual humility},
	url = {https://www.vox.com/science-and-health/2019/1/4/17989224/intellectual-humility-explained-psychology-replication},
	abstract = {Why it’s so hard to see our own ignorance, and what to do about it.},
	language = {en},
	urldate = {2023-06-21},
	journal = {Vox},
	author = {Resnick, Brian},
	month = jan,
	year = {2019},
	file = {Snapshot:C\:\\Users\\shaonl\\Zotero\\storage\\4WFGIU27\\intellectual-humility-explained-psychology-replication.html:text/html},
}

@book{grolemund_hands-programming_2014,
	title = {Hands-on programming with {R}: {Write} your own functions and simulations},
	isbn = {1-4493-5911-6},
	publisher = {" O'Reilly Media, Inc."},
	author = {Grolemund, Garrett},
	year = {2014},
}

@article{pernet_null_2016,
	title = {Null hypothesis significance testing: a short tutorial},
	volume = {4},
	issn = {2046-1402},
	shorttitle = {Null hypothesis significance testing},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5635437/},
	doi = {10.12688/f1000research.6963.3},
	abstract = {Although thoroughly criticized, null hypothesis significance testing (NHST) remains the statistical method of choice used to provide evidence for an effect, in biological, biomedical and social sciences. In this short tutorial, I first summarize the concepts behind the method, distinguishing test of significance (Fisher) and test of acceptance (Newman-Pearson) and point to common interpretation errors regarding the p-value. I then present the related concepts of confidence intervals and again point to common interpretation errors. Finally, I discuss what should be reported in which context. The goal is to clarify concepts to avoid interpretation errors and propose reporting practices.},
	urldate = {2023-06-22},
	journal = {F1000Research},
	author = {Pernet, Cyril},
	month = oct,
	year = {2016},
	pmid = {29067159},
	pmcid = {PMC5635437},
	pages = {621},
	file = {PubMed Central Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\H6USMKVD\\Pernet - 2016 - Null hypothesis significance testing a short tuto.pdf:application/pdf},
}

@article{nickerson_null_2000,
	title = {Null hypothesis significance testing: {A} review of an old and continuing controversy},
	volume = {5},
	issn = {1939-1463},
	shorttitle = {Null hypothesis significance testing},
	doi = {10.1037/1082-989X.5.2.241},
	abstract = {Null hypothesis significance testing (NHST) is arguably the most widely used approach to hypothesis evaluation among behavioral and social scientists. It is also very controversial. A major concern expressed by critics is that such testing is misunderstood by many of those who use it. Several other objections to its use have also been raised. In this article the author reviews and comments on the claimed misunderstandings as well as on other criticisms of the approach, and he notes arguments that have been advanced in support of NHST. Alternatives and supplements to NHST are considered, as are several related recommendations regarding the interpretation of experimental data. The concluding opinion is that NHST is easily misunderstood and misused but that when applied with good judgment it can be an effective aid to the interpretation of experimental data. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {2},
	journal = {Psychological Methods},
	author = {Nickerson, Raymond S.},
	year = {2000},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Statistical Analysis, Null Hypothesis Testing},
	pages = {241--301},
	file = {Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\NGQI9U5R\\Nickerson - 2000 - Null hypothesis significance testing A review of .pdf:application/pdf;Snapshot:C\:\\Users\\shaonl\\Zotero\\storage\\3U8R42W9\\2000-07827-007.html:text/html},
}

@article{lakens_practical_2021,
	title = {The {Practical} {Alternative} to the p {Value} {Is} the {Correctly} {Used} p {Value}},
	volume = {16},
	issn = {1745-6916},
	url = {https://doi.org/10.1177/1745691620958012},
	doi = {10.1177/1745691620958012},
	abstract = {Because of the strong overreliance on p values in the scientific literature, some researchers have argued that we need to move beyond p values and embrace practical alternatives. When proposing alternatives to p values statisticians often commit the ?statistician?s fallacy,? whereby they declare which statistic researchers really ?want to know.? Instead of telling researchers what they want to know, statisticians should teach researchers which questions they can ask. In some situations, the answer to the question they are most interested in will be the p value. As long as null-hypothesis tests have been criticized, researchers have suggested including minimum-effect tests and equivalence tests in our statistical toolbox, and these tests have the potential to greatly improve the questions researchers ask. If anyone believes p values affect the quality of scientific research, preventing the misinterpretation of p values by developing better evidence-based education and user-centered statistical software should be a top priority. Polarized discussions about which statistic scientists should use has distracted us from examining more important questions, such as asking researchers what they want to know when they conduct scientific research. Before we can improve our statistical inferences, we need to improve our statistical questions.},
	number = {3},
	urldate = {2023-06-27},
	journal = {Perspectives on Psychological Science},
	author = {Lakens, Daniël},
	month = may,
	year = {2021},
	note = {Publisher: SAGE Publications Inc},
	pages = {639--648},
	file = {Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\AXWECDK6\\Lakens - 2021 - The Practical Alternative to the p Value Is the Co.pdf:application/pdf},
}

@article{wasserstein_asa_2016,
	title = {The {ASA} {Statement} on p-{Values}: {Context}, {Process}, and {Purpose}},
	volume = {70},
	issn = {0003-1305},
	shorttitle = {The {ASA} {Statement} on p-{Values}},
	url = {https://doi.org/10.1080/00031305.2016.1154108},
	doi = {10.1080/00031305.2016.1154108},
	number = {2},
	urldate = {2023-06-27},
	journal = {The American Statistician},
	author = {Wasserstein, Ronald L. and Lazar, Nicole A.},
	month = apr,
	year = {2016},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00031305.2016.1154108},
	pages = {129--133},
	file = {Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\AW98GC7I\\Wasserstein and Lazar - 2016 - The ASA Statement on p-Values Context, Process, a.pdf:application/pdf},
}

@misc{harrell_statistical_2017,
	title = {Statistical {Thinking} - {A} {Litany} of {Problems} {With} p-values},
	url = {https://www.fharrell.com/post/pval-litany/},
	abstract = {p-values are very often misinterpreted. p-values and null hypothesis significant testing have hurt science. This article attempts to catalog all the ways in which these happen.},
	language = {en},
	urldate = {2023-06-27},
	author = {Harrell, Frank},
	month = feb,
	year = {2017},
	file = {Snapshot:C\:\\Users\\shaonl\\Zotero\\storage\\EASGHGCI\\pval-litany.html:text/html},
}

@article{nuzzo_scientific_2014,
	title = {Scientific method: {Statistical} errors},
	volume = {506},
	copyright = {2014 Springer Nature Limited},
	issn = {1476-4687},
	shorttitle = {Scientific method},
	url = {https://www.nature.com/articles/506150a},
	doi = {10.1038/506150a},
	abstract = {P values, the 'gold standard' of statistical validity, are not as reliable as many scientists assume.},
	language = {en},
	number = {7487},
	urldate = {2023-06-27},
	journal = {Nature},
	author = {Nuzzo, Regina},
	month = feb,
	year = {2014},
	note = {Number: 7487
Publisher: Nature Publishing Group},
	keywords = {Medical research, Publishing, Lab life, Mathematics and computing},
	pages = {150--152},
	file = {Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\A5ZL22RZ\\Nuzzo - 2014 - Scientific method Statistical errors.pdf:application/pdf;Snapshot:C\:\\Users\\shaonl\\Zotero\\storage\\SVYDXNA2\\506150a.html:text/html},
}

@article{greenland_statistical_2016,
	title = {Statistical tests, {P} values, confidence intervals, and power: a guide to misinterpretations},
	volume = {31},
	issn = {1573-7284},
	shorttitle = {Statistical tests, {P} values, confidence intervals, and power},
	url = {https://doi.org/10.1007/s10654-016-0149-3},
	doi = {10.1007/s10654-016-0149-3},
	abstract = {Misinterpretation and abuse of statistical tests, confidence intervals, and statistical power have been decried for decades, yet remain rampant. A key problem is that there are no interpretations of these concepts that are at once simple, intuitive, correct, and foolproof. Instead, correct use and interpretation of these statistics requires an attention to detail which seems to tax the patience of working scientists. This high cognitive demand has led to an epidemic of shortcut definitions and interpretations that are simply wrong, sometimes disastrously so—and yet these misinterpretations dominate much of the scientific literature. In light of this problem, we provide definitions and a discussion of basic statistics that are more general and critical than typically found in traditional introductory expositions. Our goal is to provide a resource for instructors, researchers, and consumers of statistics whose knowledge of statistical theory and technique may be limited but who wish to avoid and spot misinterpretations. We emphasize how violation of often unstated analysis protocols (such as selecting analyses for presentation based on the P values they produce) can lead to small P values even if the declared test hypothesis is correct, and can lead to large P values even if that hypothesis is incorrect. We then provide an explanatory list of 25 misinterpretations of P values, confidence intervals, and power. We conclude with guidelines for improving statistical interpretation and reporting.},
	language = {en},
	number = {4},
	urldate = {2023-06-27},
	journal = {European Journal of Epidemiology},
	author = {Greenland, Sander and Senn, Stephen J. and Rothman, Kenneth J. and Carlin, John B. and Poole, Charles and Goodman, Steven N. and Altman, Douglas G.},
	month = apr,
	year = {2016},
	keywords = {Confidence intervals, Hypothesis testing, Null testing, P value, Power, Significance tests, Statistical testing},
	pages = {337--350},
	file = {Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\U6AX6NR9\\Greenland et al. - 2016 - Statistical tests, P values, confidence intervals,.pdf:application/pdf},
}

@article{wasserstein_moving_2019,
	title = {Moving to a {World} {Beyond} “ \textit{p} {\textless} 0.05”},
	volume = {73},
	issn = {0003-1305, 1537-2731},
	url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2019.1583913},
	doi = {10.1080/00031305.2019.1583913},
	language = {en},
	number = {sup1},
	urldate = {2023-06-27},
	journal = {The American Statistician},
	author = {Wasserstein, Ronald L. and Schirm, Allen L. and Lazar, Nicole A.},
	month = mar,
	year = {2019},
	pages = {1--19},
	file = {Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\US6RF7JB\\Wasserstein et al. - 2019 - Moving to a World Beyond “ p  0.05”.pdf:application/pdf},
}

@article{engber_daryl_2017,
	title = {Daryl {Bem} {Proved} {ESP} {Is} {Real}},
	issn = {1091-2339},
	url = {https://slate.com/health-and-science/2017/06/daryl-bem-proved-esp-is-real-showed-science-is-broken.html},
	abstract = {Which means science is broken.},
	language = {en-US},
	urldate = {2023-07-13},
	journal = {Slate},
	author = {Engber, Daniel},
	month = jun,
	year = {2017},
	note = {Section: Science},
	keywords = {psychology, science, cover story, redux},
	file = {Snapshot:C\:\\Users\\shaonl\\Zotero\\storage\\K5JXD82R\\daryl-bem-proved-esp-is-real-showed-science-is-broken.html:text/html},
}

@misc{schimmack_why_2018,
	title = {Why the {Journal} of {Personality} and {Social} {Psychology} {Should} {Retract} {Article} {DOI}: 10.1037/a0021524 “{Feeling} the {Future}: {Experimental} evidence for anomalous retroactive influences on cognition and affect” by {Daryl} {J}. {Bem}},
	shorttitle = {Why the {Journal} of {Personality} and {Social} {Psychology} {Should} {Retract} {Article} {DOI}},
	url = {https://replicationindex.com/2018/01/05/bem-retraction/},
	abstract = {Added February 06, 2023 A collaboration between Bem and other believers in ESP and skeptics produced no evidence for a real effect and indirectly confirms that Bem’s incredible results were p…},
	language = {en-US},
	urldate = {2023-07-13},
	journal = {Replicability-Index},
	author = {Schimmack, Ulrich},
	month = jan,
	year = {2018},
	file = {Snapshot:C\:\\Users\\shaonl\\Zotero\\storage\\GCFAN3DJ\\bem-retraction.html:text/html},
}

@article{galak_correcting_2012,
	title = {Correcting the past: failures to replicate ψ},
	volume = {103},
	issn = {1939-1315},
	shorttitle = {Correcting the past},
	doi = {10.1037/a0029709},
	abstract = {Across 7 experiments (N = 3,289), we replicate the procedure of Experiments 8 and 9 from Bem (2011), which had originally demonstrated retroactive facilitation of recall. We failed to replicate that finding. We further conduct a meta-analysis of all replication attempts of these experiments and find that the average effect size (d = 0.04) is no different from 0. We discuss some reasons for differences between the results in this article and those presented in Bem (2011).},
	language = {eng},
	number = {6},
	journal = {Journal of Personality and Social Psychology},
	author = {Galak, Jeff and LeBoeuf, Robyn A. and Nelson, Leif D. and Simmons, Joseph P.},
	month = dec,
	year = {2012},
	pmid = {22924750},
	keywords = {Adult, Female, Humans, Male, Cognition, Young Adult, Time Factors, Mental Recall, Perception, Psycholinguistics, Neuropsychological Tests, Anticipation, Psychological, Parapsychology},
	pages = {933--948},
	file = {Full Text:C\:\\Users\\shaonl\\Zotero\\storage\\8UEW2YHS\\Galak et al. - 2012 - Correcting the past failures to replicate ψ.pdf:application/pdf},
}

@article{kekecs_raising_2023,
	title = {Raising the value of research studies in psychological science by increasing the credibility of research reports: the transparent {Psi} project},
	volume = {10},
	shorttitle = {Raising the value of research studies in psychological science by increasing the credibility of research reports},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.191375},
	doi = {10.1098/rsos.191375},
	abstract = {The low reproducibility rate in social sciences has produced hesitation among researchers in accepting published findings at their face value. Despite the advent of initiatives to increase transparency in research reporting, the field is still lacking tools to verify the credibility of research reports. In the present paper, we describe methodologies that let researchers craft highly credible research and allow their peers to verify this credibility. We demonstrate the application of these methods in a multi-laboratory replication of Bem's Experiment 1 (Bem 2011 J. Pers. Soc. Psychol. 100, 407–425. (doi:10.1037/a0021524)) on extrasensory perception (ESP), which was co-designed by a consensus panel including both proponents and opponents of Bem's original hypothesis. In the study we applied direct data deposition in combination with born-open data and real-time research reports to extend transparency to protocol delivery and data collection. We also used piloting, checklists, laboratory logs and video-documented trial sessions to ascertain as-intended protocol delivery, and external research auditors to monitor research integrity. We found 49.89\% successful guesses, while Bem reported 53.07\% success rate, with the chance level being 50\%. Thus, Bem's findings were not replicated in our study. In the paper, we discuss the implementation, feasibility and perceived usefulness of the credibility-enhancing methodologies used throughout the project.},
	number = {2},
	urldate = {2023-07-13},
	journal = {Royal Society Open Science},
	author = {Kekecs, Zoltan and Palfi, Bence and Szaszi, Barnabas and Szecsi, Peter and Zrubka, Mark and Kovacs, Marton and Bakos, Bence E. and Cousineau, Denis and Tressoldi, Patrizio and Schmidt, Kathleen and Grassi, Massimo and Evans, Thomas Rhys and Yamada, Yuki and Miller, Jeremy K. and Liu, Huanxu and Yonemitsu, Fumiya and Dubrov, Dmitrii and Röer, Jan Philipp and Becker, Marvin and Schnepper, Roxane and Ariga, Atsunori and Arriaga, Patrícia and Oliveira, Raquel and Põldver, Nele and Kreegipuu, Kairi and Hall, Braeden and Wiechert, Sera and Verschuere, Bruno and Girán, Kyra and Aczel, Balazs},
	month = feb,
	year = {2023},
	note = {Publisher: Royal Society},
	keywords = {research methods, credibility, metascience, real-time procedures, replication, transparency},
	pages = {191375},
	file = {Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\WII862LM\\Kekecs et al. - 2023 - Raising the value of research studies in psycholog.pdf:application/pdf},
}

@article{bem_feeling_2016,
	title = {Feeling the future: {A} meta-analysis of 90 experiments on the anomalous anticipation of random future events},
	volume = {4},
	issn = {2046-1402},
	shorttitle = {Feeling the future},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4706048/},
	doi = {10.12688/f1000research.7177.2},
	abstract = {In 2011, one of the authors (DJB) published a report of nine experiments in the
Journal of Personality and Social Psychology purporting to demonstrate that an individual’s cognitive and affective responses can be influenced by randomly selected stimulus events that do not occur until after his or her responses have already been made and recorded, a generalized variant of the phenomenon traditionally denoted by the term
precognition. To encourage replications, all materials needed to conduct them were made available on request. We here report a meta-analysis of 90 experiments from 33 laboratories in 14 countries which yielded an overall effect greater than 6 sigma,
z = 6.40,
p = 1.2 × 10
-10  with an effect size (Hedges’
g) of 0.09. A Bayesian analysis yielded a Bayes Factor of 5.1 × 10
9, greatly exceeding the criterion value of 100 for “decisive evidence” in support of the experimental hypothesis. When DJB’s original experiments are excluded, the combined effect size for replications by independent investigators is 0.06,
z = 4.16,
p = 1.1 × 10
-5, and the BF value is 3,853, again exceeding the criterion for “decisive evidence.” The number of potentially unretrieved experiments required to reduce the overall effect size of the complete database to a trivial value of 0.01 is 544, and seven of eight additional statistical tests support the conclusion that the database is not significantly compromised by either selection bias or by intense “
p-hacking”—the selective suppression of findings or analyses that failed to yield statistical significance.
P-curve analysis, a recently introduced statistical technique, estimates the true effect size of the experiments to be 0.20 for the complete database and 0.24 for the independent replications, virtually identical to the effect size of DJB’s original experiments (0.22) and the closely related “presentiment” experiments (0.21). We discuss the controversial status of precognition and other anomalous effects collectively known as
psi.},
	urldate = {2023-07-14},
	journal = {F1000Research},
	author = {Bem, Daryl and Tressoldi, Patrizio and Rabeyron, Thomas and Duggan, Michael},
	month = jan,
	year = {2016},
	pmid = {26834996},
	pmcid = {PMC4706048},
	pages = {1188},
	file = {PubMed Central Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\PI24Z8KX\\Bem et al. - 2016 - Feeling the future A meta-analysis of 90 experime.pdf:application/pdf},
}

@misc{lakens_20_2014,
	title = {The 20\% {Statistician}: {A} pre-publication peer-review of the '{Feeling} {The} {Future}' meta-analysis},
	shorttitle = {The 20\% {Statistician}},
	url = {http://daniellakens.blogspot.com/2014/05/a-pre-publication-peer-review-of-meta.html},
	urldate = {2023-07-14},
	journal = {The 20\% Statistician},
	author = {Lakens, Daniel},
	month = may,
	year = {2014},
	keywords = {meta-analysis, Statistics, Methodology, p-curve},
	file = {Blogspot Snapshot:C\:\\Users\\shaonl\\Zotero\\storage\\6HGECBB3\\a-pre-publication-peer-review-of-meta.html:text/html},
}

@article{doyen_behavioral_2012,
	title = {Behavioral {Priming}: {It}'s {All} in the {Mind}, but {Whose} {Mind}?},
	volume = {7},
	issn = {1932-6203},
	shorttitle = {Behavioral {Priming}},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0029081},
	doi = {10.1371/journal.pone.0029081},
	abstract = {The perspective that behavior is often driven by unconscious determinants has become widespread in social psychology. Bargh, Chen, and Burrows' (1996) famous study, in which participants unwittingly exposed to the stereotype of age walked slower when exiting the laboratory, was instrumental in defining this perspective. Here, we present two experiments aimed at replicating the original study. Despite the use of automated timing methods and a larger sample, our first experiment failed to show priming. Our second experiment was aimed at manipulating the beliefs of the experimenters: Half were led to think that participants would walk slower when primed congruently, and the other half was led to expect the opposite. Strikingly, we obtained a walking speed effect, but only when experimenters believed participants would indeed walk slower. This suggests that both priming and experimenters' expectations are instrumental in explaining the walking speed effect. Further, debriefing was suggestive of awareness of the primes. We conclude that unconscious behavioral priming is real, while real, involves mechanisms different from those typically assumed to cause the effect.},
	language = {en},
	number = {1},
	urldate = {2023-07-14},
	journal = {PLOS ONE},
	author = {Doyen, Stéphane and Klein, Olivier and Pichon, Cora-Lise and Cleeremans, Axel},
	month = jan,
	year = {2012},
	note = {Publisher: Public Library of Science},
	keywords = {Behavior, Social psychology, Animal behavior, Conceptual semantics, Experimental psychology, Priming (psychology), Social cognition, Time measurement},
	pages = {e29081},
	file = {Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\INNRTLNX\\Doyen et al. - 2012 - Behavioral Priming It's All in the Mind, but Whos.pdf:application/pdf},
}

@article{bargh_automaticity_1996,
	title = {Automaticity of social behavior: {Direct} effects of trait construct and stereotype activation on action.},
	volume = {71},
	issn = {1939-1315},
	number = {2},
	journal = {Journal of personality and social psychology},
	author = {Bargh, John A and Chen, Mark and Burrows, Lara},
	year = {1996},
	note = {Publisher: American Psychological Association},
	pages = {230},
}

@misc{bartlett_power_2013,
	title = {Power of {Suggestion}},
	url = {https://www.chronicle.com/article/power-of-suggestion/},
	abstract = {Experiments by social psychologists that showed the influence of unconscious cues over our behavior are now getting a skeptical second look.},
	language = {en},
	urldate = {2023-07-14},
	journal = {The Chronicle of Higher Education},
	author = {Bartlett, Tom},
	month = jan,
	year = {2013},
	note = {Section: The Review},
	file = {Snapshot:C\:\\Users\\shaonl\\Zotero\\storage\\TFE2QFL9\\power-of-suggestion.html:text/html},
}

@misc{schimmack_replicability_2019,
	title = {Replicability {Audit} of {John} {A}. {Bargh}},
	url = {https://replicationindex.com/2019/03/17/raudit-bargh/},
	abstract = {“Trust is good, but control is better”   INTRODUCTION Information about the replicability of published results is important because empirical results can only be used as evidence if the r…},
	language = {en-US},
	urldate = {2023-07-14},
	journal = {Replicability-Index},
	author = {Schimmack, Ulrich},
	month = mar,
	year = {2019},
	file = {Snapshot:C\:\\Users\\shaonl\\Zotero\\storage\\3RCXVA6A\\raudit-bargh.html:text/html},
}

@misc{yong_failed_2012,
	title = {A failed replication draws a scathing personal attack from a psychology professor},
	url = {https://www.nationalgeographic.com/science/article/failed-replication-bargh-psychology-study-doyen},
	abstract = {John Bargh, a psychologist at Yale University, has published a scathing attack on a paper that failed to replicate one of his most famous studies. His post, written on his own blog on Psychology Today, is a mixture of critiques of the science within the paper, and personal attacks against the researchers, PLOS ONE, the […]},
	language = {en},
	urldate = {2023-07-14},
	journal = {Science},
	author = {Yong, Ed},
	month = mar,
	year = {2012},
	note = {Section: Science},
	file = {Snapshot:C\:\\Users\\shaonl\\Zotero\\storage\\8L7FK9MW\\failed-replication-bargh-psychology-study-doyen.html:text/html},
}

@article{simmons_false-positive_2011,
	title = {False-{Positive} {Psychology}: {Undisclosed} {Flexibility} in {Data} {Collection} and {Analysis} {Allows} {Presenting} {Anything} as {Significant}},
	volume = {22},
	issn = {0956-7976, 1467-9280},
	shorttitle = {False-{Positive} {Psychology}},
	url = {http://journals.sagepub.com/doi/10.1177/0956797611417632},
	doi = {10.1177/0956797611417632},
	abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists’ nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
	language = {en},
	number = {11},
	urldate = {2023-07-14},
	journal = {Psychological Science},
	author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
	month = nov,
	year = {2011},
	pages = {1359--1366},
	file = {Simmons et al. - 2011 - False-Positive Psychology Undisclosed Flexibility.pdf:C\:\\Users\\shaonl\\Zotero\\storage\\8YEPTT4T\\Simmons et al. - 2011 - False-Positive Psychology Undisclosed Flexibility.pdf:application/pdf},
}

@misc{schimmack_meta-scientific_2020,
	title = {A {Meta}-{Scientific} {Perspective} on “{Thinking}: {Fast} and {Slow}},
	shorttitle = {A {Meta}-{Scientific} {Perspective} on “{Thinking}},
	url = {https://replicationindex.com/2020/12/30/a-meta-scientific-perspective-on-thinking-fast-and-slow/},
	abstract = {2011 was an important year in the history of psychology, especially social psychology. First, it became apparent that one social psychologist had faked results for dozens of publications ( Second, …},
	language = {en-US},
	urldate = {2023-07-18},
	journal = {Replicability-Index},
	author = {Schimmack, Ulrich},
	month = dec,
	year = {2020},
	file = {Snapshot:C\:\\Users\\shaonl\\Zotero\\storage\\WL6S6I3L\\a-meta-scientific-perspective-on-thinking-fast-and-slow.html:text/html},
}

@article{john_measuring_2012,
	title = {Measuring the {Prevalence} of {Questionable} {Research} {Practices} {With} {Incentives} for {Truth} {Telling}},
	volume = {23},
	issn = {0956-7976, 1467-9280},
	url = {http://journals.sagepub.com/doi/10.1177/0956797611430953},
	doi = {10.1177/0956797611430953},
	abstract = {Cases of clear scientific misconduct have received significant media attention recently, but less flagrantly questionable research practices may be more prevalent and, ultimately, more damaging to the academic enterprise. Using an anonymous elicitation format supplemented by incentives for honest reporting, we surveyed over 2,000 psychologists about their involvement in questionable research practices. The impact of truth-telling incentives on self-admissions of questionable research practices was positive, and this impact was greater for practices that respondents judged to be less defensible. Combining three different estimation methods, we found that the percentage of respondents who have engaged in questionable practices was surprisingly high. This finding suggests that some questionable practices may constitute the prevailing research norm.},
	language = {en},
	number = {5},
	urldate = {2023-07-18},
	journal = {Psychological Science},
	author = {John, Leslie K. and Loewenstein, George and Prelec, Drazen},
	month = may,
	year = {2012},
	pages = {524--532},
	file = {John et al. - 2012 - Measuring the Prevalence of Questionable Research .pdf:C\:\\Users\\shaonl\\Zotero\\storage\\K7DUR3KE\\John et al. - 2012 - Measuring the Prevalence of Questionable Research .pdf:application/pdf},
}

@article{camerer_evaluating_2018,
	title = {Evaluating the replicability of social science experiments in {Nature} and {Science} between 2010 and 2015},
	volume = {2},
	copyright = {2018 The Author(s)},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-018-0399-z},
	doi = {10.1038/s41562-018-0399-z},
	abstract = {Being able to replicate scientific findings is crucial for scientific progress1–15. We replicate 21 systematically selected experimental studies in the social sciences published in Nature and Science between 2010 and 201516–36. The replications follow analysis plans reviewed by the original authors and pre-registered prior to the replications. The replications are high powered, with sample sizes on average about five times higher than in the original studies. We find a significant effect in the same direction as the original study for 13 (62\%) studies, and the effect size of the replications is on average about 50\% of the original effect size. Replicability varies between 12 (57\%) and 14 (67\%) studies for complementary replicability indicators. Consistent with these results, the estimated true-positive rate is 67\% in a Bayesian analysis. The relative effect size of true positives is estimated to be 71\%, suggesting that both false positives and inflated effect sizes of true positives contribute to imperfect reproducibility. Furthermore, we find that peer beliefs of replicability are strongly related to replicability, suggesting that the research community could predict which results would replicate and that failures to replicate were not the result of chance alone.},
	language = {en},
	number = {9},
	urldate = {2023-07-18},
	journal = {Nature Human Behaviour},
	author = {Camerer, Colin F. and Dreber, Anna and Holzmeister, Felix and Ho, Teck-Hua and Huber, Jürgen and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Nosek, Brian A. and Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskil and Gampa, Anup and Heikensten, Emma and Hummer, Lily and Imai, Taisuke and Isaksson, Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, Eric-Jan and Wu, Hang},
	month = sep,
	year = {2018},
	note = {Number: 9
Publisher: Nature Publishing Group},
	keywords = {Psychology, Economics},
	pages = {637--644},
	file = {Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\VJTKASNE\\Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf:application/pdf},
}

@article{klein_many_2018,
	title = {Many {Labs} 2: {Investigating} {Variation} in {Replicability} {Across} {Samples} and {Settings}},
	volume = {1},
	issn = {2515-2459},
	shorttitle = {Many {Labs} 2},
	url = {https://doi.org/10.1177/2515245918810225},
	doi = {10.1177/2515245918810225},
	abstract = {We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were peer reviewed in advance, to examine variation in effect magnitudes across samples and settings. Each protocol was administered to approximately half of 125 samples that comprised 15,305 participants from 36 countries and territories. Using the conventional criterion of statistical significance (p {\textless} .05), we found that 15 (54\%) of the replications provided evidence of a statistically significant effect in the same direction as the original finding. With a strict significance criterion (p {\textless} .0001), 14 (50\%) of the replications still provided such evidence, a reflection of the extremely high-powered design. Seven (25\%) of the replications yielded effect sizes larger than the original ones, and 21 (75\%) yielded effect sizes smaller than the original ones. The median comparable Cohen’s ds were 0.60 for the original findings and 0.15 for the replications. The effect sizes were small ({\textless} 0.20) in 16 of the replications (57\%), and 9 effects (32\%) were in the direction opposite the direction of the original effect. Across settings, the Q statistic indicated significant heterogeneity in 11 (39\%) of the replication effects, and most of those were among the findings with the largest overall effect sizes; only 1 effect that was near zero in the aggregate showed significant heterogeneity according to this measure. Only 1 effect had a tau value greater than .20, an indication of moderate heterogeneity. Eight others had tau values near or slightly above .10, an indication of slight heterogeneity. Moderation tests indicated that very little heterogeneity was attributable to the order in which the tasks were performed or whether the tasks were administered in lab versus online. Exploratory comparisons revealed little heterogeneity between Western, educated, industrialized, rich, and democratic (WEIRD) cultures and less WEIRD cultures (i.e., cultures with relatively high and low WEIRDness scores, respectively). Cumulatively, variability in the observed effect sizes was attributable more to the effect being studied than to the sample or setting in which it was studied.},
	language = {en},
	number = {4},
	urldate = {2023-07-18},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {Klein, Richard A. and Vianello, Michelangelo and Hasselman, Fred and Adams, Byron G. and Adams, Reginald B. and Alper, Sinan and Aveyard, Mark and Axt, Jordan R. and Babalola, Mayowa T. and Bahník, Štěpán and Batra, Rishtee and Berkics, Mihály and Bernstein, Michael J. and Berry, Daniel R. and Bialobrzeska, Olga and Binan, Evans Dami and Bocian, Konrad and Brandt, Mark J. and Busching, Robert and Rédei, Anna Cabak and Cai, Huajian and Cambier, Fanny and Cantarero, Katarzyna and Carmichael, Cheryl L. and Ceric, Francisco and Chandler, Jesse and Chang, Jen-Ho and Chatard, Armand and Chen, Eva E. and Cheong, Winnee and Cicero, David C. and Coen, Sharon and Coleman, Jennifer A. and Collisson, Brian and Conway, Morgan A. and Corker, Katherine S. and Curran, Paul G. and Cushman, Fiery and Dagona, Zubairu K. and Dalgar, Ilker and Dalla Rosa, Anna and Davis, William E. and de Bruijn, Maaike and De Schutter, Leander and Devos, Thierry and de Vries, Marieke and Doğulu, Canay and Dozo, Nerisa and Dukes, Kristin Nicole and Dunham, Yarrow and Durrheim, Kevin and Ebersole, Charles R. and Edlund, John E. and Eller, Anja and English, Alexander Scott and Finck, Carolyn and Frankowska, Natalia and Freyre, Miguel-Ángel and Friedman, Mike and Galliani, Elisa Maria and Gandi, Joshua C. and Ghoshal, Tanuka and Giessner, Steffen R. and Gill, Tripat and Gnambs, Timo and Gómez, Ángel and González, Roberto and Graham, Jesse and Grahe, Jon E. and Grahek, Ivan and Green, Eva G. T. and Hai, Kakul and Haigh, Matthew and Haines, Elizabeth L. and Hall, Michael P. and Heffernan, Marie E. and Hicks, Joshua A. and Houdek, Petr and Huntsinger, Jeffrey R. and Huynh, Ho Phi and IJzerman, Hans and Inbar, Yoel and Innes-Ker, Åse H. and Jiménez-Leal, William and John, Melissa-Sue and Joy-Gaba, Jennifer A. and Kamiloğlu, Roza G. and Kappes, Heather Barry and Karabati, Serdar and Karick, Haruna and Keller, Victor N. and Kende, Anna and Kervyn, Nicolas and Knežević, Goran and Kovacs, Carrie and Krueger, Lacy E. and Kurapov, German and Kurtz, Jamie and Lakens, Daniël and Lazarević, Ljiljana B. and Levitan, Carmel A. and Lewis, Neil A. and Lins, Samuel and Lipsey, Nikolette P. and Losee, Joy E. and Maassen, Esther and Maitner, Angela T. and Malingumu, Winfrida and Mallett, Robyn K. and Marotta, Satia A. and Međedović, Janko and Mena-Pacheco, Fernando and Milfont, Taciano L. and Morris, Wendy L. and Murphy, Sean C. and Myachykov, Andriy and Neave, Nick and Neijenhuijs, Koen and Nelson, Anthony J. and Neto, Félix and Lee Nichols, Austin and Ocampo, Aaron and O’Donnell, Susan L. and Oikawa, Haruka and Oikawa, Masanori and Ong, Elsie and Orosz, Gábor and Osowiecka, Malgorzata and Packard, Grant and Pérez-Sánchez, Rolando and Petrović, Boban and Pilati, Ronaldo and Pinter, Brad and Podesta, Lysandra and Pogge, Gabrielle and Pollmann, Monique M. H. and Rutchick, Abraham M. and Saavedra, Patricio and Saeri, Alexander K. and Salomon, Erika and Schmidt, Kathleen and Schönbrodt, Felix D. and Sekerdej, Maciej B. and Sirlopú, David and Skorinko, Jeanine L. M. and Smith, Michael A. and Smith-Castro, Vanessa and Smolders, Karin C. H. J. and Sobkow, Agata and Sowden, Walter and Spachtholz, Philipp and Srivastava, Manini and Steiner, Troy G. and Stouten, Jeroen and Street, Chris N. H. and Sundfelt, Oskar K. and Szeto, Stephanie and Szumowska, Ewa and Tang, Andrew C. W. and Tanzer, Norbert and Tear, Morgan J. and Theriault, Jordan and Thomae, Manuela and Torres, David and Traczyk, Jakub and Tybur, Joshua M. and Ujhelyi, Adrienn and van Aert, Robbie C. M. and van Assen, Marcel A. L. M. and van der Hulst, Marije and van Lange, Paul A. M. and van ’t Veer, Anna Elisabeth and Vásquez- Echeverría, Alejandro and Ann Vaughn, Leigh and Vázquez, Alexandra and Vega, Luis Diego and Verniers, Catherine and Verschoor, Mark and Voermans, Ingrid P. J. and Vranka, Marek A. and Welch, Cheryl and Wichman, Aaron L. and Williams, Lisa A. and Wood, Michael and Woodzicka, Julie A. and Wronska, Marta K. and Young, Liane and Zelenski, John M. and Zhijia, Zeng and Nosek, Brian A.},
	month = dec,
	year = {2018},
	note = {Publisher: SAGE Publications Inc},
	pages = {443--490},
	file = {SAGE PDF Full Text:C\:\\Users\\shaonl\\Zotero\\storage\\IV333PHF\\Klein et al. - 2018 - Many Labs 2 Investigating Variation in Replicabil.pdf:application/pdf},
}

@article{oransky_retractions_2022,
	title = {Retractions are increasing, but not enough},
	volume = {608},
	copyright = {2022 Springer Nature Limited},
	url = {https://www.nature.com/articles/d41586-022-02071-6},
	doi = {10.1038/d41586-022-02071-6},
	abstract = {Retraction Watch has witnessed a retraction boom since its founding 12 years ago. But the scientific community must do much more.},
	language = {en},
	number = {7921},
	urldate = {2023-07-19},
	journal = {Nature},
	author = {Oransky, Ivan},
	month = aug,
	year = {2022},
	note = {Bandiera\_abtest: a
Cg\_type: World View
Number: 7921
Publisher: Nature Publishing Group
Subject\_term: Publishing, Ethics, Journalism},
	keywords = {Ethics, Publishing, Journalism},
	pages = {9--9},
	file = {Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\BQ9YMMUH\\Oransky - 2022 - Retractions are increasing, but not enough.pdf:application/pdf;Snapshot:C\:\\Users\\shaonl\\Zotero\\storage\\HJXE34WA\\d41586-022-02071-6.html:text/html},
}

@article{brainard_what_2018,
	title = {What a massive database of retracted papers reveals about science publishing’s ‘death penalty’},
	volume = {25},
	number = {1},
	journal = {Science},
	author = {Brainard, Jeffrey and You, Jia},
	year = {2018},
	pages = {1--5},
}

@article{kuhberger_self-correction_2022,
	title = {Self-correction in science: {The} effect of retraction on the frequency of citations},
	volume = {17},
	issn = {1932-6203},
	shorttitle = {Self-correction in science},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9728909/},
	doi = {10.1371/journal.pone.0277814},
	abstract = {We investigate the citation frequency of retracted scientific papers in science. For the period of five years before and after retraction, we counted the citations to papers in a sample of over 3,000 retracted, and a matched sample of another 3,000 non-retracted papers. Retraction led to a decrease in average annual citation frequency from about 5 before, to 2 citations after retraction. In contrast, for non-retracted control papers the citation counts were 4, and 5, respectively. Put differently, we found only a limited effect of retraction: retraction decreased citation frequency only by about 60\%, as compared to non-retracted papers. Thus, retracted papers often live on. For effective self-correction the scientific enterprise needs to be more effective in removing retracted papers from the scientific record. We discuss recent proposals to do so.},
	number = {12},
	urldate = {2023-07-19},
	journal = {PLOS ONE},
	author = {Kühberger, Anton and Streit, Daniel and Scherndl, Thomas},
	month = dec,
	year = {2022},
	pmid = {36477092},
	pmcid = {PMC9728909},
	pages = {e0277814},
	file = {PubMed Central Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\TMRNAQAB\\Kühberger et al. - 2022 - Self-correction in science The effect of retracti.pdf:application/pdf},
}

@misc{retraction_watch_top_2015,
	title = {Top 10 most highly cited retracted papers},
	url = {https://retractionwatch.com/the-retraction-watch-leaderboard/top-10-most-highly-cited-retracted-papers/},
	abstract = {Ever curious which retracted papers have been most cited by other scientists? Below, we present the list of the 10 most highly cited retractions as of December 2020. Readers will see some familiar …},
	language = {en-US},
	urldate = {2023-07-19},
	journal = {Retraction Watch},
	author = {{Retraction Watch}},
	month = dec,
	year = {2015},
	file = {Snapshot:C\:\\Users\\shaonl\\Zotero\\storage\\7GZIIZ49\\top-10-most-highly-cited-retracted-papers.html:text/html},
}

@article{bornemann-cimenti_perpetuation_2016,
	title = {Perpetuation of {Retracted} {Publications} {Using} the {Example} of the {Scott} {S}. {Reuben} {Case}: {Incidences}, {Reasons} and {Possible} {Improvements}},
	volume = {22},
	issn = {1471-5546},
	shorttitle = {Perpetuation of {Retracted} {Publications} {Using} the {Example} of the {Scott} {S}. {Reuben} {Case}},
	url = {https://doi.org/10.1007/s11948-015-9680-y},
	doi = {10.1007/s11948-015-9680-y},
	abstract = {In 2009, Scott S. Reuben was convicted of fabricating data, which lead to 25 of his publications being retracted. Although it is clear that the perpetuation of retracted articles negatively effects the appraisal of evidence, the extent to which retracted literature is cited had not previously been investigated. In this study, to better understand the perpetuation of discredited research, we examine the number of citations of Reuben’s articles within 5 years of their retraction. Citations of Reuben’s retracted articles were assessed using the Web of Science Core Collection (Thomson Reuters, NY). All citing articles were screened to discriminate between articles in which Reuben’s work was quoted as retracted, and articles in which his data was wrongly cited without any note of the retraction status. Twenty of Reuben’s publications had been cited 274 times between 2009 and 1024. In 2014, 45 \% of the retracted articles had been cited at least once. In only 25.8 \% of citing articles was it clearly stated that Reuben’s work had been retracted. Annual citations decreased from 108 in 2009 to 18 in 2014; however, the percentage of publications correctly indicating the retraction status also declined. The percentage of citations in top-25 \%-journals, as well as the percentage of citations in journals from Reuben’s research area, declined sharply after 2009. Our data show that even 5 years after their retraction, nearly half of Reuben’s articles are still being quoted and the retraction status is correctly mentioned in only one quarter of the citations.},
	language = {en},
	number = {4},
	urldate = {2023-07-19},
	journal = {Science and Engineering Ethics},
	author = {Bornemann-Cimenti, Helmar and Szilagyi, Istvan S. and Sandner-Kiesling, Andreas},
	month = aug,
	year = {2016},
	keywords = {Ethics research, Retraction of publication, Scientific misconduct, Scott S. Reuben},
	pages = {1063--1072},
	file = {Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\6JX4LSZ4\\Bornemann-Cimenti et al. - 2016 - Perpetuation of Retracted Publications Using the E.pdf:application/pdf},
}

@misc{gorski_when_2009,
	title = {When fraud undermines science-based medicine},
	url = {https://sciencebasedmedicine.org/when-fraud-undermines-science-based-medicine/},
	language = {en-US},
	urldate = {2023-07-19},
	journal = {Science-Based Medicine},
	author = {Gorski, David},
	month = mar,
	year = {2009},
	file = {Snapshot:C\:\\Users\\shaonl\\Zotero\\storage\\FQUIX366\\when-fraud-undermines-science-based-medicine.html:text/html},
}

@article{teixeira_da_silva_why_2017,
	title = {Why do some retracted papers continue to be cited?},
	volume = {110},
	issn = {1588-2861},
	url = {https://doi.org/10.1007/s11192-016-2178-9},
	doi = {10.1007/s11192-016-2178-9},
	abstract = {Perpetuation of retracted publications is an ongoing and even increasing problem in the scientific community. In addition to the direct distortion of scientific credibility, the use of retracted findings for interpretation und discussion in subsequent publications poses the risk of drawing false and, for example in medical research, even harmful conclusions. One major contributor to this development is that many authors are not aware of the retraction status of a paper they cite. COPE guidelines state that the “retracted status should be indicated as clearly as possible”, but this is definitely not true for many retracted publications. Likewise, databases do not consequently link retracted articles with the notice of retraction. Furthermore, many papers are deposit in the “original”, i.e. pre-retraction version on personal or institutional websites or online repositories. Similarly, printed “stock files” are obviously unaffected by a retraction. Clear identification of a retracted article using a watermark and in databases is a crucial step while incorporation of an electronic “retraction check” in reference management software and during the online submission is necessary to detect and avoid citing retracted literature. Solving this problem needs the close attention of everybody involved in the publishing process: authors, reviewers, and publishers.},
	language = {en},
	number = {1},
	urldate = {2023-07-19},
	journal = {Scientometrics},
	author = {Teixeira da Silva, Jaime A. and Bornemann-Cimenti, Helmar},
	month = jan,
	year = {2017},
	keywords = {Citations, COPE, Eric Poehlman, PubMed, Scott Reuben},
	pages = {365--370},
	file = {Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\FLKGDZMJ\\Teixeira da Silva and Bornemann-Cimenti - 2017 - Why do some retracted papers continue to be cited.pdf:application/pdf},
}

@misc{desbarats_ive_2021,
	title = {I’ve {Stopped} {Using} {Box} {Plots}. {Should} {You}?, {Nightingale}},
	shorttitle = {I’ve {Stopped} {Using} {Box} {Plots}. {Should} {You}?},
	url = {https://nightingaledvs.com/ive-stopped-using-box-plots-should-you/},
	abstract = {tl;dr: After having explained how to read box plots to thousands of workshop participants, I now believe that they’re poorly conceived (box plots, not...},
	language = {en-US},
	urldate = {2023-08-26},
	journal = {Nightingale},
	author = {Desbarats, Nick},
	month = nov,
	year = {2021},
	file = {Snapshot:C\:\\Users\\shaonl\\Zotero\\storage\\ALUY4MWX\\ive-stopped-using-box-plots-should-you.html:text/html},
}

@article{kerr_harking_1998,
	title = {{HARKing}: {Hypothesizing} after the results are known},
	volume = {2},
	issn = {1088-8683},
	number = {3},
	journal = {Personality and social psychology review},
	author = {Kerr, Norbert L},
	year = {1998},
	note = {Publisher: Sage Publications Sage CA: Los Angeles, CA},
	pages = {196--217},
}

@article{lishner_harking_2021,
	title = {{HARKing}: {Conceptualizations}, harms, and two fundamental remedies.},
	volume = {41},
	issn = {2151-3341},
	number = {4},
	journal = {Journal of Theoretical and Philosophical Psychology},
	author = {Lishner, David A},
	year = {2021},
	note = {Publisher: Educational Publishing Foundation},
	pages = {248},
}

@article{hollenbeck_harking_2017,
	title = {Harking, sharking, and tharking: {Making} the case for post hoc analysis of scientific data},
	volume = {43},
	issn = {0149-2063},
	number = {1},
	journal = {Journal of Management},
	author = {Hollenbeck, John R and Wright, Patrick M},
	year = {2017},
	note = {Publisher: Sage Publications Sage CA: Los Angeles, CA},
	pages = {5--18},
}

@article{thornton_publication_2000,
	title = {Publication bias in meta-analysis: its causes and consequences},
	volume = {53},
	issn = {0895-4356},
	shorttitle = {Publication bias in meta-analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S0895435699001614},
	doi = {10.1016/S0895-4356(99)00161-4},
	abstract = {Publication bias is a widespread problem that may seriously distort attempts to estimate the effect under investigation. The literature is reviewed to determine features of the design and execution of both single studies and meta-analyses leading to publication bias, and the role the author, journal editor, and reviewer play in selecting studies for publication. Methods of detecting, correcting for, and preventing publication bias are reviewed. The design of the meta-analysis itself, and the studies included in it, are shown to be important among a number of sources of publication bias. Various factors influence an author's decision to submit results for publication. Journal editors and reviewers are crucial in deciding which studies to publish. Various methods proposed for detecting and correcting for publication bias, though useful, all have limitations. However, prevention of publication bias by registering every trial undertaken or publishing all studies is an ideal that is hard to achieve.},
	number = {2},
	urldate = {2023-08-28},
	journal = {Journal of Clinical Epidemiology},
	author = {Thornton, Alison and Lee, Peter},
	month = feb,
	year = {2000},
	keywords = {Publication bias},
	pages = {207--216},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\WUGC9HC2\\Thornton and Lee - 2000 - Publication bias in meta-analysis its causes and .pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\shaonl\\Zotero\\storage\\ZESG52SP\\S0895435699001614.html:text/html},
}

@article{moller_testing_2001,
	title = {Testing and adjusting for publication bias},
	volume = {16},
	issn = {0169-5347},
	url = {https://www.cell.com/trends/ecology-evolution/abstract/S0169-5347(01)02235-2},
	doi = {10.1016/S0169-5347(01)02235-2},
	language = {English},
	number = {10},
	urldate = {2023-08-28},
	journal = {Trends in Ecology \& Evolution},
	author = {Møller, Anders Pape and Jennions, Michael D.},
	month = oct,
	year = {2001},
	note = {Publisher: Elsevier},
	keywords = {publication bias, effect size, adjustment for publication bias, Bioinformatics, Ecology, Evolution, fail-safe number, funnel plot, Parasitology, Plant Science, submission bias},
	pages = {580--586},
	file = {Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\6D3NSB9A\\Møller and Jennions - 2001 - Testing and adjusting for publication bias.pdf:application/pdf},
}

@book{porta_dictionary_2014,
	title = {A dictionary of epidemiology},
	isbn = {0-19-939004-5},
	publisher = {Oxford university press},
	author = {Porta, Miquel},
	year = {2014},
}

@article{mertens_effectiveness_2022,
	title = {The effectiveness of nudging: {A} meta-analysis of choice architecture interventions across behavioral domains},
	volume = {119},
	shorttitle = {The effectiveness of nudging},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.2107346118},
	doi = {10.1073/pnas.2107346118},
	abstract = {Over the past decade, choice architecture interventions or so-called nudges have received widespread attention from both researchers and policy makers. Built on insights from the behavioral sciences, this class of behavioral interventions focuses on the design of choice environments that facilitate personally and socially desirable decisions without restricting people in their freedom of choice. Drawing on more than 200 studies reporting over 440 effect sizes (n = 2,148,439), we present a comprehensive analysis of the effectiveness of choice architecture interventions across techniques, behavioral domains, and contextual study characteristics. Our results show that choice architecture interventions overall promote behavior change with a small to medium effect size of Cohen’s d = 0.43 (95\% CI [0.38, 0.48]). In addition, we find that the effectiveness of choice architecture interventions varies significantly as a function of technique and domain. Across behavioral domains, interventions that target the organization and structure of choice alternatives (decision structure) consistently outperform interventions that focus on the description of alternatives (decision information) or the reinforcement of behavioral intentions (decision assistance). Food choices are particularly responsive to choice architecture interventions, with effect sizes up to 2.5 times larger than those in other behavioral domains. Overall, choice architecture interventions affect behavior relatively independently of contextual study characteristics such as the geographical location or the target population of the intervention. Our analysis further reveals a moderate publication bias toward positive results in the literature. We end with a discussion of the implications of our findings for theory and behaviorally informed policy making.},
	number = {1},
	urldate = {2023-08-28},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Mertens, Stephanie and Herberz, Mario and Hahnel, Ulf J. J. and Brosch, Tobias},
	month = jan,
	year = {2022},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {e2107346118},
	file = {Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\QI65A7YJ\\Mertens et al. - 2022 - The effectiveness of nudging A meta-analysis of c.pdf:application/pdf},
}

@article{maier_no_2022,
	title = {No evidence for nudging after adjusting for publication bias},
	volume = {119},
	url = {https://www.pnas.org/doi/10.1073/pnas.2200300119},
	doi = {10.1073/pnas.2200300119},
	number = {31},
	urldate = {2023-08-28},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Maier, Maximilian and Bartoš, František and Stanley, T. D. and Shanks, David R. and Harris, Adam J. L. and Wagenmakers, Eric-Jan},
	month = aug,
	year = {2022},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {e2200300119},
	file = {Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\MNJ8PF22\\Maier et al. - 2022 - No evidence for nudging after adjusting for public.pdf:application/pdf},
}

@article{maier_robust_2023,
	title = {Robust {Bayesian} meta-analysis: {Addressing} publication bias with model-averaging},
	volume = {28},
	issn = {1939-1463},
	shorttitle = {Robust {Bayesian} meta-analysis},
	doi = {10.1037/met0000405},
	abstract = {Meta-analysis is an important quantitative tool for cumulative science, but its application is frustrated by publication bias. In order to test and adjust for publication bias, we extend model-averaged Bayesian meta-analysis with selection models. The resulting robust Bayesian meta-analysis (RoBMA) methodology does not require all-or-none decisions about the presence of publication bias, can quantify evidence in favor of the absence of publication bias, and performs well under high heterogeneity. By model-averaging over a set of 12 models, RoBMA is relatively robust to model misspecification and simulations show that it outperforms existing methods. We demonstrate that RoBMA finds evidence for the absence of publication bias in Registered Replication Reports and reliably avoids false positives. We provide an implementation in R so that researchers can easily use the new methodology in practice. (PsycInfo Database Record (c) 2023 APA, all rights reserved)},
	number = {1},
	journal = {Psychological Methods},
	author = {Maier, Maximilian and Bartoš, František and Wagenmakers, Eric-Jan},
	year = {2023},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Statistical Probability, Models, Simulation, Sciences, Meta Analysis, Adjustment, Bayesian Analysis, Errors},
	pages = {107--122},
	file = {Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\2S8YTIXD\\Maier et al. - 2023 - Robust Bayesian meta-analysis Addressing publicat.pdf:application/pdf;Snapshot:C\:\\Users\\shaonl\\Zotero\\storage\\QYFY2IJ4\\2022-62552-001.html:text/html},
}

@article{rodgers_evaluating_2021,
	title = {Evaluating meta-analytic methods to detect selective reporting in the presence of dependent effect sizes.},
	volume = {26},
	issn = {1939-1463},
	number = {2},
	journal = {Psychological methods},
	author = {Rodgers, Melissa A and Pustejovsky, James E},
	year = {2021},
	note = {Publisher: American Psychological Association},
	pages = {141},
}

@article{shi_trim-and-fill_2019,
	title = {The trim-and-fill method for publication bias: practical guidelines and recommendations based on a large database of meta-analyses},
	volume = {98},
	issn = {0025-7974},
	shorttitle = {The trim-and-fill method for publication bias},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6571372/},
	doi = {10.1097/MD.0000000000015987},
	abstract = {Publication bias is a type of systematic error when synthesizing evidence that cannot represent the underlying truth. Clinical studies with favorable results are more likely published and thus exaggerate the synthesized evidence in meta-analyses. The trim-and-fill method is a popular tool to detect and adjust for publication bias. Simulation studies have been performed to assess this method, but they may not fully represent realistic settings about publication bias. Based on real-world meta-analyses, this article provides practical guidelines and recommendations for using the trim-and-fill method. We used a worked illustrative example to demonstrate the idea of the trim-and-fill method, and we reviewed three estimators (R0, L0, and Q0) for imputing missing studies. A resampling method was proposed to calculate P values for all 3 estimators. We also summarized available meta-analysis software programs for implementing the trim-and-fill method. Moreover, we applied the method to 29,932 meta-analyses from the Cochrane Database of Systematic Reviews, and empirically evaluated its overall performance. We carefully explored potential issues occurred in our analysis. The estimators L0 and Q0 detected at least one missing study in more meta-analyses than R0, while Q0 often imputed more missing studies than L0. After adding imputed missing studies, the significance of heterogeneity and overall effect sizes changed in many meta-analyses. All estimators generally converged fast. However, L0 and Q0 failed to converge in a few meta-analyses that contained studies with identical effect sizes. Also, P values produced by different estimators could yield different conclusions of publication bias significance. Outliers and the pre-specified direction of missing studies could have influential impact on the trim-and-fill results. Meta-analysts are recommended to perform the trim-and-fill method with great caution when using meta-analysis software programs. Some default settings (e.g., the choice of estimators and the direction of missing studies) in the programs may not be optimal for a certain meta-analysis; they should be determined on a case-by-case basis. Sensitivity analyses are encouraged to examine effects of different estimators and outlying studies. Also, the trim-and-fill estimator should be routinely reported in meta-analyses, because the results depend highly on it.},
	number = {23},
	urldate = {2023-08-28},
	journal = {Medicine},
	author = {Shi, Linyu and Lin, Lifeng},
	month = jun,
	year = {2019},
	pmid = {31169736},
	pmcid = {PMC6571372},
	pages = {e15987},
	file = {PubMed Central Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\2M5MAKBG\\Shi and Lin - 2019 - The trim-and-fill method for publication bias pra.pdf:application/pdf},
}

@article{nuzzo_how_2015,
	title = {How scientists fool themselves – and how they can stop},
	volume = {526},
	copyright = {2015 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/526182a},
	doi = {10.1038/526182a},
	abstract = {Humans are remarkably good at self-deception. But growing concern about reproducibility is driving many researchers to seek ways to fight their own worst instincts.},
	language = {en},
	number = {7572},
	urldate = {2023-08-28},
	journal = {Nature},
	author = {Nuzzo, Regina},
	month = oct,
	year = {2015},
	note = {Number: 7572
Publisher: Nature Publishing Group},
	keywords = {Research management, Lab life},
	pages = {182--185},
	file = {Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\JN2VALUC\\Nuzzo - 2015 - How scientists fool themselves – and how they can .pdf:application/pdf;Snapshot:C\:\\Users\\shaonl\\Zotero\\storage\\FBUCFVH3\\526182a.html:text/html},
}

@article{stevens_replicability_2017,
	title = {Replicability and {Reproducibility} in {Comparative} {Psychology}},
	volume = {8},
	issn = {1664-1078},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5445189/},
	doi = {10.3389/fpsyg.2017.00862},
	urldate = {2023-08-28},
	journal = {Frontiers in Psychology},
	author = {Stevens, Jeffrey R.},
	month = may,
	year = {2017},
	pmid = {28603511},
	pmcid = {PMC5445189},
	pages = {862},
	file = {PubMed Central Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\3SPTCSN7\\Stevens - 2017 - Replicability and Reproducibility in Comparative P.pdf:application/pdf},
}

@article{nickerson_confirmation_1998,
	title = {Confirmation bias: {A} ubiquitous phenomenon in many guises},
	volume = {2},
	issn = {1089-2680},
	number = {2},
	journal = {Review of general psychology},
	author = {Nickerson, Raymond S},
	year = {1998},
	note = {Publisher: SAGE Publications Sage CA: Los Angeles, CA},
	pages = {175--220},
}

@book{mackay_memoirs_1869,
	title = {Memoirs of extraordinary popular delusions and the madness of crowds},
	publisher = {George Routledge and sons},
	author = {Mackay, Charles},
	year = {1869},
}

@article{pusztai_reproducibility_2013,
	title = {Reproducibility of research and preclinical validation: problems and solutions},
	volume = {10},
	issn = {1759-4774},
	number = {12},
	journal = {Nature Reviews Clinical Oncology},
	author = {Pusztai, Lajos and Hatzis, Christos and Andre, Fabrice},
	year = {2013},
	note = {Publisher: Nature Publishing Group UK London},
	pages = {720--724},
}

@article{fischhoff_hindsight_1975,
	title = {Hindsight is not equal to foresight: {The} effect of outcome knowledge on judgment under uncertainty.},
	volume = {1},
	issn = {1939-1277},
	number = {3},
	journal = {Journal of Experimental Psychology: Human perception and performance},
	author = {Fischhoff, Baruch},
	year = {1975},
	note = {Publisher: American Psychological Association},
	pages = {288},
}

@article{nosek_preregistration_2018,
	title = {Preregistration becoming the norm in psychological science},
	volume = {31},
	journal = {APS Observer},
	author = {Nosek, Brian A and Lindsay, D Stephen},
	year = {2018},
}

@misc{nasa_science_unmasking_2001,
	title = {Unmasking the {Face} on {Mars} {\textbar} {Science} {Mission} {Directorate}},
	url = {https://science.nasa.gov/science-news/science-at-nasa/2001/ast24may_1},
	urldate = {2023-08-29},
	journal = {Unmasking the Face on Mars},
	author = {{NASA Science}},
	month = may,
	year = {2001},
	file = {Unmasking the Face on Mars | Science Mission Directorate:C\:\\Users\\shaonl\\Zotero\\storage\\QUFGQ7WZ\\ast24may_1.html:text/html},
}

@misc{center_for_open_science_preregistration_nodate,
	title = {Preregistration},
	url = {https://www.cos.io/initiatives/prereg},
	abstract = {Preregistration},
	language = {en},
	urldate = {2023-08-29},
	author = {{Center for Open Science}},
	file = {Snapshot:C\:\\Users\\shaonl\\Zotero\\storage\\XU7QGKXW\\prereg.html:text/html},
}

@article{nosek_preregistration_2018-1,
	title = {The preregistration revolution},
	volume = {115},
	url = {https://www.pnas.org/doi/10.1073/pnas.1708274114},
	doi = {10.1073/pnas.1708274114},
	abstract = {Progress in science relies in part on generating hypotheses with existing observations and testing hypotheses with new observations. This distinction between postdiction and prediction is appreciated conceptually but is not respected in practice. Mistaking generation of postdictions with testing of predictions reduces the credibility of research findings. However, ordinary biases in human reasoning, such as hindsight bias, make it hard to avoid this mistake. An effective solution is to define the research questions and analysis plan before observing the research outcomes—a process called preregistration. Preregistration distinguishes analyses and outcomes that result from predictions from those that result from postdictions. A variety of practical strategies are available to make the best possible use of preregistration in circumstances that fall short of the ideal application, such as when the data are preexisting. Services are now available for preregistration across all disciplines, facilitating a rapid increase in the practice. Widespread adoption of preregistration will increase distinctiveness between hypothesis generation and hypothesis testing and will improve the credibility of research findings.},
	number = {11},
	urldate = {2023-08-29},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Nosek, Brian A. and Ebersole, Charles R. and DeHaven, Alexander C. and Mellor, David T.},
	month = mar,
	year = {2018},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {2600--2606},
	file = {Full Text PDF:C\:\\Users\\shaonl\\Zotero\\storage\\BEG9FGL8\\Nosek et al. - 2018 - The preregistration revolution.pdf:application/pdf},
}

@misc{schwarzkopf_its_2016,
	title = {It’s not the end of the world if your research gets ‘scooped’},
	url = {https://www.timeshighereducation.com/blog/its-not-end-world-if-your-research-gets-scooped},
	language = {en},
	urldate = {2023-08-29},
	journal = {Times Higher Education (THE)},
	author = {Schwarzkopf, Sam},
	month = apr,
	year = {2016},
}
