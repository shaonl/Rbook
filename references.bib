
@article{goodman_what_2016,
	title = {What does research reproducibility mean?},
	volume = {8},
	copyright = {Copyright © 2016, American Association for the Advancement of Science},
	issn = {1946-6234, 1946-6242},
	url = {https://stm.sciencemag.org/content/8/341/341ps12},
	doi = {10.1126/scitranslmed.aaf5027},
	abstract = {The language and conceptual framework of “research reproducibility” are nonstandard and unsettled across the sciences. In this Perspective, we review an array of explicit and implicit definitions of reproducibility and related terminology, and discuss how to avoid potential misunderstandings when these terms are used as a surrogate for “truth.”
The language and conceptual framework of “research reproducibility” are nonstandard and unsettled across the sciences.
The language and conceptual framework of “research reproducibility” are nonstandard and unsettled across the sciences.},
	language = {en},
	number = {341},
	urldate = {2020-09-18},
	journal = {Science Translational Medicine},
	author = {Goodman, Steven N. and Fanelli, Daniele and Ioannidis, John P. A.},
	month = jun,
	year = {2016},
	pmid = {27252173},
	note = {Publisher: American Association for the Advancement of Science
Section: Perspective},
	pages = {341ps12--341ps12},
	file = {Full Text PDF:C\:\\Users\\Shaon\\Zotero\\storage\\76TLAYG9\\Goodman et al. - 2016 - What does research reproducibility mean.pdf:application/pdf;Snapshot:C\:\\Users\\Shaon\\Zotero\\storage\\IVIR2CYS\\341ps12.html:text/html},
}

@article{ioannidis_why_2005,
	title = {Why {Most} {Published} {Research} {Findings} {Are} {False}},
	volume = {2},
	issn = {1549-1676},
	url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124},
	doi = {10.1371/journal.pmed.0020124},
	abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
	language = {en},
	number = {8},
	urldate = {2020-09-18},
	journal = {PLOS Medicine},
	author = {Ioannidis, John P. A.},
	month = aug,
	year = {2005},
	note = {Publisher: Public Library of Science},
	keywords = {Cancer risk factors, Finance, Genetic epidemiology, Genetics of disease, Metaanalysis, Randomized controlled trials, Research design, Schizophrenia, Clinical research design, Meta-analysis},
	pages = {e124},
	file = {Full Text PDF:C\:\\Users\\Shaon\\Zotero\\storage\\9NS8NJXD\\Ioannidis - 2005 - Why Most Published Research Findings Are False.pdf:application/pdf;Snapshot:C\:\\Users\\Shaon\\Zotero\\storage\\LBZDLL49\\article.html:text/html},
}

@article{wicherts_degrees_2016,
	title = {Degrees of {Freedom} in {Planning}, {Running}, {Analyzing}, and {Reporting} {Psychological} {Studies}: {A} {Checklist} to {Avoid} p-{Hacking}},
	volume = {7},
	issn = {1664-1078},
	shorttitle = {Degrees of {Freedom} in {Planning}, {Running}, {Analyzing}, and {Reporting} {Psychological} {Studies}},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2016.01832/full},
	doi = {10.3389/fpsyg.2016.01832},
	abstract = {The designing, collecting, analyzing, and reporting of psychological studies entail many choices that are often arbitrary. The opportunistic use of these so-called researcher degrees of freedom aimed at obtaining statistically significant results is problematic because it enhances the chances of false positive results and may inflate effect size estimates. In this review article, we present an extensive list of 34 degrees of freedom that researchers have in formulating hypotheses, and in designing, running, analyzing, and reporting of psychological research. The list can be used in research methods education, and as a checklist to assess the quality of preregistrations and to determine the potential for bias due to (arbitrary) choices in unregistered studies.},
	language = {English},
	urldate = {2020-09-18},
	journal = {Frontiers in Psychology},
	author = {Wicherts, Jelte M. and Veldkamp, Coosje L. S. and Augusteijn, Hilde E. M. and Bakker, Marjan and van Aert, Robbie C. M. and van Assen, Marcel A. L. M.},
	year = {2016},
	note = {tex.ids= wichertsDegreesFreedomPlanning2016a
publisher: Frontiers},
	keywords = {Bias, Experimental design (study designs), p-hacking, questionable research practices, Research methods education, significance chasing, Significance testing},
	file = {Full Text PDF:C\:\\Users\\Shaon\\Zotero\\storage\\NH67UCZJ\\Wicherts et al. - 2016 - Degrees of Freedom in Planning, Running, Analyzing.pdf:application/pdf;PubMed Central Full Text PDF:C\:\\Users\\Shaon\\Zotero\\storage\\ID3JIVHC\\Wicherts et al. - 2016 - Degrees of Freedom in Planning, Running, Analyzing.pdf:application/pdf},
}

@article{klein_investigating_2014,
	title = {Investigating {Variation} in {Replicability}},
	volume = {45},
	issn = {1864-9335},
	url = {https://econtent.hogrefe.com/doi/10.1027/1864-9335/a000178},
	doi = {10.1027/1864-9335/a000178},
	abstract = {Although replication is a central tenet of science, direct replications are rare in psychology. This research tested variation in the replicability of 13 classic and contemporary effects across 36 independent samples totaling 6,344 participants. In the aggregate, 10 effects replicated consistently. One effect – imagined contact reducing prejudice – showed weak support for replicability. And two effects – flag priming influencing conservatism and currency priming influencing system justification – did not replicate. We compared whether the conditions such as lab versus online or US versus international sample predicted effect magnitudes. By and large they did not. The results of this small sample of effects suggest that replicability is more dependent on the effect itself than on the sample and setting used to investigate the effect.},
	number = {3},
	urldate = {2020-09-18},
	journal = {Social Psychology},
	author = {Klein, Richard A. and Ratliff, Kate A. and Vianello, Michelangelo and Adams, Reginald B. and Bahník, Štěpán and Bernstein, Michael J. and Bocian, Konrad and Brandt, Mark J. and Brooks, Beach and Brumbaugh, Claudia Chloe and Cemalcilar, Zeynep and Chandler, Jesse and Cheong, Winnee and Davis, William E. and Devos, Thierry and Eisner, Matthew and Frankowska, Natalia and Furrow, David and Galliani, Elisa Maria and Hasselman, Fred and Hicks, Joshua A. and Hovermale, James F. and Hunt, S. Jane and Huntsinger, Jeffrey R. and IJzerman, Hans and John, Melissa-Sue and Joy-Gaba, Jennifer A. and Barry Kappes, Heather and Krueger, Lacy E. and Kurtz, Jaime and Levitan, Carmel A. and Mallett, Robyn K. and Morris, Wendy L. and Nelson, Anthony J. and Nier, Jason A. and Packard, Grant and Pilati, Ronaldo and Rutchick, Abraham M. and Schmidt, Kathleen and Skorinko, Jeanine L. and Smith, Robert and Steiner, Troy G. and Storbeck, Justin and Van Swol, Lyn M. and Thompson, Donna and van ‘t Veer, A. E. and Ann Vaughn, Leigh and Vranka, Marek and Wichman, Aaron L. and Woodzicka, Julie A. and Nosek, Brian A.},
	month = jan,
	year = {2014},
	note = {Publisher: Hogrefe Publishing},
	pages = {142--152},
	file = {Full Text PDF:C\:\\Users\\Shaon\\Zotero\\storage\\IZFPQ3RW\\Klein et al. - 2014 - Investigating Variation in Replicability.pdf:application/pdf;Snapshot:C\:\\Users\\Shaon\\Zotero\\storage\\JRM6774W\\a000178.html:text/html},
}

@article{benjamin_redefine_2018,
	title = {Redefine statistical significance},
	volume = {2},
	copyright = {2017 The Author(s)},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-017-0189-z},
	doi = {10.1038/s41562-017-0189-z},
	abstract = {We propose to change the default P-value threshold for statistical significance from 0.05 to 0.005 for claims of new discoveries.},
	language = {en},
	number = {1},
	urldate = {2020-09-18},
	journal = {Nature Human Behaviour},
	author = {Benjamin, Daniel J. and Berger, James O. and Johannesson, Magnus and Nosek, Brian A. and Wagenmakers, E.-J. and Berk, Richard and Bollen, Kenneth A. and Brembs, Björn and Brown, Lawrence and Camerer, Colin and Cesarini, David and Chambers, Christopher D. and Clyde, Merlise and Cook, Thomas D. and De Boeck, Paul and Dienes, Zoltan and Dreber, Anna and Easwaran, Kenny and Efferson, Charles and Fehr, Ernst and Fidler, Fiona and Field, Andy P. and Forster, Malcolm and George, Edward I. and Gonzalez, Richard and Goodman, Steven and Green, Edwin and Green, Donald P. and Greenwald, Anthony G. and Hadfield, Jarrod D. and Hedges, Larry V. and Held, Leonhard and Hua Ho, Teck and Hoijtink, Herbert and Hruschka, Daniel J. and Imai, Kosuke and Imbens, Guido and Ioannidis, John P. A. and Jeon, Minjeong and Jones, James Holland and Kirchler, Michael and Laibson, David and List, John and Little, Roderick and Lupia, Arthur and Machery, Edouard and Maxwell, Scott E. and McCarthy, Michael and Moore, Don A. and Morgan, Stephen L. and Munafó, Marcus and Nakagawa, Shinichi and Nyhan, Brendan and Parker, Timothy H. and Pericchi, Luis and Perugini, Marco and Rouder, Jeff and Rousseau, Judith and Savalei, Victoria and Schönbrodt, Felix D. and Sellke, Thomas and Sinclair, Betsy and Tingley, Dustin and Van Zandt, Trisha and Vazire, Simine and Watts, Duncan J. and Winship, Christopher and Wolpert, Robert L. and Xie, Yu and Young, Cristobal and Zinman, Jonathan and Johnson, Valen E.},
	month = jan,
	year = {2018},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {6--10},
	file = {Full Text PDF:C\:\\Users\\Shaon\\Zotero\\storage\\H7NCT6LB\\Benjamin et al. - 2018 - Redefine statistical significance.pdf:application/pdf;Snapshot:C\:\\Users\\Shaon\\Zotero\\storage\\BQ8LLZYW\\s41562-017-0189-z.html:text/html},
}

@article{plemmons_randomized_2020,
	title = {A randomized trial of a lab-embedded discourse intervention to improve research ethics},
	volume = {117},
	copyright = {Copyright © 2020 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by-nc-nd/4.0/This open access article is distributed under Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND).},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/117/3/1389},
	doi = {10.1073/pnas.1917848117},
	abstract = {We report a randomized trial of a research ethics training intervention designed to enhance ethics communication in university science and engineering laboratories, focusing specifically on authorship and data management. The intervention is a project-based research ethics curriculum that was designed to enhance the ability of science and engineering research laboratory members to engage in reason giving and interpersonal communication necessary for ethical practice. The randomized trial was fielded in active faculty-led laboratories at two US research-intensive institutions. Here, we show that laboratory members perceived improvements in the quality of discourse on research ethics within their laboratories and enhanced awareness of the relevance and reasons for that discourse for their work as measured by a survey administered over 4 mo after the intervention. This training represents a paradigm shift compared with more typical module-based or classroom ethics instruction that is divorced from the everyday workflow and practices within laboratories and is designed to cultivate a campus culture of ethical science and engineering research in the very work settings where laboratory members interact.},
	language = {en},
	number = {3},
	urldate = {2020-02-26},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Plemmons, Dena K. and Baranski, Erica N. and Harp, Kyle and Lo, David D. and Soderberg, Courtney K. and Errington, Timothy M. and Nosek, Brian A. and Esterling, Kevin M.},
	month = jan,
	year = {2020},
	pmid = {31919283},
	note = {Number: 3},
	keywords = {authorship, data management, randomized trial, research ethics},
	pages = {1389--1394},
}

@article{chang_is_2018,
	title = {Is {Economics} {Research} {Replicable}? {Sixty} {Published} {Papers} {From} {Thirteen} {Journals} {Say} “{Often} {Not}”},
	volume = {7},
	issn = {2164-5744, 2164-5760},
	shorttitle = {Is {Economics} {Research} {Replicable}?},
	url = {https://www.nowpublishers.com/article/Details/CFR-0053},
	doi = {10.1561/104.00000053},
	abstract = {Is Economics Research Replicable? Sixty Published Papers From Thirteen Journals Say “Often Not”},
	language = {English},
	urldate = {2020-02-24},
	journal = {Critical Finance Review},
	author = {Chang, Andrew C. and Li, Phillip},
	month = oct,
	year = {2018},
}

@article{king_replication_1995,
	title = {Replication, {Replication}},
	volume = {28},
	url = {https://gking.harvard.edu/files/gking/files/replication.pdf},
	journal = {PS: Political Science and Politics},
	author = {King, Gary},
	year = {1995},
	pages = {444--452},
}

@article{burlig_improving_2017,
	title = {Improving transparency in observational social science research: {A} pre-analysis plan approach},
	shorttitle = {Improving transparency in observational social science research},
	url = {https://osf.io/preprints/bitss/qemkz/},
	doi = {10.17605/OSF.IO/QEMKZ},
	abstract = {Social science research has undergone a credibility revolution, but these gains are at risk due to problematic research practices. Existing research on transparency has centered around randomized controlled trials, which constitute only a small fraction of research in economics. In this paper, I discuss three scenarios in which study preregistration can be credibly applied in non-experimental settings: cases where researchers collect their own data; prospective studies; and research using restricted-access data. Finally, I outline suggested contents for observational pre-analysis plans, and highlight where these plans should deviate from those designed for experimental research.},
	urldate = {2017-11-21},
	journal = {MetaArXiv},
	author = {Burlig, Fiona},
	month = oct,
	year = {2017},
}

@article{grant_consort-spi_2018,
	title = {{CONSORT}-{SPI} 2018 {Explanation} and {Elaboration}: guidance for reporting social and psychological intervention trials},
	volume = {19},
	issn = {1745-6215},
	shorttitle = {{CONSORT}-{SPI} 2018 {Explanation} and {Elaboration}},
	url = {https://doi.org/10.1186/s13063-018-2735-z},
	doi = {10.1186/s13063-018-2735-z},
	abstract = {The CONSORT (Consolidated Standards of Reporting Trials) Statement was developed to help biomedical researchers report randomised controlled trials (RCTs) transparently. We have developed an extension to the CONSORT 2010 Statement for social and psychological interventions (CONSORT-SPI 2018) to help behavioural and social scientists report these studies transparently.},
	number = {1},
	urldate = {2019-08-19},
	journal = {Trials},
	author = {Grant, Sean and Mayo-Wilson, Evan and Montgomery, Paul and Macdonald, Geraldine and Michie, Susan and Hopewell, Sally and Moher, David and Aber, J. Lawrence and Altman, Doug and Bhui, Kamaldeep and Booth, Andrew and Clark, David and Craig, Peter and Eisner, Manuel and Fraser, Mark W. and Gardner, Frances and Grant, Sean and Hedges, Larry and Hollon, Steve and Hopewell, Sally and Kaplan, Robert and Kaufmann, Peter and Konstantopoulos, Spyros and Macdonald, Geraldine and Mayo-Wilson, Evan and McLeroy, Kenneth and Michie, Susan and Mittman, Brian and Moher, David and Montgomery, Paul and Nezu, Arthur and Sherman, Lawrence and Sonuga-Barke, Edmund and Thomas, James and VandenBos, Gary and Waters, Elizabeth and West, Robert and Yaffe, Joanne and , on behalf of the CONSORT-SPI Group},
	month = jul,
	year = {2018},
	note = {Number: 1},
	pages = {406},
}

@article{ozier_replication_2019,
	title = {Replication {Redux}: {The} {Reproducibility} {Crisis} and the {Case} of {Deworming}},
	volume = {8835},
	url = {http://documents.worldbank.org/curated/en/118271556632669793/pdf/Replication-Redux-The-Reproducibility-Crisis-and-the-Case-of-Deworming.pdf},
	urldate = {2019-08-08},
	journal = {Policy  Research  Working  Paper},
	author = {Ozier, Owen},
	month = apr,
	year = {2019},
}

@misc{grant_proposed_2017,
	title = {Proposed {Reporting} {Items} for {Protocols} of {Social} {Intervention} {Trials}},
	url = {https://osf.io/x6mkb/},
	publisher = {Open Society Framework},
	author = {Grant, Sean},
	year = {2017},
	keywords = {Reporting guidelines, RT2},
}

@article{simera_transparent_2010,
	title = {Transparent and accurate reporting increases reliability, utility, and impact of your research: reporting guidelines and the {EQUATOR} {Network}},
	volume = {8},
	issn = {1741-7015},
	shorttitle = {Transparent and accurate reporting increases reliability, utility, and impact of your research},
	url = {https://doi.org/10.1186/1741-7015-8-24},
	doi = {10.1186/1741-7015-8-24},
	abstract = {Although current electronic methods of scientific publishing offer increased opportunities for publishing all research studies and describing them in sufficient detail, health research literature still suffers from many shortcomings. These shortcomings seriously undermine the value and utility of the literature and waste scarce resources invested in the research. In recent years there have been several positive steps aimed at improving this situation, such as a strengthening of journals' policies on research publication and the wide requirement to register clinical trials.},
	number = {1},
	urldate = {2019-08-03},
	journal = {BMC Medicine},
	author = {Simera, Iveta and Moher, David and Hirst, Allison and Hoey, John and Schulz, Kenneth F. and Altman, Douglas G.},
	month = apr,
	year = {2010},
	note = {Number: 1},
	pages = {24},
}

@book{cooper_handbook_2019,
	title = {The {Handbook} of {Research} {Synthesis} and {Meta}-{Analysis}, {Second} {Edition} {\textbar} {RSF}},
	url = {https://www.russellsage.org/publications/handbook-research-synthesis-and-meta-analysis-second-edition},
	urldate = {2019-08-03},
	author = {Cooper, Harris and Hedges, Larry V. and Valentine, Jeffrey},
	month = aug,
	year = {2019},
}

@article{russo_how_2007,
	title = {How to {Review} a {Meta}-analysis},
	volume = {3},
	issn = {1554-7914},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3099299/},
	abstract = {Meta-analysis is a systematic review of a focused topic in the literature that provides a quantitative estimate for the effect of a treatment intervention or exposure. The key to designing a high quality meta-analysis is to identify an area where the effect of the treatment or exposure is uncertain and where a relatively homogenous body of literature exists. The techniques used in meta-analysis provide a structured and standardized approach for analyzing prior findings in a specific topic in the literature. Meta-analysis findings may not only be quantitative but also may be qualitative and reveal the biases, strengths, and weaknesses of existing studies. The results of a meta-analysis can be used to form treatment recommendations or to provide guidance in the design of future clinical trials.},
	number = {8},
	urldate = {2019-08-03},
	journal = {Gastroenterology \& Hepatology},
	author = {Russo, Mark W.},
	month = aug,
	year = {2007},
	pmid = {21960873},
	pmcid = {PMC3099299},
	note = {Number: 8},
	pages = {637--642},
}

@article{ioannidis_meta-research_2015,
	title = {Meta-research: {Evaluation} and {Improvement} of {Research} {Methods} and {Practices}},
	volume = {13},
	issn = {1545-7885},
	shorttitle = {Meta-research},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002264},
	doi = {10.1371/journal.pbio.1002264},
	abstract = {As the scientific enterprise has grown in size and diversity, we need empirical evidence on the research process to test and apply interventions that make it more efficient and its results more reliable. Meta-research is an evolving scientific discipline that aims to evaluate and improve research practices. It includes thematic areas of methods, reporting, reproducibility, evaluation, and incentives (how to do, report, verify, correct, and reward science). Much work is already done in this growing field, but efforts to-date are fragmented. We provide a map of ongoing efforts and discuss plans for connecting the multiple meta-research efforts across science worldwide.},
	language = {en},
	number = {10},
	urldate = {2019-08-03},
	journal = {PLOS Biology},
	author = {Ioannidis, John P. A. and Fanelli, Daniele and Dunne, Debbie Drake and Goodman, Steven N.},
	month = oct,
	year = {2015},
	note = {Number: 10},
	keywords = {Research design, Publication ethics, Research funding, Research integrity, Research monitoring, Research quality assessment, Research validity, Social research},
	pages = {e1002264},
}

@article{kaiser_protecting_2009,
	title = {Protecting {Respondent} {Confidentiality} in {Qualitative} {Research}},
	volume = {19},
	issn = {1049-7323},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2805454/},
	doi = {10.1177/1049732309350879},
	abstract = {For qualitative researchers, maintaining respondent confidentiality while presenting rich, detailed accounts of social life presents unique challenges. These challenges are not adequately addressed in the literature on research ethics and research methods. Using an example from a study of breast cancer survivors, I argue that by carefully considering the audience for one’s research and by re-envisioning the informed consent process, qualitative researchers can avoid confidentiality dilemmas that might otherwise lead them not to report rich, detailed data.},
	number = {11},
	urldate = {2019-08-03},
	journal = {Qualitative health research},
	author = {Kaiser, Karen},
	month = nov,
	year = {2009},
	pmid = {19843971},
	pmcid = {PMC2805454},
	note = {Number: 11},
	pages = {1632--1641},
}

@article{goodman_ten_2014,
	title = {Ten {Simple} {Rules} for the {Care} and {Feeding} of {Scientific} {Data}},
	volume = {10},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003542},
	doi = {10.1371/journal.pcbi.1003542},
	language = {en},
	number = {4},
	urldate = {2019-08-03},
	journal = {PLOS Computational Biology},
	author = {Goodman, Alyssa and Pepe, Alberto and Blocker, Alexander W. and Borgman, Christine L. and Cranmer, Kyle and Crosas, Merce and Stefano, Rosanne Di and Gil, Yolanda and Groth, Paul and Hedstrom, Margaret and Hogg, David W. and Kashyap, Vinay and Mahabal, Ashish and Siemiginowska, Aneta and Slavkovic, Aleksandra},
	month = apr,
	year = {2014},
	note = {Number: 4},
	keywords = {Archives, Data management, Data visualization, Moons, Open source software, Scientists, Software tools, Telescopes},
	pages = {e1003542},
}

@article{playford_administrative_2016,
	title = {Administrative social science data: {The} challenge of reproducible research:},
	shorttitle = {Administrative social science data},
	url = {https://journals.sagepub.com/stoken/default+domain/10.1177/2053951716684143/full?utm_source=Adestra&utm_medium=email&utm_content=Administrative+Social+Science+Data%3A+The+Challenge+of+Reproducible+Research&utm_campaign=Methods17aug&utm_term=},
	doi = {10.1177/2053951716684143},
	abstract = {Powerful new social science data resources are emerging. One particularly important source is administrative data, which were originally collected for organisat...},
	language = {en},
	urldate = {2019-08-03},
	journal = {Big Data \& Society},
	author = {Playford, Christopher J. and Gayle, Vernon and Connelly, Roxanne and Gray, Alasdair JG},
	month = dec,
	year = {2016},
}

@article{sturdy_opening_2017,
	title = {Opening {Up} {Evaluation} {Microdata}: {Balancing} {Risks} and {Benefits} of {Research} {Transparency}},
	shorttitle = {Opening {Up} {Evaluation} {Microdata}},
	url = {https://osf.io/s67my},
	doi = {10.31222/osf.io/s67my},
	abstract = {The Millennium Challenge Corporation (MCC), an independent U.S. Government foreign aid agency, uses a data-based approach to select country partners, and design, monitor, and evaluate programs. With a commitment to transparency built into the MCC model as a means for accountability and learning, the agency has grappled with the ethical, legal, and practical issues related to public release of its evaluation microdata. In 2013, MCC identified key objectives for public release of its evaluation microdata: facilitating verification of evaluation results, maximizing data usability, and minimizing risk to survey respondents. The agency implemented a series of actions, including establishing a microdata dissemination mechanism; data protection principles; an internal Disclosure Review Board; procedures for restricted-access to low-risk data; and allocating staffing and resources to implement the data protection principles. The paper discusses MCC’s experience and concludes with a set of lessons learned about finding balance between benefits and risks when releasing evaluation microdata.},
	urldate = {2019-08-03},
	journal = {MetaArXiv},
	author = {Sturdy, Jennifer and Burch, Stephanie and Hanson, Heather and Molyneaux, Jack},
	month = mar,
	year = {2017},
}

@misc{zandbergen_ensuring_2014,
	type = {Research article},
	title = {Ensuring {Confidentiality} of {Geocoded} {Health} {Data}: {Assessing} {Geographic} {Masking} {Strategies} for {Individual}-{Level} {Data}},
	shorttitle = {Ensuring {Confidentiality} of {Geocoded} {Health} {Data}},
	url = {https://www.hindawi.com/journals/amed/2014/567049/},
	abstract = {Public health datasets increasingly use geographic identifiers such as an individual’s address. Geocoding these addresses often provides new insights since it becomes possible to examine spatial patterns and associations. Address information is typically considered confidential and is therefore not released or shared with others. Publishing maps with the locations of individuals, however, may also breach confidentiality since addresses and associated identities can be discovered through reverse geocoding. One commonly used technique to protect confidentiality when releasing individual-level geocoded data is geographic masking. This typically consists of applying a certain amount of random perturbation in a systematic manner to reduce the risk of reidentification. A number of geographic masking techniques have been developed as well as methods to quantity the risk of reidentification associated with a particular masking method. This paper presents a review of the current state-of-the-art in geographic masking, summarizing the various methods and their strengths and weaknesses. Despite recent progress, no universally accepted or endorsed geographic masking technique has emerged. Researchers on the other hand are publishing maps using geographic masking of confidential locations. Any researcher publishing such maps is advised to become familiar with the different masking techniques available and their associated reidentification risks.},
	language = {en},
	urldate = {2019-08-03},
	journal = {Advances in Medicine},
	author = {Zandbergen, Paul A.},
	year = {2014},
	doi = {10.1155/2014/567049},
}

@article{olken_promises_2015,
	title = {Promises and {Perils} of {Pre}-analysis {Plans}},
	volume = {29},
	issn = {0895-3309},
	url = {https://economics.mit.edu/files/10654},
	doi = {10.1257/jep.29.3.61},
	abstract = {The purpose of this paper is to help think through the advantages and costs of rigorous pre-specification of statistical analysis plans in economics. 
A pre-analysis plan pre-specifies in a precise way the analysis to be run before examining the data. 
A researcher can specify variables, data cleaning procedures, regression specifications, and so on. 
If the regressions are pre-specified in advance and researchers are required to report all the results they pre-specify, data-mining problems are greatly reduced. 
I begin by laying out the basics of what a statistical analysis plan actually contains so those researchers unfamiliar with it can better understand how it is done. 
In so doing, I have drawn both on standards used in clinical trials, which are clearly specified by the Food and Drug Administration, as well as my own practical experience from writing these plans in economics contexts. 
I then lay out some of the advantages of pre-specified analysis plans, both for the scientific community as a whole and also for the researcher. 
I also explore some of the limitations and costs of such plans. 
I then review a few pieces of evidence that suggest that, in many contexts, the benefits of using pre-specified analysis plans may not be as high as one might have expected initially. 
For the most part, I will focus on the relatively narrow issue of pre-analysis for randomized controlled trials.},
	language = {en},
	number = {3},
	urldate = {2019-08-03},
	journal = {Journal of Economic Perspectives},
	author = {Olken, Benjamin A.},
	month = sep,
	year = {2015},
	note = {Number: 3},
	keywords = {Cluster Analysis, Factor Models, Multiple or Simultaneous Equation Models: Classification Methods, Principal Components},
	pages = {61--80},
}

@article{lin_standard_2016,
	title = {Standard {Operating} {Procedures}: {A} {Safety} {Net} for {Pre}-{Analysis} {Plans}},
	volume = {49},
	issn = {1049-0965, 1537-5935},
	shorttitle = {Standard {Operating} {Procedures}},
	url = {https://www.stat.berkeley.edu/~winston/sop-safety-net.pdf},
	doi = {10.1017/S1049096516000810},
	abstract = {Across the social sciences, growing concerns about research transparency have led to calls for pre-analysis plans (PAPs) that specify in advance how researchers intend to analyze the data they are about to gather. PAPs promote transparency and credibility by helping readers distinguish between exploratory and confirmatory analyses. However, PAPs are time-consuming to write and may fail to anticipate contingencies that arise in the course of data collection. This article proposes the use of “standard operating procedures” (SOPs)—default practices to guide decisions when issues arise that were not anticipated in the PAP. We offer an example of an SOP that can be adapted by other researchers seeking a safety net to support their PAPs.},
	language = {en},
	number = {3},
	urldate = {2019-08-03},
	journal = {PS: Political Science \& Politics},
	author = {Lin, Winston and Green, Donald P.},
	month = jul,
	year = {2016},
	note = {Number: 3},
	pages = {495--500},
}

@article{casey_reshaping_2012,
	title = {Reshaping {Institutions}: {Evidence} on {Aid} {Impacts} {Using} a {Preanalysis} {Plan}*},
	volume = {127},
	issn = {0033-5533},
	shorttitle = {Reshaping {Institutions}},
	url = {https://www.povertyactionlab.org/sites/default/files/publications/45_reshaping%20institutions%20QJE.pdf},
	doi = {10.1093/qje/qje027},
	abstract = {Abstract.   Despite their importance, there is limited evidence on how institutions can be strengthened. Evaluating the effects of specific reforms is complicat},
	language = {en},
	number = {4},
	urldate = {2019-08-03},
	journal = {The Quarterly Journal of Economics},
	author = {Casey, Katherine and Glennerster, Rachel and Miguel, Edward},
	month = nov,
	year = {2012},
	note = {Number: 4},
	pages = {1755--1812},
}

@unpublished{dellavigna_predicting_2016,
	type = {Working {Paper}},
	title = {Predicting {Experimental} {Results}: {Who} {Knows} {What}?},
	shorttitle = {Predicting {Experimental} {Results}},
	url = {http://www.nber.org/papers/w22566},
	abstract = {Academic experts frequently recommend policies and treatments. But how well do they anticipate the impact of different treatments? And how do their predictions compare to the predictions of non-experts? We analyze how 208 experts forecast the results of 15 treatments involving monetary and non-monetary motivators in a real-effort task. We compare these forecasts to those made by PhD students and non-experts: undergraduates, MBAs, and an online sample. We document seven main results. First, the average forecast of experts predicts quite well the experimental results. Second, there is a strong wisdom-of-crowds effect: the average forecast outperforms 96 percent of individual forecasts. Third, correlates of expertise—citations, academic rank, field, and contextual experience–do not improve forecasting accuracy. Fourth, experts as a group do better than non-experts, but not if accuracy is defined as rank ordering treatments. Fifth, measures of effort, confidence, and revealed ability are predictive of forecast accuracy to some extent, especially for non-experts. Sixth, using these measures we identify `superforecasters' among the non-experts who outperform the experts out of sample. Seventh, we document that these results on forecasting accuracy surprise the forecasters themselves. We present a simple model that organizes several of these results and we stress the implications for the collection of forecasts of future experimental results.},
	urldate = {2019-08-02},
	author = {DellaVigna, Stefano and Pope, Devin},
	month = aug,
	year = {2016},
	doi = {10.3386/w22566},
}

@article{wood_push_2018,
	title = {Push button replication: {Is} impact evaluation evidence for international development verifiable?},
	volume = {13},
	issn = {1932-6203},
	shorttitle = {Push button replication},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0209416},
	doi = {10.1371/journal.pone.0209416},
	abstract = {Objective Empirical research that cannot be reproduced using the original dataset and software code (replication files) creates a credibility challenge, as it means those published findings are not verifiable. This study reports the results of a research audit exercise, known as the push button replication project, that tested a sample of studies that use similar empirical methods but span a variety of academic fields. Methods We developed and piloted a detailed protocol for conducting push button replication and determining the level of comparability of these replication findings to original findings. We drew a sample of articles from the ten journals that published the most impact evaluations from low- and middle-income countries from 2010 through 2012. This set includes health, economics, and development journals. We then selected all articles in these journals published in 2014 that meet the same inclusion criteria and implemented the protocol on the sample. Results Of the 109 articles in our sample, only 27 are push button replicable, meaning the provided code run on the provided dataset produces comparable findings for the key results in the published article. The authors of 59 of the articles refused to provide replication files. Thirty of these 59 articles were published in journals that had replication file requirements in 2014, meaning these articles are non-compliant with their journal requirements. For the remaining 23 of the 109 articles, we confirmed that three had proprietary data, we received incomplete replication files for 15, and we found minor differences in the replication results for five. Conclusion The findings presented here reveal that many economics, development, and public health researchers are a long way from adopting the norm of open research. Journals do not appear to be playing a strong role in ensuring the availability of replication files.},
	language = {en},
	number = {12},
	urldate = {2019-08-02},
	journal = {PLOS ONE},
	author = {Wood, Benjamin D. K. and Müller, Rui and Brown, Annette N.},
	month = dec,
	year = {2018},
	note = {Number: 12},
	keywords = {Development economics, Economic development, Economics, Health economics, Open data, Public and occupational health, Replication studies, Scientific publishing},
	pages = {e0209416},
}

@article{gertler_how_2018,
	title = {How to make replication the norm},
	volume = {554},
	copyright = {2018 Nature},
	url = {http://www.nature.com/articles/d41586-018-02108-9},
	doi = {10.1038/d41586-018-02108-9},
	abstract = {The publishing system builds in resistance to replication. Paul Gertler, Sebastian Galiani and Mauricio Romero surveyed economics journals to find out how to fix it.},
	language = {EN},
	number = {7693},
	urldate = {2019-08-02},
	journal = {Nature},
	author = {Gertler, Paul and Galiani, Sebastian and Romero, Mauricio},
	month = feb,
	year = {2018},
	note = {Number: 7693},
	pages = {417--419},
}

@article{dafoe_science_2014,
	title = {Science {Deserves} {Better}: {The} {Imperative} to {Share} {Complete} {Replication} {Files}},
	volume = {47},
	issn = {1049-0965, 1537-5935},
	shorttitle = {Science {Deserves} {Better}},
	url = {https://www.cambridge.org/core/journals/ps-political-science-and-politics/article/science-deserves-better-the-imperative-to-share-complete-replication-files/C19AE087642810DD9C0C83BF8D0908A9},
	doi = {10.1017/S104909651300173X},
	abstract = {In April 2013, a controversy arose when a working paper (Herndon, Ash, and Pollin 2013) claimed to show serious errors in a highly cited and influential economics paper by Carmen Reinhart and Kenneth Rogoff (2010). The Reinhart and Rogoff paper had come to serve as authoritative evidence in elite conversations (Krugman 2013) that high levels of debt, especially above the “90 percent [debt/GDP] threshold” (Reinhart and Rogoff 2010, 577), posed a risk to economic growth. Much of the coverage of this controversy focused on an error that was a “perfect made-for-TV mistake” (Stevenson and Wolfers 2013) involving a simple error in the formula used in their Excel calculations. The real story here, however, is that it took three years for this error and other issues to be discovered because replication files were not publicly available, nor were they provided to scholars when asked. If professional norms or the American Economic Review had required that authors publish replication files, this debate would be advanced by three years and discussions about austerity policies would have been based on a more clear-sighted appraisal of the evidence.},
	language = {en},
	number = {1},
	urldate = {2019-08-02},
	journal = {PS: Political Science \& Politics},
	author = {Dafoe, Allan},
	month = jan,
	year = {2014},
	note = {Number: 1},
	pages = {60--66},
}

@article{hamermesh_viewpoint_2007,
	title = {Viewpoint: {Replication} in economics},
	volume = {40},
	issn = {1540-5982},
	shorttitle = {Viewpoint},
	url = {https://www.semanticscholar.org/paper/Viewpoint%3A-Replication-in-Economics-Hamermesh/7f81fa275aca9d397ee124b9da910f7891554fa8},
	doi = {10.1111/j.1365-2966.2007.00428.x},
	abstract = {Abstract. This examination of the role and potential for replication in economics points out the paucity of both pure replication – checking on others' published papers using their data – and scientific replication – using data representing different populations in one's own work or in a comment. Several controversies in empirical economics are used to illustrate how and how not to behave when replicating others' work. The incentives for replication are examined, and proposals aimed at journal editors and authors are advanced that might stimulate an activity that most economists applaud but few perform.},
	language = {en},
	number = {3},
	urldate = {2019-08-02},
	journal = {Canadian Journal of Economics/Revue canadienne d'économique},
	author = {Hamermesh, Daniel S.},
	year = {2007},
	note = {Number: 3},
	keywords = {A14, B41, C59},
	pages = {715--733},
}

@article{bowers_six_2019,
	title = {Six  steps  to  a  better  relationship  with your future self},
	volume = {18},
	url = {http://www.jakebowers.org/PAPERS/tpm_v18_n2.pdf},
	number = {2},
	urldate = {2019-08-02},
	journal = {The Political Methodologist},
	author = {Bowers, Jake},
	month = aug,
	year = {2019},
	note = {Number: 2},
}

@book{gandrud_reproducible_2015,
	title = {Reproducible {Research} with {R} and {R} {Studio} {Second} {Edition}},
	url = {https://englianhu.files.wordpress.com/2016/01/reproducible-research-with-r-and-studio-2nd-edition.pdf},
	publisher = {CRC Press},
	author = {Gandrud, Christopher},
	year = {2015},
}

@article{burlig_panel_2017,
	title = {Panel {Data} and {Experimental} {Design}},
	url = {https://osf.io/d5eud},
	doi = {https://doi.org/10.31222/osf.io/d5eud},
	abstract = {How should researchers design experiments to detect treatment effects with panel data? In this paper, we derive analytical expressions for the variance of panel estimators under non-i.i.d. error structures, which inform power calculations in panel data settings. Using Monte Carlo simulation, we demonstrate that, with correlated errors, traditional methods for experimental design result in experiments that are incorrectly powered with proper inference. Failing to account for serial correlation yields overpowered experiments in short panels and underpowered experiments in long panels. Using both data from a randomized experiment in China and a high-frequency dataset of U.S. electricity consumption, we show that these results hold in real-world settings. Our theoretical results enable us to achieve correctly powered experiments in both simulated and real data. This paper provides researchers with the tools to design well-powered experiments in panel data settings.},
	urldate = {2019-08-02},
	journal = {MetaArXiv},
	author = {Burlig, Fiona and Preonas, Louis and Woerman, Matt},
	month = mar,
	year = {2017},
	doi = {10.31222/osf.io/d5eud},
}

@article{coville_how_2017,
	title = {How {Often} {Should} {We} {Believe} {Positive} {Results}? {Assessing} the {Credibility} of {Research} {Findings} in {Development} {Economics}},
	shorttitle = {How {Often} {Should} {We} {Believe} {Positive} {Results}?},
	url = {https://osf.io/5nsh3},
	doi = {https://doi.org/10.31222/osf.io/5nsh3},
	abstract = {Under-powered studies combined with low prior beliefs about intervention effects increase the chances that a positive result is overstated. We collect prior beliefs about intervention impacts from 125 experts to estimate the false positive and false negative report probabilities (FPRP and FNRP) as well as Type S (sign) and Type M (magnitude) errors for studies in development economics. We find that the large majority of studies in our sample are generally credible. We discuss how more systematic collection and use of prior expectations could help improve the literature.},
	urldate = {2019-08-02},
	journal = {MetaArXiv},
	author = {Coville, Aidan and Vivalt, Eva},
	month = aug,
	year = {2017},
	doi = {10.31222/osf.io/5nsh3},
}

@article{button_power_2013,
	title = {Power failure: why small sample size undermines the reliability of neuroscience},
	volume = {14},
	copyright = {2013 Nature Publishing Group},
	issn = {1471-0048},
	shorttitle = {Power failure},
	url = {https://www.nature.com/articles/nrn3475},
	doi = {10.1038/nrn3475},
	abstract = {A study with low statistical power has a reduced chance of detecting a true effect, but it is less well appreciated that low power also reduces the likelihood that a statistically significant result reflects a true effect. Here, we show that the average statistical power of studies in the neurosciences is very low. The consequences of this include overestimates of effect size and low reproducibility of results. There are also ethical dimensions to this problem, as unreliable research is inefficient and wasteful. Improving reproducibility in neuroscience is a key priority and requires attention to well-established but often ignored methodological principles.},
	language = {en},
	number = {5},
	urldate = {2019-08-02},
	journal = {Nature Reviews Neuroscience},
	author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munafò, Marcus R.},
	month = may,
	year = {2013},
	note = {Number: 5},
	pages = {365--376},
}

@article{gelman_garden_2019,
	title = {The {Garden} of {Forking} {Paths}: {Why} {Multiple} {Comparisons} {Can} {Be} a {Problem}, {Even} {When} {There} {Is} {No} ‘{Fishing} {Expedition}’ or ‘p-{Hacking}’ and the {Research} {Hypothesis} {Was} {Posited} {Ahead} of {Time}},
	url = {http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf},
	urldate = {2019-08-02},
	journal = {Unpublished},
	author = {Gelman, Andrew and Loken, Eric},
	month = aug,
	year = {2019},
}

@article{lenz_achieving_2017,
	title = {Achieving {Statistical} {Significance} with {Covariates} and without {Transparency}},
	url = {https://osf.io/preprints/metaarxiv/s42ba/},
	doi = {10.31222/osf.io/s42ba},
	abstract = {How often do articles depend on suppression effects for their findings? How often do they disclose this fact? By suppression effects, we mean covariate-induced increases in effect sizes. Researchers generally scrutinize suppression effects as they want reassurance that researchers have a strong explanation for effect size increases, especially when the statistical significance of the key finding depends on them. In this research note, we report a startling finding: Almost 40\% of observational articles in a leading journal depend on suppression effects for statistical significance and almost 0\% disclose this fact. These findings may point to a hole in the review process: journals are accepting articles that depend on suppression effects without readers, reviewers, or editors being made aware.},
	urldate = {2019-08-02},
	author = {Lenz, Gabriel and Sahn, Alexander},
	month = apr,
	year = {2017},
}

@article{simmons_false-positive_2011,
	title = {False-{Positive} {Psychology}: {Undisclosed} {Flexibility} in {Data} {Collection} and {Analysis} {Allows} {Presenting} {Anything} as {Significant}},
	volume = {22},
	issn = {0956-7976},
	shorttitle = {False-{Positive} {Psychology}},
	url = {https://doi.org/10.1177/0956797611417632},
	doi = {10.1177/0956797611417632},
	abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists’ nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
	language = {en},
	number = {11},
	urldate = {2019-08-02},
	journal = {Psychological Science},
	author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
	month = nov,
	year = {2011},
	note = {Number: 11},
	pages = {1359--1366},
}

@article{simonsohn_p-curve_2013,
	title = {P-{Curve}: {A} {Key} to the {File} {Drawer}},
	shorttitle = {P-{Curve}},
	url = {https://papers.ssrn.com/abstract=2256237},
	abstract = {Because scientists tend to report only studies (publication bias) or analyses (p-hacking) that “work”, readers must ask, “Are these effects true, or do they merely reflect selective reporting?” We introduce p-curve as a way to answer this question. P-curve is the distribution of statistically significant p-values for a set of studies (ps {\textless} .05). Because only true effects are expected to generate right-skewed p-curves – containing more low (.01s) than high (.04s) significant p-values – only right-skewed p-curves are diagnostic of evidential value. By telling us whether we can rule out selective reporting as the sole explanation for a set of findings, p-curve offers a solution to the age-old inferential problems caused by file-drawers of failed studies and analyses.},
	language = {en},
	urldate = {2019-08-02},
	journal = {Journal of Experimental Psychology},
	author = {Simonsohn, Uri and Nelson, Leif D. and Simmons, Joseph P.},
	month = apr,
	year = {2013},
	keywords = {p-hacking, Hypothesis Testing, Psychology, Scientific Communication, Statistics, decision making, file drawer, judgment, publication bias, science, statistics},
}

@article{gerber_publication_2008,
	title = {Publication {Bias} in {Empirical} {Sociological} {Research}: {Do} {Arbitrary} {Significance} {Levels} {Distort} {Published} {Results}?},
	volume = {37},
	issn = {0049-1241},
	shorttitle = {Publication {Bias} in {Empirical} {Sociological} {Research}},
	url = {https://doi.org/10.1177/0049124108318973},
	doi = {10.1177/0049124108318973},
	abstract = {Despite great attention to the quality of research methods in individual studies, if publication decisions of journals are a function of the statistical significance of research findings, the published literature as a whole may not produce accurate measures of true effects. This article examines the two most prominent sociology journals (the American Sociological Review and the American Journal of Sociology) and another important though less influential journal (The Sociological Quarterly) for evidence of publication bias. The effect of the .05 significance level on the pattern of published findings is examined using a ``caliper'' test, and the hypothesis of no publication bias can be rejected at approximately the 1 in 10 million level. Findings suggest that some of the results reported in leading sociology journals may be misleading and inaccurate due to publication bias. Some reasons for publication bias and proposed reforms to reduce its impact on research are also discussed.},
	language = {en},
	number = {1},
	urldate = {2019-08-02},
	journal = {Sociological Methods \& Research},
	author = {Gerber, Alan S. and Malhotra, Neil},
	month = aug,
	year = {2008},
	note = {Number: 1},
	pages = {3--30},
}

@book{national_academies_of_sciences_reproducibility_2019,
	title = {Reproducibility and {Replicability} in {Science}},
	isbn = {978-0-309-48613-2},
	url = {https://www.nap.edu/catalog/25303/reproducibility-and-replicability-in-science},
	abstract = {Download a PDF of "Reproducibility and Replicability in Science" by the National Academies of Sciences, Engineering, and Medicine for free.},
	language = {en},
	urldate = {2019-08-02},
	author = {National Academies of Sciences, Engineering},
	month = may,
	year = {2019},
	doi = {10.17226/25303},
}

@book{national_academies_of_sciences_fostering_2017,
	title = {Fostering {Integrity} in {Research}},
	isbn = {978-0-309-39125-2},
	url = {https://www.nap.edu/catalog/21896/fostering-integrity-in-research},
	abstract = {Download a PDF of "Fostering Integrity in Research" by the National Academies of Sciences, Engineering, and Medicine for free.},
	language = {en},
	urldate = {2019-08-02},
	author = {National Academies of Sciences, Engineering},
	month = apr,
	year = {2017},
	doi = {10.17226/21896},
}

@article{akerlof_persistence_2018,
	title = {Persistence of false paradigms in low-power sciences},
	volume = {115},
	copyright = {Copyright © 2018 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by-nc-nd/4.0/This open access article is distributed under Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND).},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/115/52/13228},
	doi = {10.1073/pnas.1816454115},
	abstract = {We develop a model describing how false paradigms may persist, hindering scientific progress. The model features two paradigms, one describing reality better than the other. Tenured scientists display homophily: They favor tenure candidates who adhere to their paradigm. As in statistics, power is the probability (absent any bias) of denying tenure to scientists adhering to the false paradigm. The model shows that because of homophily, when power is low, the false paradigm may prevail. Then, only an increase in power can ignite convergence to the true paradigm. Historical case studies suggest that low power comes either from lack of empirical evidence or from reluctance to base tenure decisions on available evidence.},
	language = {en},
	number = {52},
	urldate = {2019-08-02},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Akerlof, George A. and Michaillat, Pascal},
	month = dec,
	year = {2018},
	pmid = {30523117},
	note = {Number: 52},
	keywords = {homophily, paradigms, power, scientific progress, tenure},
	pages = {13228--13233},
}

@article{stodden_enhancing_2016,
	title = {Enhancing reproducibility for computational methods},
	volume = {354},
	copyright = {Copyright © 2016, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {http://web.stanford.edu/~vcs/papers/ERCM2016-STODDEN.pdf},
	doi = {10.1126/science.aah6168},
	abstract = {Data, code, and workflows should be available and cited
Data, code, and workflows should be available and cited},
	language = {en},
	number = {6317},
	urldate = {2019-08-02},
	journal = {Science},
	author = {Stodden, Victoria and McNutt, Marcia and Bailey, David H. and Deelman, Ewa and Gil, Yolanda and Hanson, Brooks and Heroux, Michael A. and Ioannidis, John P. A. and Taufer, Michela},
	month = dec,
	year = {2016},
	pmid = {27940837},
	note = {Number: 6317},
	pages = {1240--1241},
}

@article{christensen_transparency_2018,
	title = {Transparency, {Reproducibility}, and the {Credibility} of {Economics} {Research}},
	volume = {56},
	issn = {0022-0515},
	url = {https://www.aeaweb.org/articles?id=10.1257/jel.20171350},
	doi = {10.1257/jel.20171350},
	abstract = {There is growing interest in enhancing research transparency and reproducibility in economics and other scientific fields. We survey existing work on these topics within economics, and discuss the evidence suggesting that publication bias, inability to replicate, and specification searching remain widespread in the discipline. We next discuss recent progress in this area, including through improved research design, study registration and pre-analysis plans, disclosure standards, and open sharing of data and materials, drawing on experiences in both economics and other social sciences. We discuss areas where consensus is emerging on new practices, as well as approaches that remain controversial, and speculate about the most effective ways to make economics research more credible in the future.},
	language = {en},
	number = {3},
	urldate = {2019-08-02},
	journal = {Journal of Economic Literature},
	author = {Christensen, Garret and Miguel, Edward},
	month = sep,
	year = {2018},
	note = {Number: 3},
	keywords = {Market for Economists, Methodological Issues: General, Higher Education, Research Institutions, Role of Economics, Role of Economists},
	pages = {920--980},
}

@article{rosenthal_file_1979,
	title = {The file drawer problem and tolerance for null results},
	volume = {86},
	issn = {1939-1455(ELECTRONIC),0033-2909(PRINT)},
	doi = {10.1037/0033-2909.86.3.638},
	abstract = {For any given research area, one cannot tell how many studies have been conducted but never reported. The extreme view of the "file drawer problem" is that journals are filled with the 5\% of the studies that show Type I errors, while the file drawers are filled with the 95\% of the studies that show nonsignificant results. Quantitative procedures for computing the tolerance for filed and future null results are reported and illustrated, and the implications are discussed. (15 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {3},
	journal = {Psychological Bulletin},
	author = {Rosenthal, Robert},
	year = {1979},
	note = {Number: 3},
	keywords = {Scientific Communication, Experimentation, Statistical Probability, Statistical Tests, Type I Errors},
	pages = {638--641},
}

@article{open_science_collaboration_estimating_2015,
	title = {Estimating the reproducibility of psychological science},
	volume = {349},
	copyright = {Copyright © 2015, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/349/6251/aac4716},
	doi = {10.1126/science.aac4716},
	abstract = {Empirically analyzing empirical evidence
One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.
Science, this issue 10.1126/science.aac4716
Structured Abstract
INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.
RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.
RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P {\textless} .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.
CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that “we already know this” belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know. {\textless}img class="fragment-image" aria-describedby="F1-caption" src="https://science.sciencemag.org/content/sci/349/6251/aac4716/F1.medium.gif"/{\textgreater} Download high-res image Open in new tab Download Powerpoint Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.
Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.
A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.
A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.},
	language = {en},
	number = {6251},
	urldate = {2019-08-02},
	journal = {Science},
	author = {{Open Science Collaboration}},
	month = aug,
	year = {2015},
	pmid = {26315443},
	note = {Number: 6251},
	pages = {aac4716},
}

@article{nosek_promoting_2015,
	title = {Promoting an open research culture},
	volume = {348},
	copyright = {Copyright © 2015, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/348/6242/1422},
	doi = {10.1126/science.aab2374},
	abstract = {Author guidelines for journals could help to promote transparency, openness, and reproducibility
Author guidelines for journals could help to promote transparency, openness, and reproducibility},
	language = {en},
	number = {6242},
	urldate = {2017-12-21},
	journal = {Science},
	author = {Nosek, B. A. and Alter, G. and Banks, G. C. and Borsboom, D. and Bowman, S. D. and Breckler, S. J. and Buck, S. and Chambers, C. D. and Chin, G. and Christensen, G. and Contestabile, M. and Dafoe, A. and Eich, E. and Freese, J. and Glennerster, R. and Goroff, D. and Green, D. P. and Hesse, B. and Humphreys, M. and Ishiyama, J. and Karlan, D. and Kraut, A. and Lupia, A. and Mabry, P. and Madon, T. and Malhotra, N. and Mayo-Wilson, E. and McNutt, M. and Miguel, E. and Paluck, E. Levy and Simonsohn, U. and Soderberg, C. and Spellman, B. A. and Turitto, J. and VandenBos, G. and Vazire, S. and Wagenmakers, E. J. and Wilson, R. and Yarkoni, T.},
	month = jun,
	year = {2015},
	pmid = {26113702},
	note = {Number: 6242},
	pages = {1422--1425},
}

@article{miguel_promoting_2014,
	title = {Promoting {Transparency} in {Social} {Science} {Research}},
	volume = {343},
	copyright = {Copyright © 2014, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {https://www.gsb.stanford.edu/sites/gsb/files/publication-pdf/Science-2014-Miguel-30-1.pdf},
	doi = {10.1126/science.1245317},
	abstract = {Social scientists should adopt higher transparency standards to improve the quality and credibility of research.
Social scientists should adopt higher transparency standards to improve the quality and credibility of research.},
	language = {en},
	number = {6166},
	urldate = {2017-11-27},
	journal = {Science},
	author = {Miguel, E. and Camerer, C. and Casey, K. and Cohen, J. and Esterling, K. M. and Gerber, A. and Glennerster, R. and Green, D. P. and Humphreys, M. and Imbens, G. and Laitin, D. and Madon, T. and Nelson, L. and Nosek, B. A. and Petersen, M. and Sedlmayr, R. and Simmons, J. P. and Simonsohn, U. and Laan, M. Van der},
	month = jan,
	year = {2014},
	pmid = {24385620},
	note = {Number: 6166},
	pages = {30--31},
}

@article{leamer_lets_1983,
	title = {Let’s {Take} the {Con} {Out} of {Econometrics}},
	volume = {73},
	url = {http://www.econ.ucla.edu/workingpapers/wp239.pdf},
	urldate = {2019-08-02},
	journal = {The American Economic Review},
	author = {Leamer, Edward},
	year = {1983},
	pages = {31--43},
}

@article{francis_too_2012,
	title = {Too good to be true: {Publication} bias in two prominent studies from experimental psychology},
	volume = {19},
	issn = {1531-5320},
	shorttitle = {Too good to be true},
	url = {https://doi.org/10.3758/s13423-012-0227-9},
	doi = {10.3758/s13423-012-0227-9},
	abstract = {Empirical replication has long been considered the final arbiter of phenomena in science, but replication is undermined when there is evidence for publication bias. Evidence for publication bias in a set of experiments can be found when the observed number of rejections of the null hypothesis exceeds the expected number of rejections. Application of this test reveals evidence of publication bias in two prominent investigations from experimental psychology that have purported to reveal evidence of extrasensory perception and to indicate severe limitations of the scientific method. The presence of publication bias suggests that those investigations cannot be taken as proper scientific studies of such phenomena, because critical data are not available to the field. Publication bias could partly be avoided if experimental psychologists started using Bayesian data analysis techniques.},
	language = {en},
	number = {2},
	urldate = {2020-10-16},
	journal = {Psychonomic Bulletin \& Review},
	author = {Francis, Gregory},
	month = apr,
	year = {2012},
	pages = {151--156},
	file = {Springer Full Text PDF:C\:\\Users\\Shaon\\Zotero\\storage\\MEU8M8RS\\Francis - 2012 - Too good to be true Publication bias in two promi.pdf:application/pdf},
}

@article{munafo_manifesto_2017,
	title = {A manifesto for reproducible science},
	volume = {1},
	copyright = {2017 Macmillan Publishers Limited},
	issn = {2397-3374},
	url = {http://www.nature.com/articles/s41562-016-0021},
	doi = {10.1038/s41562-016-0021},
	abstract = {Improving the reliability and efficiency of scientific research will increase the credibility of the published scientific literature and accelerate discovery. Here we argue for the adoption of measures to optimize key elements of the scientific process: methods, reporting and dissemination, reproducibility, evaluation and incentives. There is some evidence from both simulations and empirical studies supporting the likely effectiveness of these measures, but their broad adoption by researchers, institutions, funders and journals will require iterative evaluation and improvement. We discuss the goals of these measures, and how they can be implemented, in the hope that this will facilitate action toward improving the transparency, reproducibility and efficiency of scientific research.},
	language = {en},
	number = {1},
	urldate = {2020-11-02},
	journal = {Nature Human Behaviour},
	author = {Munafò, Marcus R. and Nosek, Brian A. and Bishop, Dorothy V. M. and Button, Katherine S. and Chambers, Christopher D. and Percie du Sert, Nathalie and Simonsohn, Uri and Wagenmakers, Eric-Jan and Ware, Jennifer J. and Ioannidis, John P. A.},
	month = jan,
	year = {2017},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {1--9},
	file = {Full Text PDF:C\:\\Users\\Shaon\\Zotero\\storage\\ICQBCWTG\\Munafò et al. - 2017 - A manifesto for reproducible science.pdf:application/pdf;Snapshot:C\:\\Users\\Shaon\\Zotero\\storage\\H3UTNSQ9\\s41562-016-0021.html:text/html},
}

@article{mlinaric_dealing_2017,
	title = {Dealing with the positive publication bias: {Why} you should really publish your negative results},
	volume = {27},
	issn = {1330-0962},
	shorttitle = {Dealing with the positive publication bias},
	url = {https://hrcak.srce.hr/index.php?show=clanak&id_clanak_jezik=276521},
	doi = {10.11613/BM.2017.030201},
	abstract = {Studies with positive results are greatly more represented in literature than studies with negative results, producing so-called publication bias. This review aims to discuss occurring problems around negative results and to emphasize...},
	language = {en},
	number = {3},
	urldate = {2020-12-15},
	journal = {Biochemia Medica},
	author = {Mlinarić, Ana and Horvat, Martina and Šupak Smolčić, Vesna},
	month = oct,
	year = {2017},
	note = {tex.ids= mlinaricDealingPositivePublication2017a
publisher: Medicinska naklada},
	pages = {447--452},
	file = {Full Text PDF:C\:\\Users\\Shaon\\Zotero\\storage\\E3HBM6HM\\Mlinarić et al. - 2017 - Dealing with the positive publication bias Why yo.pdf:application/pdf;Snapshot:C\:\\Users\\Shaon\\Zotero\\storage\\MF7ZN467\\index.html:text/html;Snapshot:C\:\\Users\\Shaon\\Zotero\\storage\\9RGDVIKP\\index.html:text/html},
}

@article{franco_publication_2014,
	title = {Publication bias in the social sciences: {Unlocking} the file drawer},
	volume = {345},
	issn = {0036-8075},
	number = {6203},
	journal = {Science},
	author = {Franco, Annie and Malhotra, Neil and Simonovits, Gabor},
	year = {2014},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {1502--1505},
}

@article{kepes_avoiding_2014,
	title = {Avoiding bias in publication bias research: {The} value of “null” findings},
	volume = {29},
	issn = {0889-3268},
	number = {2},
	journal = {Journal of Business and Psychology},
	author = {Kepes, Sven and Banks, George C and Oh, In-Sue},
	year = {2014},
	note = {Publisher: Springer},
	pages = {183--203},
}

@article{rothstein_publication_2005,
	title = {Publication bias in meta-analysis},
	journal = {Publication bias in meta-analysis: Prevention, assessment and adjustments},
	author = {Rothstein, Hannah R and Sutton, Alexander J and Borenstein, Michael},
	year = {2005},
	note = {Publisher: Wiley Online Library},
	pages = {1--7},
}

@article{adams_searching_2016,
	title = {Searching and synthesising ‘grey literature’ and ‘grey information’ in public health: critical reflections on three case studies},
	volume = {5},
	issn = {2046-4053},
	shorttitle = {Searching and synthesising ‘grey literature’ and ‘grey information’ in public health},
	url = {https://doi.org/10.1186/s13643-016-0337-y},
	doi = {10.1186/s13643-016-0337-y},
	abstract = {Grey literature includes a range of documents not controlled by commercial publishing organisations. This means that grey literature can be difficult to search and retrieve for evidence synthesis. Much knowledge and evidence in public health, and other fields, accumulates from innovation in practice. This knowledge may not even be of sufficient formality to meet the definition of grey literature. We term this knowledge ‘grey information’. Grey information may be even harder to search for and retrieve than grey literature.},
	number = {1},
	urldate = {2021-04-17},
	journal = {Systematic Reviews},
	author = {Adams, Jean and Hillier-Brown, Frances C. and Moore, Helen J. and Lake, Amelia A. and Araujo-Soares, Vera and White, Martin and Summerbell, Carolyn},
	month = sep,
	year = {2016},
	keywords = {Evidence synthesis, Grey information, Grey literature, Interventions, Public health, Systematic review},
	pages = {164},
	file = {Full Text PDF:C\:\\Users\\Shaon\\Zotero\\storage\\NBWFAJTB\\Adams et al. - 2016 - Searching and synthesising ‘grey literature’ and ‘.pdf:application/pdf;Snapshot:C\:\\Users\\Shaon\\Zotero\\storage\\B8L8WF94\\s13643-016-0337-y.html:text/html},
}

@article{paez_gray_2017,
	title = {Gray literature: {An} important resource in systematic reviews},
	volume = {10},
	copyright = {© 2017 Chinese Cochrane Center, West China Hospital of Sichuan University and John Wiley \& Sons Australia, Ltd},
	issn = {1756-5391},
	shorttitle = {Gray literature},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jebm.12266},
	doi = {https://doi.org/10.1111/jebm.12266},
	abstract = {Systematic reviews aide the analysis and dissemination of evidence, using rigorous and transparent methods to generate empirically attained answers to focused research questions. Identifying all evidence relevant to the research questions is an essential component, and challenge, of systematic reviews. Gray literature, or evidence not published in commercial publications, can make important contributions to a systematic review. Gray literature can include academic papers, including theses and dissertations, research and committee reports, government reports, conference papers, and ongoing research, among others. It may provide data not found within commercially published literature, providing an important forum for disseminating studies with null or negative results that might not otherwise be disseminated. Gray literature may thusly reduce publication bias, increase reviews’ comprehensiveness and timeliness, and foster a balanced picture of available evidence. Gray literature's diverse formats and audiences can present a significant challenge in a systematic search for evidence. However, the benefits of including gray literature may far outweigh the cost in time and resource needed to search for it, and it is important for it to be included in a systematic review or review of evidence. A carefully thought out gray literature search strategy may be an invaluable component of a systematic review. This narrative review provides guidance about the benefits of including gray literature in a systematic review, and sources for searching through gray literature. An illustrative example of a search for evidence within gray literature sources is presented to highlight the potential contributions of such a search to a systematic review. Benefits and challenges of gray literature search methods are discussed, and recommendations made.},
	language = {en},
	number = {3},
	urldate = {2021-04-17},
	journal = {Journal of Evidence-Based Medicine},
	author = {Paez, Arsenio},
	year = {2017},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/jebm.12266},
	keywords = {publication bias, systematic review, evidence, search, gray literature},
	pages = {233--240},
	file = {Full Text PDF:C\:\\Users\\Shaon\\Zotero\\storage\\F3NZWKBY\\Paez - 2017 - Gray literature An important resource in systemat.pdf:application/pdf;Snapshot:C\:\\Users\\Shaon\\Zotero\\storage\\MTXWQQ6D\\jebm.html:text/html},
}

@article{riveros_timing_2013,
	title = {Timing and {Completeness} of {Trial} {Results} {Posted} at {ClinicalTrials}.gov and {Published} in {Journals}},
	volume = {10},
	issn = {1549-1676},
	url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1001566},
	doi = {10.1371/journal.pmed.1001566},
	abstract = {Agnes Dechartres and colleagues searched ClinicalTrials.gov for completed drug RCTs with results reported and then searched for corresponding studies in PubMed to evaluate timeliness and completeness of reporting. Please see later in the article for the Editors' Summary},
	language = {en},
	number = {12},
	urldate = {2021-04-17},
	journal = {PLOS Medicine},
	author = {Riveros, Carolina and Dechartres, Agnes and Perrodeau, Elodie and Haneef, Romana and Boutron, Isabelle and Ravaud, Philippe},
	month = dec,
	year = {2013},
	note = {Publisher: Public Library of Science},
	keywords = {Randomized controlled trials, Scientific publishing, Adverse events, Clinical trials, Drug administration, Medical journals, Research reporting guidelines, Statistical data},
	pages = {e1001566},
	file = {Full Text PDF:C\:\\Users\\Shaon\\Zotero\\storage\\YY7INBA7\\Riveros et al. - 2013 - Timing and Completeness of Trial Results Posted at.pdf:application/pdf;Snapshot:C\:\\Users\\Shaon\\Zotero\\storage\\UUUPWW5W\\article.html:text/html},
}

@article{joober_publication_2012,
	title = {Publication bias: {What} are the challenges and can they be overcome?},
	volume = {37},
	issn = {1180-4882},
	shorttitle = {Publication bias},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3341407/},
	doi = {10.1503/jpn.120065},
	number = {3},
	urldate = {2021-07-29},
	journal = {Journal of Psychiatry \& Neuroscience : JPN},
	author = {Joober, Ridha and Schmitz, Norbert and Annable, Lawrence and Boksa, Patricia},
	month = may,
	year = {2012},
	pmid = {22515987},
	pmcid = {PMC3341407},
	pages = {149--152},
	file = {PubMed Central Full Text PDF:C\:\\Users\\Shaon\\Zotero\\storage\\C9IGF5ZZ\\Joober et al. - 2012 - Publication bias What are the challenges and can .pdf:application/pdf},
}

@article{fanelli_negative_2012,
	title = {Negative results are disappearing from most disciplines and countries},
	volume = {90},
	issn = {0138-9130},
	number = {3},
	journal = {Scientometrics},
	author = {Fanelli, Daniele},
	year = {2012},
	note = {Publisher: Akadémiai Kiadó, co-published with Springer Science+ Business Media BV …},
	pages = {891--904},
}

@article{head_extent_2015,
	title = {The {Extent} and {Consequences} of {P}-{Hacking} in {Science}},
	volume = {13},
	issn = {1545-7885},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002106},
	doi = {10.1371/journal.pbio.1002106},
	abstract = {A focus on novel, confirmatory, and statistically significant results leads to substantial bias in the scientific literature. One type of bias, known as “p-hacking,” occurs when researchers collect or select data or statistical analyses until nonsignificant results become significant. Here, we use text-mining to demonstrate that p-hacking is widespread throughout science. We then illustrate how one can test for p-hacking when performing a meta-analysis and show that, while p-hacking is probably common, its effect seems to be weak relative to the real effect sizes being measured. This result suggests that p-hacking probably does not drastically alter scientific consensuses drawn from meta-analyses.},
	language = {en},
	number = {3},
	urldate = {2021-08-06},
	journal = {PLOS Biology},
	author = {Head, Megan L. and Holman, Luke and Lanfear, Rob and Kahn, Andrew T. and Jennions, Michael D.},
	month = mar,
	year = {2015},
	note = {Publisher: Public Library of Science},
	keywords = {Metaanalysis, Publication ethics, Reproducibility, Statistical data, Bibliometrics, Binomials, Medicine and health sciences, Test statistics},
	pages = {e1002106},
	file = {Full Text PDF:C\:\\Users\\Shaon\\Zotero\\storage\\IVZABFQQ\\Head et al. - 2015 - The Extent and Consequences of P-Hacking in Scienc.pdf:application/pdf;Snapshot:C\:\\Users\\Shaon\\Zotero\\storage\\PEHWA7MZ\\article.html:text/html},
}

@article{gadbury_inappropriate_2012,
	title = {Inappropriate fiddling with statistical analyses to obtain a desirable p-value: tests to detect its presence in published literature},
	issn = {1932-6203},
	author = {Gadbury, Gary L and Allison, David B},
	year = {2012},
	note = {Publisher: Public Library of Science San Francisco, USA},
}

@misc{schimmack_meta-scientific_2020,
	title = {A {Meta}-{Scientific} {Perspective} on “{Thinking}: {Fast} and {Slow}},
	shorttitle = {A {Meta}-{Scientific} {Perspective} on “{Thinking}},
	url = {https://replicationindex.com/2020/12/30/a-meta-scientific-perspective-on-thinking-fast-and-slow/},
	abstract = {2011 was an important year in the history of psychology, especially social psychology. First, it became apparent that one social psychologist had faked results for dozens of publications ( Second, …},
	language = {en-US},
	urldate = {2023-07-16},
	journal = {Replicability-Index},
	author = {Schimmack, Ulrich},
	month = dec,
	year = {2020},
	file = {Snapshot:C\:\\Users\\Shaon\\Zotero\\storage\\VBV37FJE\\a-meta-scientific-perspective-on-thinking-fast-and-slow.html:text/html},
}

@article{wickham_layered_2010,
	title = {A {Layered} {Grammar} of {Graphics}},
	volume = {19},
	issn = {1061-8600},
	url = {https://doi.org/10.1198/jcgs.2009.07098},
	doi = {10.1198/jcgs.2009.07098},
	abstract = {A grammar of graphics is a tool that enables us to concisely describe the components of a graphic. Such a grammar allows us to move beyond named graphics (e.g., the “scatterplot”) and gain insight into the deep structure that underlies statistical graphics. This article builds on Wilkinson, Anand, and Grossman (2005), describing extensions and refinements developed while building an open source implementation of the grammar of graphics for R, ggplot2. The topics in this article include an introduction to the grammar by working through the process of creating a plot, and discussing the components that we need. The grammar is then presented formally and compared to Wilkinson’s grammar, highlighting the hierarchy of defaults, and the implications of embedding a graphical grammar into a programming language. The power of the grammar is illustrated with a selection of examples that explore different components and their interactions, in more detail. The article concludes by discussing some perceptual issues, and thinking about how we can build on the grammar to learn how to create graphical “poems.” Supplemental materials are available online.},
	number = {1},
	urldate = {2023-08-25},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Wickham, Hadley},
	month = jan,
	year = {2010},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/jcgs.2009.07098},
	keywords = {Grammar of graphics, Statistical graphics},
	pages = {3--28},
	file = {Full Text PDF:C\:\\Users\\Shaon\\Zotero\\storage\\6CX2KMMZ\\Wickham - 2010 - A Layered Grammar of Graphics.pdf:application/pdf},
}

@article{kerr_harking_1998,
	title = {{HARKing}: hypothesizing after the results are known},
	volume = {2},
	issn = {1088-8683},
	shorttitle = {{HARKing}},
	doi = {10.1207/s15327957pspr0203_4},
	abstract = {This article considers a practice in scientific communication termed HARKing (Hypothesizing After the Results are Known). HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in one's research report as i f it were, in fact, an a priori hypotheses. Several forms of HARKing are identified and survey data are presented that suggests that at least some forms of HARKing are widely practiced and widely seen as inappropriate. I identify several reasons why scientists might HARK. Then I discuss several reasons why scientists ought not to HARK. It is conceded that the question of whether HARKing ' s costs exceed its benefits is a complex one that ought to be addressed through research, open discussion, and debate. To help stimulate such discussion (and for those such as myself who suspect that HARKing's costs do exceed its benefits), I conclude the article with some suggestions for deterring HARKing.},
	language = {eng},
	number = {3},
	journal = {Personality and Social Psychology Review: An Official Journal of the Society for Personality and Social Psychology, Inc},
	author = {Kerr, N. L.},
	year = {1998},
	pmid = {15647155},
	pages = {196--217},
	file = {Submitted Version:C\:\\Users\\Shaon\\Zotero\\storage\\QV25N9MM\\Kerr - 1998 - HARKing hypothesizing after the results are known.pdf:application/pdf},
}

@book{mackay_memoirs_1869,
	title = {Memoirs of extraordinary popular delusions and the madness of crowds},
	publisher = {George Routledge and sons},
	author = {Mackay, Charles},
	year = {1869},
}

@article{nickerson_confirmation_1998,
	title = {Confirmation {Bias}: {A} {Ubiquitous} {Phenomenon} in {Many} {Guises}},
	volume = {2},
	issn = {1089-2680},
	shorttitle = {Confirmation {Bias}},
	url = {https://doi.org/10.1037/1089-2680.2.2.175},
	doi = {10.1037/1089-2680.2.2.175},
	abstract = {Confirmation bias, as the term is typically used in the psychological literature, connotes the seeking or interpreting of evidence in ways that are partial to existing beliefs, expectations, or a hypothesis in hand. The author reviews evidence of such a bias in a variety of guises and gives examples of its operation in several practical contexts. Possible explanations are considered, and the question of its utility or disutility is discussed.},
	language = {en},
	number = {2},
	urldate = {2023-08-28},
	journal = {Review of General Psychology},
	author = {Nickerson, Raymond S.},
	month = jun,
	year = {1998},
	note = {Publisher: SAGE Publications Inc},
	pages = {175--220},
	file = {SAGE PDF Full Text:C\:\\Users\\Shaon\\Zotero\\storage\\54H9BKG6\\Nickerson - 1998 - Confirmation Bias A Ubiquitous Phenomenon in Many.pdf:application/pdf},
}

@article{pusztai_reproducibility_2013,
	title = {Reproducibility of research and preclinical validation: problems and solutions},
	volume = {10},
	copyright = {2013 Springer Nature Limited},
	issn = {1759-4782},
	shorttitle = {Reproducibility of research and preclinical validation},
	url = {http://www.nature.com/articles/nrclinonc.2013.171},
	doi = {10.1038/nrclinonc.2013.171},
	abstract = {There is much concern in the literature over the lack of reproducibility of many scientific reports. In this Perspective, the authors discuss how cognitive biases in research and flaws in the academic incentive system also contribute to the publication of immature results. The authors suggest some changes to the grant submission and funding system that could further improve the reproducibility of research findings.},
	language = {en},
	number = {12},
	urldate = {2023-08-28},
	journal = {Nature Reviews Clinical Oncology},
	author = {Pusztai, Lajos and Hatzis, Christos and Andre, Fabrice},
	month = dec,
	year = {2013},
	note = {Number: 12
Publisher: Nature Publishing Group},
	keywords = {Translational research},
	pages = {720--724},
	file = {Full Text PDF:C\:\\Users\\Shaon\\Zotero\\storage\\EJAG8DTM\\Pusztai et al. - 2013 - Reproducibility of research and preclinical valida.pdf:application/pdf},
}

@article{jones_increasing_2021,
	title = {Increasing the reproducibility of research will reduce the problem of apophenia (and more)},
	volume = {68},
	issn = {1496-8975},
	doi = {10.1007/s12630-021-02006-1},
	language = {eng},
	number = {8},
	journal = {Canadian Journal of Anaesthesia = Journal Canadien D'anesthesie},
	author = {Jones, Philip M. and Martin, Janet},
	month = aug,
	year = {2021},
	pmid = {33963518},
	keywords = {Humans, Reproducibility of Results},
	pages = {1120--1134},
	file = {Full Text:C\:\\Users\\Shaon\\Zotero\\storage\\MRIXW5US\\Jones and Martin - 2021 - Increasing the reproducibility of research will re.pdf:application/pdf},
}

@article{fischhoff_hindsight_1975,
	title = {Hindsight is not equal to foresight: {The} effect of outcome knowledge on judgment under uncertainty},
	volume = {1},
	issn = {1939-1277},
	shorttitle = {Hindsight is not equal to foresight},
	doi = {10.1037/0096-1523.1.3.288},
	abstract = {Notes that a major difference between historical and nonhistorical judgment is that the historical judge typically knows how things turned out. 3 experiments are described with a total of 479 college students. In Exp I, receipt of such outcome knowledge was found to increase the postdicted likelihood of reported events and change the perceived relevance of event-descriptive data, regardless of the likelihood of the outcome and the truth of the report. Ss were, however, largely unaware of the effect that outcome knowledge had on their perceptions. As a result, they overestimated what they would have known without outcome knowledge (Exp II), as well as what others (Exp III) actually did know without outcome knowledge. It is argued that this lack of awareness can seriously restrict one's ability to judge or learn from the past. (16 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {3},
	journal = {Journal of Experimental Psychology: Human Perception and Performance},
	author = {Fischhoff, Baruch},
	year = {1975},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {History, Judgment, Knowledge of Results},
	pages = {288--299},
	file = {Full Text PDF:C\:\\Users\\Shaon\\Zotero\\storage\\CNINJ2ZE\\Fischhoff - 1975 - Hindsight is not equal to foresight The effect of.pdf:application/pdf;Snapshot:C\:\\Users\\Shaon\\Zotero\\storage\\XU96IUNK\\1976-00159-001.html:text/html},
}

@article{nosek_preregistration_2018,
	title = {The preregistration revolution},
	volume = {115},
	url = {http://www.pnas.org/doi/abs/10.1073/pnas.1708274114},
	doi = {10.1073/pnas.1708274114},
	abstract = {Progress in science relies in part on generating hypotheses with existing observations
and testing hypotheses with new observations. This distincti...},
	language = {en},
	number = {11},
	urldate = {2023-08-28},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Nosek, Brian A. and Ebersole, Charles R. and DeHaven, Alexander C. and Mellor, David T.},
	month = mar,
	year = {2018},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {2600--2606},
	file = {Full Text PDF:C\:\\Users\\Shaon\\Zotero\\storage\\IXYW2X4D\\Nosek et al. - 2018 - The preregistration revolution.pdf:application/pdf},
}

@misc{nasa_science_unmasking_2001,
	title = {Unmasking the {Face} on {Mars} {\textbar} {Science} {Mission} {Directorate}},
	url = {https://science.nasa.gov/science-news/science-at-nasa/2001/ast24may_1},
	urldate = {2023-08-28},
	author = {{NASA Science}},
	month = may,
	year = {2001},
	file = {Unmasking the Face on Mars | Science Mission Directorate:C\:\\Users\\Shaon\\Zotero\\storage\\872S67EG\\ast24may_1.html:text/html},
}
