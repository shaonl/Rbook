[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Research Transparency, Reproducibility, and Basic Data Analysis in R!",
    "section": "",
    "text": "Preface\nI started writing this book in the summer of 2023 primarily for my students in PPE 4000 - Research in PPE: Research Transparency, Reproducibility and Basic Data Analysis in R. This is a small research course for advanced undergraduates in the Philosophy, Politics & Economics (PPE) Program at the University of Pennsylvania. I am currently a Postdoctoral Researcher in the PPE Program. I’m also a BITSS Catalyst, which means that I am dedicated to educating the next generation of social scientists on research transparency tools and practices.\nBeyond my students, I believe that the book can be useful for just about anyone interested in learning basic data analysis in R, especially if you have never learned any coding before. You may be wondering what separates this course from other introductory R courses or books that are freely available in the public domain? While there are MANY free resources to learn R, and MANY free resources to understand issues of Research Transparency & Reproducibility (RT2), I try to bridge the two ideas in this book at an undergraduate level. I have not seen a similar book, and thus it may be useful.\nThe prime motivation for doing so is that combining them in this way makes understanding RT2 concepts more concrete by implementing them directly in R. Additionally, by learning some basic R skills, students can walk away not only with an understanding of RT2 concepts, but also with a marketable skill for industry and academia. I never liked separating RT2 concepts and practices from actual quantitative work, and for that reason I have attempted to bridge the two (somewhat) cohesively in this semester-long course. Eventually the bridge will be more evident in the book (at least that’s my hope).\nI wrote this book using Quarto Book in R, and created the illustrations using Canva and DALL-E 2. I intend to keep updating this book and adding new content when I have time. If you have any feedback on this book, please email me at shaonl@sas.upenn.edu."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "3  Introduction",
    "section": "",
    "text": "By now, you should have installed R and R Studio (see Chapter 1 for details), and customized R Studio to the Editor theme and font size of your choice (see Chapter 2 for details). If you have not done these things, please do so now before proceeding."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "5  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "15  References",
    "section": "",
    "text": "1. Ioannidis JPA. Why Most\nPublished Research Findings\nAre False. PLOS Medicine.\n2005;2(8):e124. doi:10.1371/journal.pmed.0020124\n\n\n2. Simmons JP, Nelson LD, Simonsohn U.\nFalse-Positive Psychology:\nUndisclosed Flexibility in Data\nCollection and Analysis Allows\nPresenting Anything as\nSignificant. Psychological Science.\n2011;22(11):1359-1366. doi:10.1177/0956797611417632\n\n\n3. National Academies of Sciences E.\nReproducibility and Replicability in\nScience.; 2019. doi:10.17226/25303\n\n\n4. Open\nScience Collaboration. Estimating the reproducibility of psychological\nscience. Science. 2015;349(6251):aac4716. doi:10.1126/science.aac4716\n\n\n5. Schimmack U. A\nMeta-Scientific Perspective on\n“Thinking: Fast and Slow.\nReplicability-Index. Published online December 2020. Accessed\nJuly 16, 2023. https://replicationindex.com/2020/12/30/a-meta-scientific-perspective-on-thinking-fast-and-slow/\n\n\n6. Wickham H. A Layered\nGrammar of Graphics. Journal of\nComputational and Graphical Statistics. 2010;19(1):3-28. doi:10.1198/jcgs.2009.07098"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "1  Syllabus",
    "section": "",
    "text": "It’s a pleasure to have you in this course. I mean it! Without you, this course would not exist, and my job would be in jeopardy. So I’m glad to have you here, and not just because I need this job to purchase food and shelter, but also because teaching Research Transparency and Reproducibility (RT2) and R are my passions!"
  },
  {
    "objectID": "syllabus.html#whats-this-course-about",
    "href": "syllabus.html#whats-this-course-about",
    "title": "Syllabus",
    "section": "What’s this course about?",
    "text": "What’s this course about?\nA great question! Here’s the deal: across the sciences, there are often problems with published research and the way in which research is conducted. Some of these problems are pretty major, like fabricating data and images. Others are not as scandalous but still problematic, such as not providing the full details of your study. These problems are bad for science. They hold us back from solving problems and understanding complex phenomena. They give us distorted or incorrect insights that can then lead to bad policies and programs.\nIn this course we will discuss many issues related to the conduct of research, including bad or unethical approaches to research, how to think about replication and reproducibility, incentives to produce research, and solutions for making the research world a better place. You will hear stories of scandal and lies, but also tales of hope for a better future.\nBut this course is not just about learning about research, but also learning how to do research with code. We will learn basic data preparation, management, analysis, and reporting in R. Finally, when it comes to data analysis, we will go over basic statistical analysis from descriptives, bivariate analyses, and to multivariable analyses staying mainly within the General Linear Model family of techniques. We will also get a little taste of more advanced techniques in R, and Bayesian probability and inference. All in all, it’s going to be a good time!\nTL;DR - this course is about learning R, research transparency & replication, and basic data analysis. The instructor is reasonably competent, motivated, and chill."
  },
  {
    "objectID": "syllabus.html#any-prerequisites-for-this-course",
    "href": "syllabus.html#any-prerequisites-for-this-course",
    "title": "Syllabus",
    "section": "Any prerequisites for this course?",
    "text": "Any prerequisites for this course?\nIf you’ve never used R in your life, nor have even heard of RT2, that’s fine! I assume no prior knowledge of these concepts. It would be helpful to have completed at least a high school level course in statistics before taking this one. But if you haven’t, that’s no problem as we will go over the necessary statistical concepts.\nThe only thing you really need for this course is a modicum of motivation to learn R and RT2! But also a computer with enough memory to download R, R Studio, and maybe a few other things (~300 MB in total)."
  },
  {
    "objectID": "syllabus.html#why-should-you-take-this-course",
    "href": "syllabus.html#why-should-you-take-this-course",
    "title": "Syllabus",
    "section": "Why should you take this course?",
    "text": "Why should you take this course?\nMillions of people use the statistical programming language R to solve complex issues every day. If you have any interest in using data to understand the world, and visualize data in a way that communicates information succinctly and impactfully, this course is for you!\nNeed a programming skill to add to that resume to be more competitive on the job market? This course is for you!\nBesides being a highly marketable skill, learning R is incredibly useful to undertake quantitative research. If you have any interest in doing research, having R in your aRsenal can help you quickly and efficiently write reproducible code to import, clean, analyze, and interpret data in various forms. Finally, doing good research means doing ethical research. I will teach you how to avoid bad practices and malignant influences in order to conduct research in a reproducible, ethical, and rigorous manner.\n\n\n\n\n\n\nIf you want to impress potential romantic partners, R is also useful to have in your back pocket. I once went on a date where the subject of R came up (I brought it up), and my date asked how much it cost. When I said it was free, she was shocked and intrigued. I then explained all the things R can do. The relationship never went anywhere, but still…she was impressed."
  },
  {
    "objectID": "syllabus.html#what-are-the-requirements-of-this-course",
    "href": "syllabus.html#what-are-the-requirements-of-this-course",
    "title": "Syllabus",
    "section": "What are the requirements of this course?",
    "text": "What are the requirements of this course?\nThe pie chart breaks down the grade distribution for this course. Each component is described in detail below.\n\n\nClass participation (20%)\nTo receive an ‘A’ for this component, you must not accure more than two (2) unexcused absences during the semester. If you miss class due to religious holiday, illness, or an emergency, those are excused absences and do not count toward your two miss-able class limit. You must also complete all assignments prior to class, and be ready to engage in discussion, ask questions, engage in activities, and generally be ready to contribute. I run this class as a seminar, so it’s going to be quite interactive. If you feel sick, please do not come to class. Instead, please email me and we can set up a time to meet if you have any questions about the course material. No documentation is required by me for illness or emergencies because I trust you. Beyond the two excused absences, each missed class will decrease your Class Participation grade by 10%.\n\n\nClass presentations (20%)\nOver the course of the semester, each student will deliver two presentations to the class lasting approximately 15 minutes, followed by five minutes of Q&A. The first of these presentations will cover a recent or emerging topic within the world of research transparency and reproducibility. The second presentation will be an introduction and demonstration of a particular R package. I’m happy to help you come up with topics or troubleshoot for any of the presentations. All presentation topics must be approved by me before you proceed with the preparation.\n\n\nMonthly quizzes (10% each; 40% total)\nFour quizzes will be administered in class during the semester. Each quiz will include a section on research transparency & reproducibility (Section 1), and a section involving completing tasks in R (Section 2). Section 1 is ‘closed-book’ and will be administered on paper (no notes or digital media will be allowed during the completion of this section). Students will then complete Section 2 on their laptops, and will be permitted to use their notes and the Internet (including Chat GPT or similar natural language processing tools). The content covered in each quiz is listed in the Outline of classes section below. Each quiz is worth 10%, for a total of 40% of one’s grade for the course. There are no make-up quizzes except in case of emergency. All quizzes must be completed alone. If you receive a poor grade in Quizzes 1-3, you can substitute another RT2 presentation for it during our November 30th class.\n\n\nWeekly Discussion Board posts (20%)\nPrior to class each week (except the first and last week of class), students are required to post a question, comment, or reflection on the Discussion Board on Canvas about a topic related to research transparency & reproducibility, OR about R (either is fine with me). The post should be made on the appropriate Discussion Board on Canvas by 11am on the day we have class. Posts made after this time will not count toward the student’s grade. This is essentially a chance for you to explore, critique, or reflect on the class content or any related content from outside the class.\nThe content of Discussion Board posts should be reasonably substantive. For example, a post saying “this was really hard to do, and I think there should be a better way” is not very substantive, whereas a post saying “this was challenging, but I found some other code online that accomplishes this task in a better way. Here it is. . .” is a much more substantive post. Use your judgment and always feel free to check with me if you have any questions or doubts. Multiple posts are permitted. Students may miss posting for up to two (2) weeks (i.e. two posts) without penalty. Outside of this allowance, missed discussion posts will subtract 10% of a student’s grade for this component per week of missed posts."
  },
  {
    "objectID": "syllabus.html#what-is-the-grading-scale-for-this-course",
    "href": "syllabus.html#what-is-the-grading-scale-for-this-course",
    "title": "Syllabus",
    "section": "What is the grading scale for this course?",
    "text": "What is the grading scale for this course?\n\n\n\n\n\n\n\n\nPercentage\nLetter Grade\nInterpretation\n\n\n\n\n93-100%\nA\nOutstanding work. Meets all expectations of the assignment with excellent command of material.\n\n\n90-92%\nA-\nHigh-quality work. Meets nearly all expectations of the assignment, but is lacking in 1-2 areas.\n\n\n87-89%\nB+\nGood work. Meets most expectations of the assignment, but is lacking in 3 or more areas.\n\n\n83-86%\nB\nAdequate work. Meets the majority of expectations of the assignment, but is weak in 1-2 areas.\n\n\n80-82%\nB-\nBelow average work. Meets some expectations of the assignment, but is weak in 3 or more areas.\n\n\n77-79%\nC+\nDid not meet the expectations of the assignment. Please meet with me.\n\n\n73-76%\nC\nDid not meet the expectations of the assignment. Please meet with me.\n\n\n70-72%\nC-\nDid not meet the expectations of the assignment. Please meet with me.\n\n\n67-69%\nD\nDid not meet the expectations of the assignment. Please meet with me.\n\n\n&lt;67%\nF\nDid not meet the expectations of the assignment. Please meet with me.\n\n\n\nGrades will be calculated using standard rounding rules, in which a number in the tenths decimal place will be rounded down to the nearest integer if it is between 0-4, and rounded up to the nearest integer if it is between 5-9. For example, a grade of 89.4% will be rounded to 89%, and a grade of 89.5% will be rounded to 90%. For all quizzes, I provide an answer key after it has been completed. For all assignments, I provide a rubric."
  },
  {
    "objectID": "syllabus.html#what-materials-do-i-need-to-buy-for-this-course",
    "href": "syllabus.html#what-materials-do-i-need-to-buy-for-this-course",
    "title": "Syllabus",
    "section": "What materials do I need to buy for this course?",
    "text": "What materials do I need to buy for this course?\nNothing beyond what you use normally for other courses at Penn (i.e. a personal computer or laptop, internet access, word processing software, and pdf reading software). We will primarily be using the free e-textbook that I have written for this class. All other materials will be provided on Canvas. Please make sure R and RStudio are downloaded and installed on your laptop before the first class following the directions in Chapter 1. If you have trouble installing either of them, please email me."
  },
  {
    "objectID": "syllabus.html#do-you-have-an-email-policy",
    "href": "syllabus.html#do-you-have-an-email-policy",
    "title": "Syllabus",
    "section": "Do you have an “Email Policy”?",
    "text": "Do you have an “Email Policy”?\nI do! Please allow me up to two business days (i.e. M-F except holidays) to get back to your emails. In most cases, I will get back to you as soon as possible. If I haven’t replied to you in two business days, please follow-up. I stick to traditional work hours (i.e 9am - 5pm) when responding to emails. I request that you to begin all emails to me with “Hi Shaon,” because I prefer personalized emails addressing me by name. If you have any questions about course material, or if there is anything I can do to help you succeed in the course, please email me. I am here to support your growth and nurture your interests. Additionally, if anything at all in the course does not sit right with you at any point, please get in touch."
  },
  {
    "objectID": "syllabus.html#what-are-you-doing-to-make-the-course-more-accessible-and-inclusive",
    "href": "syllabus.html#what-are-you-doing-to-make-the-course-more-accessible-and-inclusive",
    "title": "Syllabus",
    "section": "What are you doing to make the course more accessible and inclusive?",
    "text": "What are you doing to make the course more accessible and inclusive?\nI strive to create a welcoming environment for all, and have a zero tolerance policy regarding bullying, harassment, or threatening behavior. Part of creating an inclusive environment can involve occasionally hearing a viewpoint contrary to your own. I expect all of us to engage in respectful civil discourse, especially when disagreeing with one another. This is usually not an issue in my courses, but as we sometimes deal with sensitive subject matter, please remember it is encouraged to share your thoughts and opinions, but please accord the same respect to others as you would have them accord to you. If you feel at any point that I or someone in the class has violated that trust, please contact me as soon as possible.\nPart of inclusion means supporting our classmates who are breastfeeding or have gaps in child-care. Babies are always welcome in the class, and I ask students who bring babies to sit near a door in case you need to step out briefly to provide special attention to your little one. For young children, while not a long-term child care solution, occasionally bringing a child to class is perfectly fine. I ask that all students work together with me to support our classmates who bring babies or young children to class. While I hold all students to the same high-quality standards for the work submitted in this course, if you are a student-parent, I will be happy to work with you to support you however I can.\nService animals are always welcome in the classroom.\nIf you require any other accommodations, please let me know and I will do my best to make them happen. Additionally, please contact me if you ever just want to talk.\nIf you’re an international student, I feel your pain (#relatable)! Let’s talk about it."
  },
  {
    "objectID": "syllabus.html#is-cheating-in-the-course-ok",
    "href": "syllabus.html#is-cheating-in-the-course-ok",
    "title": "Syllabus",
    "section": "Is cheating in the course ok?",
    "text": "Is cheating in the course ok?\nNo! Please familiarize yourself with Penn’s Code of Academic Integrity if you have not already done so here. I have a zero tolerance policy for cheating and plagiarism. If you are ever unsure what the line is between plagiarism and appropriate citation/reference, please contact me BEFORE submitting your work! If you feel that cannot succeed in this course without cheating, please contact me and we can speak confidentially. I will never punish you for the temptation, and we can develop a plan together to figure out a plan for avoiding the temptation. Please always work alone on all quizzes and written assignments in this course, and never copy a sentence or phrase from another work without appropriate citations and references."
  },
  {
    "objectID": "outline.html",
    "href": "outline.html",
    "title": "Outline of Classes",
    "section": "",
    "text": "Date\nTopics and Events\n\n\n\n\nAugust 31\nIntroduction to Base R and RStudio (operations, objects, data types, IDE).\nIntroduction to RT2 (history of replication and Mertonian Norms).\n\n\nSeptember 7\n\n\n\nSeptember 14\n\n\n\nSeptember 21\n\n\n\nSeptember 28\n\n\n\nOctober 5\n\n\n\nOctober 12\nFall Break (No Class)\n\n\nOctober 19\n\n\n\nOctober 26\n\n\n\nNovember 2\n\n\n\nNovember 9\nStudent Presentations - Round 1.\n\n\nNovember 16\n\n\n\nNovember 21 (Tuesday)\nThur-Fri class schedule on Tue-Wed.\n\n\nNovember 30\n\n\n\nDecember 7\nLast Day of Class.\nStudent Presentations - Round 2."
  },
  {
    "objectID": "syllabus.html#what-are-the-course-details",
    "href": "syllabus.html#what-are-the-course-details",
    "title": "Syllabus",
    "section": "What are the course details?",
    "text": "What are the course details?\nCourse Title: Research in PPE: Research Transparency, Reproducibility and Basic Data Analysis in R\nCourse Instructor: Shaon Lahiri, PhD, MPH (he/him)\nCourse Instructor Email: shaonl@sas.upenn.edu\nCourse Number: PPE 4000-301\nCRN: 60098\nSchedule: Thursdays, 12pm - 3pm (August 29, 2023 - December 11, 2023)\nLocation: Fisher-Bennett Hall 017"
  },
  {
    "objectID": "installation.html",
    "href": "installation.html",
    "title": "1  Installing R and RStudio",
    "section": "",
    "text": "Let’s make sure we understand what R and RStudio are before proceeding. Firstly, R is a programming language that is commonly used for statistical analysis and data visualization. RStudio is an Integrated Development Environment (IDE) which sits on top of R. It provides a very nice graphical user interface that allows us to use R along with many other useful features.\nR is the engine, and RStudio is the car frame. You can use R without RStudio (known as Base R), but you cannot use RStudio without R.\nBefore we meet for the first class, ensure you have downloaded and installed the following:\n\nR\n\nDownload from here: https://cran.r-project.org\n\nRStudio Desktop\n\nDownload from here: https://posit.co/download/rstudio-desktop/\n\n\nIf you’re using a Mac and you’re having trouble, try switching browsers from Safari to Chrome. Sometimes Safari has some extra security provisions which prevent you from installing software successfully, and this can be remedied by using a different browser like Chrome.\nPlease email me if you have any trouble installing R and R Studio. If you’ve successfully installed both, head over to Chapter 2 ."
  },
  {
    "objectID": "getstarted.html",
    "href": "getstarted.html",
    "title": "2  Getting Accustomed to R Studio",
    "section": "",
    "text": "Once you’ve successfully installed R and RStudio, go ahead and open RStudio. It should look something like this:\n\nLooks pretty plain right? No razzle dazzle?\nBefore we do anything, let’s first customize the way RStudio looks by going to Tools –&gt; Global Options:\n\nNext, go to Appearance and select an Editor Theme (personally I prefer dark themes like Cobalt). You can also modify the Editor font size in case you want to zoom in on the code. Click Apply to try out different themes, and Ok once you’ve settled on a theme.\n\nYou can change the Editor font size to something larger if you have trouble seeing the code at size 10. You can also do this anytime in RStudio by going to View and then clicking Zoom In or Zoom Out:\n\nNow that you’ve settled on a theme and Editor font size, let’s see what each of the panels in RStudio mean:\n\n\nThis area is called the Console, and this is where code is executed.\nThis area is the Environments Pane. This is where you can see the objects stored in your R session, such as data, functions, variables, and other things.\nThis area is the Output Pane. This is where you can see files in your Working Directory (under the tab Files), see a preview Plots you might have created, see your list of packages, and more.\n\nThere is also a fourth pane, which is usually created by clicking File –&gt; New File –&gt; R Script. This will create a setup that looks like this:\n\n\nThis is the Source Pane. This is where you can view and edit various code files. By default, we will use this pane to write, edit, and execute R Script files. These files tell R what code to run and in what order to run it.\n\nSo there you have it. Those are the four main panels that we will use in R Studio to write and execute R Code. Now, let’s learn some R starting with Chapter 3 !"
  },
  {
    "objectID": "Installation.html",
    "href": "Installation.html",
    "title": "Appendix A: Installing R and RStudio",
    "section": "",
    "text": "Let’s make sure we understand what R and RStudio are before proceeding. Firstly, R is a programming language that is commonly used for statistical analysis and data visualization. RStudio is an Integrated Development Environment (IDE) which sits on top of R. It provides a very nice graphical user interface that allows us to use R along with many other useful features.\nR is the engine, and RStudio is the car frame. You can use R without RStudio (known as Base R), but you cannot use RStudio without R.\nBefore we meet for the first class, ensure you have downloaded and installed the following:\n\nR\n\nDownload from here: https://cran.r-project.org\n\nRStudio Desktop\n\nDownload from here: https://posit.co/download/rstudio-desktop/\n\n\nIf you’re using a Mac and you’re having trouble, try switching browsers from Safari to Chrome. Sometimes Safari has some extra security provisions which prevent you from installing software successfully, and this can be remedied by using a different browser like Chrome.\nPlease email me if you have any trouble installing R and R Studio. If you’ve successfully installed both, head over to Appendix B."
  },
  {
    "objectID": "intro.html#what-exactly-is-r-and-why-is-it-called-r-and-not-something-more-informative",
    "href": "intro.html#what-exactly-is-r-and-why-is-it-called-r-and-not-something-more-informative",
    "title": "3  Introduction to R",
    "section": "3.1 What exactly is R, and why is it called R and not something more informative?",
    "text": "3.1 What exactly is R, and why is it called R and not something more informative?\nR is a computer language and run-time environment which can be used to do many things, including statistical computing and data visualization. I know, the name is weird. How can it be named after a letter?!\nHere’s the deal. R was created by statisticians Ross Ihaka and Robert Gentleman in the early 1990s at the University of Auckland. They developed a coding language to teach introductory statistics based on the syntax of another language at the time called S (programmers loved single-letter names back in the day). Since both their first names (Robert and Ross) started with the letter R, that’s what they chose to call their new language. Seriously.\n\n\n\nRoss Ihaka (right) and Robert Gentleman (left) - the creatoRs.\n\n\nImportantly, Gentleman and Ihaka spoke with their colleagues who convinced them to release the language for free as part of a philosophy called the Free Software Movement. Under this philosophy, “users have the freedom to run, copy, distribute, study, change and improve the software” (see GNU for more details).\nThey created a mailing list to discuss all things R in 1994, which quickly became big enough to overwhelm them with many feature requests and bug reports. They then selected a core group of developers who would maintain R. They also enlisted the support of their colleagues Kurt Hornik and Fritz Leisch at the Technical University of Vienna who created the Comprehensive R Archive Network (CRAN), which is a repository of user contributions.\nToday, R is a thriving language that receives regular updates by the R Core Team, and boasts about 19,657 user-contributed packages on CRAN. R is free, flexible, powerful, and awesome.\n\n\n\nThis art was made in R by Danielle Navarro. That's right, you can even make art in R. Click on the image if you want to see more of her work.\n\n\nYou might have heard of other programs for data analysis, and might be wondering which of these are open-source (meaning free!), and which are proprietary. I list some of the most common statistics programs, and whether they are open source or not in Table 3.1.\n\n\nTable 3.1: Open source or not? How R compares to other programs used for data analysis.\n\n\n\n\n\n\n\nProgram/language used for data analysis\nIs it Open Source (free) or not?\nCompany or organization maintaining the program/language\n\n\n\n\nR\nYES\nSupported by R Core Team and R Foundation for Statistical Computing\n\n\nPython\nYES\nPython Software Foundation\n\n\nSAS\nNO\nSAS Institute\n\n\nSPSS\nNO\nIBM\n\n\nStata\nNO\nStataCorp LLC\n\n\nMATLAB\nNO\nMathWorks\n\n\nJMP\nNO\nJMP Statistical Discovery LLC\n\n\nMPlus\nNO\nMuthén & Muthén\n\n\n\n\nAfter looking at the list in Table 3.1, it’s natural to wonder “Why would I ever need to use a proprietary software when free versions exist?” That’s a great question to ask. Personally, I see the main reason people use proprietary software is because they were trained to use a particular program and stuck with it. Whatever works for you! Another reason to use a proprietary software might be because it has functionality which you cannot find in the open-source versions. I have only encountered this case once with a very specific type of analytical approach, but generally speaking there are far more contributors to open-source packages and functions than you will find with proprietary software companies.\nThis book proudly takes the very opinionated position that R is the best."
  },
  {
    "objectID": "intro.html#using-r-as-a-calculator",
    "href": "intro.html#using-r-as-a-calculator",
    "title": "3  Introduction to R",
    "section": "3.2 Using R as a calculator",
    "text": "3.2 Using R as a calculator\nThe most basic use of R is as a calculator. You can execute calculations in a Script file or in the Console directly. In addition to using any real number, the following symbols and operations can also be used:\n\n\n\nCode\nMeaning\n\n\n\n\n+\nAddition\n\n\n-\nSubtraction\n\n\n*\nMultiplication\n\n\n/\nDivision\n\n\n^\nExponentiation\n\n\n( )\nParentheses/Brackets\n\n\nsqrt()\nSquare Root\n\n\nlog()\nNatural Logarithm\n\n\nexp()\nExponential Value\n\n\n\nJust enter the calculation in the Console (or Script File, though the Console is easier for quick calculations) following the appropriate order of operations (PEMDAS). As a matter of style, you want to include a space between arithmetic operators like +,-,*, and/. You also want to add a space after any commas, just like in English. This makes your code more readable.\n\n(6 * 6) + (12 / 2)\n\n[1] 42\n\nsqrt(81) * sqrt(9)\n\n[1] 27\n\n(2 / 3)^(4)\n\n[1] 0.1975309\n\nlog(100)\n\n[1] 4.60517\n\nexp(4.61)\n\n[1] 100.4841\n\n10^5\n\n[1] 1e+05\n\n\nAs you see, the answer is given following the [1], which just refers to the first position in a vector (to be discussed shortly). Note also that R uses scientific notation, so instead of printing 10^5=1000000, R will print 10^5 = 1e+05. The e here is completely unrelated to the number \\(e\\) (Euler’s number). It just means “10 to the power of.”\nYou can also easily round a number to a given place using the round() function, in which the first argument is the number or numeric vector to round, and the second argument digits = is the number of decimal places to which you want to round.\n\nx &lt;- log(100)*2.45\n\nprint(x)\n\n[1] 11.28267\n\n# Let's round this to two decimal places, then one decimal place, and then no decimal places.\n\nround(x, digits = 2)\n\n[1] 11.28\n\nround(x, digits = 1)\n\n[1] 11.3\n\nround(x, digits = 0)\n\n[1] 11\n\n\n\n\n\n\n\n\n“We talking about practice, man” - Allen Iverson\n\n\n\nGo ahead and try doing some calculations of your own in the Console! Especially if you’re new to R, the best way to learn R is by doing R over and over again."
  },
  {
    "objectID": "intro.html#allen-iverson",
    "href": "intro.html#allen-iverson",
    "title": "5  Introduction to R",
    "section": "5.3 - Allen Iverson",
    "text": "5.3 - Allen Iverson\nGo ahead and try doing some calculations of your own in the Console! Especially if you’re new to R, the best way to learn R is by doing R over and over again."
  },
  {
    "objectID": "intro.html#r-packages",
    "href": "intro.html#r-packages",
    "title": "3  Introduction to R",
    "section": "3.5 R Packages",
    "text": "3.5 R Packages\nPackages are user-written contributions which extend the functionality of R. They typically comprise a set of functions, code, documentation, and occasionally some datasets as well. Packages are stored in R as a ‘library’. They only have to installed once, but must be loaded each time you intend to use them. Typically, we only have need for a few packages at a time.\nInstalling a package typically involves communicating with CRAN, and is easily accomplished using the install.packages() function in which the name of the package you want to install should be placed in the parentheses with quotation marks \" \". Go ahead now and give it a shot with the tidyverse package (or set of packages rather) using the following code.\ninstall.packages(\"tidyverse\")\nIf the installation was successful, you should see a message that mentions the package has been successfully unpacked and located somewhere on your computer like this:\n\nSee how it says that the downloaded package is located on my computer? This type of message indicates that there were no problems with the installation. Now, if we want to use this package, it must be explicitly called using the library() function. This is ideally done in a dedicated section called “Load Libraries” or something similar.\n# Load libraries --------------------------\nlibrary(tidyverse)\nNow you know how to install and load packages from CRAN. This is what you will mostly do in R, as CRAN packages are all tested and meet its quality standards. To update a package, I find the easiest, cleanest approach is to go to your Output Pane in the bottom right of the screen, go to the Packages tab, and then hit the Update button.\n\nThat will bring up a menu which gives you several relevant pieces of information:\n\nWhich package(s) have an update available.\nWhat version of the package you have installed vs what version is available.\nWhat changes the update is making under ‘NEWS’.\n\n\nMaybe you don’t want to sit through the updates of EVERY package for which an update is available. This menu allows you to pick and choose the packages you want to update, and then click “Install Updates.” If you have a specific package you want to update and want to do it quickly, you can use the install.packages() function with the package name to be updated in quotes.\ninstall.packages(\"bayesplot\")\nFinally, say you want to just have all your packages up to date, and have some time to let R do its thing while you get a coffee, you can just type:\nupdate.packages(ask=FALSE)\nThis will update every package for which an update is available, and the ask=FALSE argument suppresses a prompt from appearing before every package update.\nFinally, you may be wondering which packages are essential to install and have ready to go. Typically, you know which packages to use based on a particular task you want to accomplish. Thus, I wouldn’t worry about this for now. Any R script file using packages will always have those listed in the file, and thus you will typically know what packages to install and load."
  },
  {
    "objectID": "intro.html#comments",
    "href": "intro.html#comments",
    "title": "3  Introduction to R",
    "section": "3.3 Comments",
    "text": "3.3 Comments\nYou can and should always write comments in your R Script file using a single hash #. It’s a good idea to use - or = to also clearly break up the Script file into chunks. The easiest and cleanest way to do this is to use Section Headers. On a Windows, you can use Ctrl+Shift+R and on Mac, you can use Cmd+Shift+R to quickly add a section header. You can also go to Code –&gt; Insert Section. Then you can very neatly organize your Script File, like so:\n\n# Load Libraries --------------------------------------------------------\n\n# Load Data -------------------------------------------------------------\n\nIt’s a good idea to use comments to explain why you are doing something. For instance, if you are using a specific analytic approach, mention in a comment why you are doing that. I also recommend starting every Script File with the following information on separate commented lines: Title of file, Author, Date, Purpose. Here is an example:\n\n# Data Cleaning\n# By Shaon Lahiri \n# June 14, 2027 \n# This file imports the dataset \"piracy.csv\" and prepares it for data analysis. \n\nBy having clear meta-data for each Script file like this, someone else who reads your code can better understand its purpose, provenance, and format. Additionally, comments on the purpose of a particular line of code can also help you remember why you did something, as it can be easy to forget years later what you had in mind for a particular task.\n\n\n\n\n\n\nKeyboard Shortcuts\n\n\n\nIt can be very useful to know keyboard shortcuts for common tasks. You may want to bookmark this page which lists keyboard shortcuts in R Studio for Mac and Windows."
  },
  {
    "objectID": "intro.html#functions",
    "href": "intro.html#functions",
    "title": "3  Introduction to R",
    "section": "3.4 Functions",
    "text": "3.4 Functions\nA function is a block of code that runs only when it is called. Calling a function just means you are giving the computer a single instruction to do a particular thing. In R, every function comprise a word followed immediately by parentheses function(). We often have information we want to pass to a function, and these are called arguments. Essentially, the function does the thing we want, and the arguments specify how we want them done, or what information we want the function to use.\nLet’s try using the simple print() function to display the phrase “Hello World.”\n\nprint(\"Hello, World!\")\n\n[1] \"Hello, World!\"\n\n\nHere we called the print() function and passed it the argument \"Hello World!\". This resulted in the expected behavior of the value \"Hello, World!\" being printed (i.e. shown on screen).\n\n\n\n\n\n\nWhy “Hello, World!”?\n\n\n\nDid you know that writing code to print “Hello World” is a programming tradition? It’s often the first thing a student learns to write in a programming language, and comes to us from the 1970s in Bell Laboratories. Learn more here.\n\n\nBesides using functions to accomplish various tasks, we can also easily create our own functions in R. Creating your own function is often a way to automate a series of tasks to streamline your work. We will tackle this issue later. For now, it will be sufficient to be familiar with the terms function, argument, and call."
  },
  {
    "objectID": "intro.html#assignments",
    "href": "intro.html#assignments",
    "title": "3  Introduction to R",
    "section": "3.6 Assignments",
    "text": "3.6 Assignments\nIn R, everything is an object. An object is just some data that you have stored. It can be a number, a dataset, a function, or anything else. For example, let’s say we want to perform the following calculations:\n\n((sqrt(81) * sqrt(9)) / log(3.6)) + 6\n((sqrt(81) * sqrt(9)) / log(3.6)) - 12\n((sqrt(81) * sqrt(9)) / log(3.6)) * (2/3)\n\nWe may not want to keep writing ((sqrt(81) * sqrt(9)) / log(3.6)) as it’s cumbersome to work with and to look at. We can streamline the process by assigning ((sqrt(81) * sqrt(9)) / log(3.6)) to a simple name of our choice. Let’s say we assign it to an object called quantity1, the resulting computation would be much easier to write. We can do this by using the assignment operator &lt;- which is comprised of the symbol for less than followed immediately by a minus sign -. So, the code incorporating assignment of the quantity to an object called quantity1 would look like this:\n\nquantity1 &lt;- ((sqrt(81) * sqrt(9)) / log(3.6)) \n\nquantity1 + 6\nquantity1 - 12\nquantity1 * (2/3)\n\nAs you’ll notice, the name of the object always comes first, then the assignment operator &lt;-, and finally the value we are assigning to the object.\nYou can also assign text to an object if you enclose the text in quotes \" \".\n\nmyfavorite &lt;- \"Cat\"\nmyfavorite\n\n[1] \"Cat\"\n\n\nHere, I assigned the value \"Cat\" to an object called myfavorite. I then called that object, and this printed its value.\nOne you assign a value to an object, it will show up in your Environments Pane on the top-right of your screen. This allows you to keep track of all the objects you have created in your R session, along with their values.\n\nYou can also assign the same value to multiple variables using multiple assignment operators &lt;- &lt;- &lt;-. For example, if I want to assign the value male to three names, this is what that would look like.\n\n# Assign multiple variables the value male.\nBrendan &lt;- Mark &lt;- Tyrone &lt;- \"male\"\n\nprint(Brendan)\n\n[1] \"male\"\n\nprint(Mark)\n\n[1] \"male\"\n\nprint(Tyrone)\n\n[1] \"male\"\n\n\nIf you want to remove an object from your environment, you can use the rm() function with the name of your object in parentheses. For example, if I wanted to remove the object quantity1, I would write rm(quantity1). If you want to remove all objects from your environment, you can run the command rm(list=ls()).\n\n\n\n\n\n\nYou can’t name an object just anything!\n\n\n\nThere are some words in R that are reserved for particular tasks. These Reserved Words cannot be used to name an object. So, don’t ever use these words alone to name an object.\n\n\n\nbreak\nNA\n\n\nelse\nNaN\n\n\nFALSE\nnext\n\n\nTRUE\nrepeat\n\n\nfor\nreturn\n\n\nfunction\nwhile\n\n\nInf\nif"
  },
  {
    "objectID": "intro.html#help",
    "href": "intro.html#help",
    "title": "3  Introduction to R",
    "section": "3.7 Help",
    "text": "3.7 Help\nIf you ever need to know how a function works, such as what arguments it take, you can easily see documentation for it by adding a question mark ? before the function name. For example, if you can’t remember what the rm() function does, or what arguments it takes, you can write ?rm and see the documentation for it. For more complex problems or troubleshooting, you can and should become familiar with Googling the issue, and then seeing the best answer on Stack Overflow in particular. Stack Overflow is one the best places to get answers for troublesome R code. You should also note that everyone, and I mean absolutely, everyone Googles code issues at some point or another. This is because coding and data science are dynamic fields, and things are often changing. It’s sensible to keep up with how people are solving particular problems by seeing what people are saying on Stack Overflow or other fora."
  },
  {
    "objectID": "replication1.html",
    "href": "replication1.html",
    "title": "6  A Very Brief History of Replication",
    "section": "",
    "text": "6.0.1 What is Research Transparency, Reproducibility, and Replication?\nBy now, you should be swimming along in understanding the preliminaries of R. Hopefully, you are practicing and ensuring you have nailed down all the basics.\nWhile it can be tempting to continue along this road, and immediately jump to the next topic in R, this is actually a good time to slow things down, and understand what we are doing in this course. This is not a course in how to code in R, at least not entirely. This course is also about the very consequential problems and solutions involved in Research Transparency & Reproducibility (RT2). Let’s begin with some definitions of these terms from the United States National Academies of Sciences, Engineering, and Medicine.1\n\n\n\n\n\n\nDefinitions\n\n\n\nReproducibility - Obtaining consistent computational results using the sample input data, computational steps, methods, code, and conditions of analysis.\nReplicability - Obtaining consistent results across studies aimed at answering the same scientific question, each of which has obtained its own data.\n\n\nThink of reproducibility as checking the results of the original study. If a researcher gave you their code and data, could you reproduce or recreate their analyses and have them match? That’s the hope anyway, but you would be surprised how often that does not happen.\nThink of replication as conducting multiple studies, using the same methods and procedures as the original study. If you arrive at similar results, that’s good and shows that the underlying theory seems to hold. If not, then it could be a) a problem with the underlying theory, b) a problem with the methods or procedures, or c) both.\nFinally, as stated by the National Academies, “Reproducibility is strongly associated with transparency; a study’s data and code have to be available in order for others to reproduce and confirm results.”1 Thus, transparency typically refers to availability of data, code, and materials used to produce the results of a study. However, the term conveys more than that. Another definition of research transparency is as the obligation to “make data, analysis, methods, and interpretive choices underlying their claims visible in a way that allows others to evaluation them - as a fundamental ethical obligation.”2 This extends beyond mere availability to a fundamental way of doing ethical research.\nNow that we’ve got definitions for the terms, let’s have a look at some key (and in some cases oft-forgotten) players in the history of replication and reproducibility.\n\n\n6.0.2 Replication Origins\n\n\n\n\n1. National Academies of Sciences E. Reproducibility and Replicability in Science.; 2019. doi:10.17226/25303\n\n\n2. Moravcsik A. Transparency in Qualitative Research. SAGE Publications Limited; 2020."
  },
  {
    "objectID": "replication1.html#what-is-research-transparency-reproducibility-and-replication",
    "href": "replication1.html#what-is-research-transparency-reproducibility-and-replication",
    "title": "11  A Very Brief History of Replication",
    "section": "11.1 What is Research Transparency, Reproducibility, and Replication?",
    "text": "11.1 What is Research Transparency, Reproducibility, and Replication?\nBy now, you should be swimming along in understanding the preliminaries of R. Hopefully, you are practicing and ensuring you have nailed down all the basics.\nWhile it can be tempting to continue along this road, and immediately jump to the next topic in R, this is actually a good time to slow things down, and understand what we are doing in this course. This is not a course in how to code in R, at least not entirely. This course is also about the very consequential problems and solutions involved in Research Transparency & Reproducibility (RT2). Let’s begin with some definitions of these terms from the United States National Academies of Sciences, Engineering, and Medicine.1\n\n\n\n\n\n\nDefinitions\n\n\n\nReproducibility - Obtaining consistent computational results using the sample input data, computational steps, methods, code, and conditions of analysis.\nReplicability - Obtaining consistent results across studies aimed at answering the same scientific question, each of which has obtained its own data.\n\n\nThink of reproducibility as checking the results of the original study. If a researcher gave you their code and data, could you reproduce or recreate their analyses and have them match? That’s the hope anyway, but you would be surprised how often that does not happen.\nThink of replication as conducting multiple studies, using the same methods and procedures as the original study. If you arrive at similar results, that’s good and shows that the underlying theory seems to hold. If not, then it could be a) a problem with the underlying theory, b) a problem with the methods or procedures, or c) both.\nFinally, as stated by the National Academies, “Reproducibility is strongly associated with transparency; a study’s data and code have to be available in order for others to reproduce and confirm results.”1 Thus, transparency typically refers to availability of data, code, and materials used to produce the results of a study. However, the term conveys more than that. Another definition of research transparency is as the obligation to “make data, analysis, methods, and interpretive choices underlying their claims visible in a way that allows others to evaluation them - as a fundamental ethical obligation.”2 This extends beyond mere availability of data and code to a fundamental way of doing ethical research.\nThe definitions are illustrated in the Figure 11.1.\n\n\n\nFigure 11.1: Transparency is ideal for replication efforts, but often materials are not provided by authors.\n\n\nNow that we’ve got definitions for the terms, let’s have a look at some key (and in some cases oft-forgotten) players in the history of replication and reproducibility."
  },
  {
    "objectID": "replication1.html#replication-origins",
    "href": "replication1.html#replication-origins",
    "title": "11  A Very Brief History of Replication",
    "section": "11.2 Replication Origins",
    "text": "11.2 Replication Origins\nThe history of replication is closely tied to the history of the Scientific Method. Many individuals over time likely contributed to it, but let’s have a look at a few key figures throughout history. The focus here is not on the Scientific Method per se, but rather on the use of replication or repeated experimentation.\n\n11.2.1 Ibn al Haytham\n\n\n\nPortrait of Ibn Al Haytham by Zargar Zahoor.\n\n\nOne of the earliest proponents of an approach we later would know as the Scientific Method was Ibn al-Haytham, a mathematician, astronomer, physicist, and all-around scholar who lived from 965-1040 CE. Born in Basra, modern-day Iraq, Al Haytham is known as the Father of Optics for his pioneering work on the subject in his famous work كتاب المناظر (Kitab al-Manazir or Book of Optics), which provided empirical evidence that vision occurred when light entered the eyes (intromission), rather than light being emitted by the eyes (extramission), which was argued by Euclid in Optica.\nBesides his many scientific contributions, particularly to the field of Optics, al Haytham made use of repeated systematic observation of natural phenomena to argue his points. Even in his magnum opus Book of Optics, he often ends a description of an experiment with the phrase “this is always found to be so, with no variation or change”, emphasizing that repetition was a central argument of his experimental findings.3\nWhile this may not seem that impressive to us today, consider that Al Haytham was working in his way before we had the term Scientific Method, and about 500 years before Galileo started looking through telescopes! The important takeaway from Al Haytham is that accidental conditions might distort one’s observation of a phenomenon, and we’re better off making multiple observations before making any grand claims.\n\n\n11.2.2 Abu Rayhan Al Biruni\n\n\n\nAl Biruni on the cover of the UNESCO Courier magazine in 1974.\n\n\nAbu Rayhan Muhammad ibn Ahmad al-Biruni was a Muslim polymath who lived from 973 - 1050 CE. He was known to have authored 146 books, with the majority written on mathematics, astronomy, and related subjects. He was also one of the earliest scholars to offer a formal refutation of astrology in contradistinction to astronomy, which he considered a legitimate science based on empiricism. He made contributions to many fields such as physics, astronomy, geography, and Indology (the study of India).\nImportantly, al Biruni contributed to the development of the scientific method, and was one of the first documented scientists who thought about how to separate measurement of a phenomenon from errors in the measurement device. This was remarkably foresighted, as the issue of measurement error is unavoidable in the natural and social sciences. Al Biruni noted that measurement errors can come from different sources such as measurement tools and human judgment, and can compound.4 To address the issue, he recommended taking multiple measurements of a phenomenon and then quantitatively combining them to arrive at a common-sense estimate. This is an approach we still take today with a number of applications, but especially in the use of multiple survey questions to study latent or unobserved phenomena like attitudes, emotions, and perceptions.\n\n\n11.2.3 Roger Bacon\n\n\n\nBacon in his observatory in Merton College, Oxford by Ernest Board.\n\n\nPicking up where Ibn al Haytham left off, Roger Bacon was a philosopher and Franciscan friar from England who lived from 1219 - 1292 CE. He took al Haytham’s scientific method and applied it to works by Aristotle. He was the first person in Europe to describe the formula for gunpowder (it was invented and known about in China already). Crucial for our purposes, he was an early proponent of a method of observation, hypothesis, experimentation, and the need for independent verification as noted in his Opus Tertium, published in 1267. Regarding the latter, he kept very meticulous notes about his experiments, permitting reproducibility by others. This is exactly what we try to do today!\n\n\n11.2.4 Andries van Wesel\n\n\n\nAndries van Wesel revolutionized our understanding of human anatomy.\n\n\nAndries van Wesel (also known as Andreas Vesalius) as a Flemish anatomist who lived from 1514-1564 CE. He is most famous for creating detailed drawings of human anatomy that provided new insights into the subject, correcting a number of errors made by the Greek physician Galen (129-216 CE), and followed by physicians for centuries.\nIn the summer of 1542, he published his magnum opus De humani coporis fabrica libri septem (On the Fabric of the Human Body in Seven Books), which was based on a collection of his lectures delivered at the University of Padua, where he was a professor. Based on his own experiences dissecting corpses (not a common practice among physicians at the time), he was able to show that the Galen never dissected a human corpse because he made many errors in human anatomy.5 Galen instead had relied on dissections of animal corpses, and assumed humans had the same anatomy. Van Wesel was able to show that Galen made 300 errors in human anatomy. Finally, van Wesel was able to create detailed visualizations leveraging newer techniques of printing with woodcut engravings. This was huge, because it allowed other physicians to dissect corpses and verify what van Wesel was claiming. This is one of the earliest examples that I’ve seen of evidence-based medicine triumphing over eminence-based medicine. This meant that you didn’t have to take his word for it, you could actually dissect a corpse and check if what you saw lined up with his illustrations. This is very much in line with how we think about replication today!\n\n\n\n\n\n\nBans on (Dead) Bodies\n\n\n\nDid you know that dissecting human corpses was strictly prohibited by the Catholic Church in the 16th century? Andries van Wesel had to secretly ‘steal’ the bodies of executed criminals to perform his dissections. This allowed him to correct many of Galen’s errors of anatomy. Have a look at his illustrations here.\n\n\n\n\n11.2.5 Francis Bacon\n\n\n\nPortrait of Francis Bacon by Paul van Somer I.\n\n\nSir Francis Bacon was an English philosopher and statesman who lived from 1561 - 1626 CE. He is sometimes called the father of empiricism, but of course we know that he stood on the shoulders of giants who came before him. He put forward a method of inductive reasoning in his magnum opus Novum Organum (New Method) which stressed the importance of making systematic observations of phenomena, and then to generalize the observations to a few axioms. He also noted that it was important to have a skeptical and methodological approach, so that scientists would not mislead themselves, and to not generalize beyond what the facts demonstrate. The latter sentiment is particularly relevant today as authors of scientific papers frequently discuss broad-reaching implications of their findings, which really cannot generalize beyond their samples. In fact, some have suggested adding a “Constraints on Generality” section to the discussion section of every paper, in which authors clearly state the specific population to which their findings apply.6 I feel like Sir Francis would approve.\nIn addition to his many contributions to the formation of the Scientific Method, Bacon also stressed the need for repeated experimentation to create an increasingly complex knowledge base that is always supported by observable facts. This is highly in line with how we think about replication today, as we aim to support or reject theories of how things work. Finally, Bacon also listed several “idols of the mind”, which he noted as obscuring the path of correct scientific reasoning. These include the human tendency to perceive more order in a system than actually exists, confusion arising from the scientific use of certain words compared with their common usage, and the tendency to follow dogma and not ask questions about the world. These are what we would today refer to as cognitive biases, and they have an important role in inhibiting research transparency and replication, which we will discuss later.\nIn short, Sir Francis reaffirmed the need for repeated experimentation, careful observation, and meticulous record-keeping in the conduct of science.\nWhile many others contributed to the refinement of the Scientific Method over the next 500 years or so, for our purposes to will skip to the 20th century.\n\n\n\n\n1. National Academies of Sciences E. Reproducibility and Replicability in Science.; 2019. doi:10.17226/25303\n\n\n2. Moravcsik A. Transparency in Qualitative Research. SAGE Publications Limited; 2020.\n\n\n3. Steinle F. Stability and Replication of Experimental Results: A Historical Perspective. In: Reproducibility : Principles, Problems, Practices, and Prospects. John Wiley & Sons, Incorporated; 2016. http://ebookcentral.proquest.com/lib/upenn-ebooks/detail.action?docID=4547409\n\n\n4. Glick TF, Livesey SJ, Wallis F. Routledge Revivals: Medieval Science, Technology and Medicine (2006): An Encyclopedia. Taylor & Francis; 2017.\n\n\n5. Zampieri F, ElMaghawry M, Zanatta A, Thiene G. Andreas Vesalius: Celebrating 500 years of dissecting nature. Global Cardiology Science & Practice. 2015;2015(5):66. doi:10.5339/gcsp.2015.66\n\n\n6. Simons DJ, Shoda Y, Lindsay DS. Constraints on Generality (COG): A Proposed Addition to All Empirical Papers. Perspectives on Psychological Science. 2017;12(6):1123-1128. doi:10.1177/1745691617708630"
  },
  {
    "objectID": "replication1.html#mertonian-norms",
    "href": "replication1.html#mertonian-norms",
    "title": "6  A Very Brief History of Replication",
    "section": "6.3 Mertonian Norms",
    "text": "6.3 Mertonian Norms\n\n\n\n\n1. National Academies of Sciences E. Reproducibility and Replicability in Science.; 2019. doi:10.17226/25303\n\n\n2. Moravcsik A. Transparency in Qualitative Research. SAGE Publications Limited; 2020.\n\n\n3. Steinle F. Stability and Replication of Experimental Results: A Historical Perspective. In: Reproducibility : Principles, Problems, Practices, and Prospects. John Wiley & Sons, Incorporated; 2016. http://ebookcentral.proquest.com/lib/upenn-ebooks/detail.action?docID=4547409\n\n\n4. Glick TF, Livesey SJ, Wallis F. Routledge Revivals: Medieval Science, Technology and Medicine (2006): An Encyclopedia. Taylor & Francis; 2017."
  },
  {
    "objectID": "replication1.html#section",
    "href": "replication1.html#section",
    "title": "6  A Very Brief History of Replication",
    "section": "6.3 ",
    "text": "6.3 \n\n\n\n\n1. National Academies of Sciences E. Reproducibility and Replicability in Science.; 2019. doi:10.17226/25303\n\n\n2. Moravcsik A. Transparency in Qualitative Research. SAGE Publications Limited; 2020.\n\n\n3. Steinle F. Stability and Replication of Experimental Results: A Historical Perspective. In: Reproducibility : Principles, Problems, Practices, and Prospects. John Wiley & Sons, Incorporated; 2016. http://ebookcentral.proquest.com/lib/upenn-ebooks/detail.action?docID=4547409\n\n\n4. Glick TF, Livesey SJ, Wallis F. Routledge Revivals: Medieval Science, Technology and Medicine (2006): An Encyclopedia. Taylor & Francis; 2017."
  },
  {
    "objectID": "replication2.html",
    "href": "replication2.html",
    "title": "7  Mertonian Norms",
    "section": "",
    "text": "Robert K. Merton - the father of modern sociology.\n\n\nRobert K. Merton lived from 1910 - 2003 and made a number of major contributions to sociology, criminology, and the sociology of science. Unlike some of the early figures in the history of replication we saw in Chapter 6, Merton advocated for theorizing to begin with middle-range theory, or starting with aspects of a phenomenon that are clearly understood, constructed with observed data. He intended this to be a middle ground between a grand theory and a description of a phenomenon. This is remarkably contemporary, as a great deal of middle range theory is about underlying mechanisms that give rise to phenomena.1\nWhile Merton made many valuable contributions to social science, we are primarily concerned with his notion of Mertonian Norms, which are given by the acronym CUDOS:\n\nMertonian Norms of Science\n\n\n\n\n\n\nComponent\nDescription\n\n\n\n\nCommunalism (called Communism by Merton)\nPublic ownership of scientific goods.\n\n\nUniversalism\nValidity of findings as independent of the status of human subjects.\n\n\nDisinterestedness\nScientific institutions act for the benefit of a common scientific enterprise, and not for personal gain.\n\n\nOrganized Skepticism\nScientific claims should undergo critical scrutiny before being accepted.\n\n\n\nReflecting on the history of science, Merton noted that scientists could no longer isolate themselves from the concerns of society.2 He could have been a poet, because his descriptions of this observation are loaded with powerful imagery that summarize his position nicely. Let’s look at a few of these quotes with my crude interpretations interspersed.\n\nMerton, please forgive me my crude interpretations of your dense, though mellifluous, prose.\n\n\n\n\n\n\nMerton’s Quote\nMy Crude Interpretation\n\n\n\n\nA tower of ivory becomes untenable when its walls are under prolonged assault.\nHey scientists, you can only pretend society doesn’t exist for so long, until you can’t.\n\n\nA frontal assault on the autonomy of science was required to convert this sanguine isolationism into realistic participation in the revolutionary conflict of cultures.\nHey scientists, when bombs start falling, you better get in the game!\n\n\nScience is a deceptively inclusive word which refers to a variety of distinct though interrelated items.\nLike, what even is Science?\n\n\nThe ethos of science is that affectively toned complex of values and norms which is held to be binding on the man of science. The norms are expressed in the form of prescriptions, proscriptions, preferences, and permissions. They are legitimized in terms of institutional values.\nScience is done a particular way, and you can see that in a bunch of places.\n\n\n\nAs you will note, Merton was interested in the way science was done, and with its explicit and implicit beliefs, norms, and aspirations. In short, he was interested in spelling out an ethos of science, or the character of ideal scientific inquiry. Let’s make sure we have a good sense of each of his norms (i.e. CUDOS) before proceeding.\nHe believed that science should produce goods under common ownership of the community (he called it Communism but scientists instead often call this Communalism because the former has…certain connotations unrelated to science). The idea here is that a major discovery in science in not the exclusive property of the discoverer. You might get something named after you (known as Eponymy, such as Boyle’s Law or Schrödinger’s cat), and maybe some recognition, but you don’t get to own the discovery as this contradicts the scientific ethic. Secrecy in the antithesis of this norm, and “full and open communication” is related to putting the norm into practice. Merton notes that this norm is incompatible with the definition of technology as private property.\n\n\n\n\n\n\nReal-World Applications: Communalism\n\n\n\nA study investigated 36 top ranked apps for depression and smoking cessation for Android and iOS in the US and Australia in 2018.3 The study found that of the 36 apps, 29 shared user data to Google and Facebook, and only 12 of these accurately disclosed this information in a privacy policy. How does sharing findings on users with commercial entities align with the Mertonian norm of Communalism.\n\n\n\n\n\n\n1. Cartwright N. Middle-range theory: Without it what could anyone do? THEORIA An International Journal for Theory, History and Foundations of Science. Published online September 2020. Accessed June 13, 2023. https://ojs.ehu.eus/index.php/THEORIA/article/view/21479\n\n\n2. Merton RK. Science and technology in a democratic order, reprinted as The normative structure of science. Published online 1942. https://sciencepolicy.colorado.edu/students/envs_5110/merton_sociology_science.pdf\n\n\n3. Huckvale K, Torous J, Larsen ME. Assessment of the Data Sharing and Privacy Practices of Smartphone Apps for Depression and Smoking Cessation. JAMA Network Open. 2019;2(4):e192542. doi:10.1001/jamanetworkopen.2019.2542"
  },
  {
    "objectID": "replication2.html#background",
    "href": "replication2.html#background",
    "title": "12  Mertonian Norms and Counter-Norms",
    "section": "12.1 Background",
    "text": "12.1 Background\nRobert K. Merton lived from 1910 - 2003 and made a number of major contributions to sociology, criminology, and the sociology of science. Unlike some of the early figures in the history of replication we saw in Chapter 11, Merton advocated for theorizing to begin with middle-range theory, or starting with aspects of a phenomenon that are clearly understood, constructed with observed data. He intended this to be a middle ground between a grand theory and a description of a phenomenon. This is remarkably contemporary, as a great deal of middle range theory is about underlying mechanisms that give rise to phenomena.1\nReflecting on the history of science, Merton noted that scientists could no longer isolate themselves from the concerns of society.2 He could have been a poet, because his descriptions of this observation are loaded with powerful imagery that summarize his position nicely. Let’s look at a few of these quotes with my crude interpretations interspersed.\n\nMerton, please forgive me my crude interpretations of your dense, though mellifluous, prose.\n\n\n\n\n\n\nMerton’s Quote\nMy Crude Interpretation\n\n\n\n\nA tower of ivory becomes untenable when its walls are under prolonged assault.\nHey scientists, you can only pretend society doesn’t exist for so long, until you can’t.\n\n\nA frontal assault on the autonomy of science was required to convert this sanguine isolationism into realistic participation in the revolutionary conflict of cultures.\nHey scientists, when bombs start falling, you better get in the game!\n\n\nScience is a deceptively inclusive word which refers to a variety of distinct though interrelated items.\nLike, what even is Science?\n\n\nThe ethos of science is that affectively toned complex of values and norms which is held to be binding on the man of science. The norms are expressed in the form of prescriptions, proscriptions, preferences, and permissions. They are legitimized in terms of institutional values.\nScience is done a particular way, and you can see that in a bunch of places.\n\n\n\nAs you will note, Merton was interested in the way science was done, and with its explicit and implicit beliefs, norms, and aspirations. In short, he was interested in spelling out an ethos of science, or the character of ideal scientific inquiry. While Merton made many valuable contributions to social science, we are primarily concerned with his notion of Mertonian Norms, which are given by the acronym CUDOS."
  },
  {
    "objectID": "replication2.html#cudos",
    "href": "replication2.html#cudos",
    "title": "12  Mertonian Norms and Counter-Norms",
    "section": "12.2 CUDOS",
    "text": "12.2 CUDOS\n\nMertonian Norms of Science\n\n\n\n\n\n\nComponent\nDescription\n\n\n\n\nCommunalism (called Communism by Merton)\nPublic ownership of scientific goods.\n\n\nUniversalism\nValidity of findings as independent of the status of human subjects.\n\n\nDisinterestedness\nScientific institutions act for the benefit of a common scientific enterprise, and not for personal gain.\n\n\nOrganized Skepticism\nScientific claims should undergo critical scrutiny before being accepted.\n\n\n\nLet’s make sure we have a good sense of each of his norms (i.e. CUDOS) before proceeding.\n\n12.2.1 Communalism\n\nMerton believed that science should produce goods under common ownership of the community (he called it Communism but scientists instead often call this Communalism because the former has…certain connotations unrelated to science). The idea here is that a major discovery in science in not the exclusive property of the discoverer. You might get something named after you (known as Eponymy, such as Boyle’s Law or Schrödinger’s cat), and maybe some recognition, but you don’t get to own the discovery as this contradicts the scientific ethic. Secrecy in the antithesis of this norm, and “full and open communication” is related to putting the norm into practice. Merton notes that this norm is incompatible with the definition of technology as private property.\n\n\n\n\n\n\nReal-World Applications: Communalism\n\n\n\nA study investigated 36 top ranked apps for depression and smoking cessation for Android and iOS in the US and Australia in 2018.3 The study found that of the 36 apps, 29 shared user data to Google and Facebook, and only 12 of these accurately disclosed this information in a privacy policy. How does sharing findings on users with commercial entities align with the Mertonian norm of Communalism?\n\n\n\n\n12.2.2 Universalism\n\nThe acceptance or rejection of particular research claims should be impersonal. It should not depend on the author’s race, gender, nationality, alma mater, or any personal characteristic. Merton believed that the impersonal character of science was deep and rooted in Universalism. He was also realistic and understood that the larger culture within which science is housed can be antagonistic to the norm of Universalism. In particular, nationalistic bias and related forces can create the situation where “the man of science may be converted into a man of way - and act accordingly.”2 Knowledge cannot be advanced and furthered if scientific careers are restricted on grounds other than competence, according to Merton. It shouldn’t matter who you are as long your work is competent and adheres to the standard of quality in a field. Unfortunately, implicit and explicit can creep in during process of hiring researchers, the peer review process, and the consumption of research. This is why many journals operate using a double-blind process, whereby neither the authors nor the reviewers know the identity of the other.\n\n\n\n\n\n\nLess than Universal - A Personal Anecdote\n\n\n\nWhen I was an undergraduate research assistant at the University of Michigan in the US, I once brought a paper to the attention of my supervisor, who was a young PhD student educated in the US. When they asked what journal the paper came from, I replied “the Indian Journal of Psychology.” Without looking at the paper, they said “Hmm, see if you can find a paper from a better journal.” I dutifully went looking for another paper, but not before reflecting “Why didn’t they look at the study before making a decision on its quality?” As an Indian studying in the US, this experience illustrated the mental shortcuts frequently made in academia, illustrating a stark deviation from the norm of Universalism.\n\n\nWhen I first read about the Mertonian norm of Universalism, my first thought was of a specific scene in Antoine de Saint-Exupéry’s classic Le Petit Prince [The Little Prince].4 The scene is presented in Figure 12.1 .\n\n\n\nFigure 12.1: Le Petit Prince (1943). Reynal & Hitchcock. No one believed the astronomer when he wore non-European clothing. But when he donned a three-piece suit, they listened then. A violation of the norm of Universalism.\n\n\nIn this scene, a Turkish astronomer who discovered an asteroid and tried to present the results of his discovery to his peers at an international astronomy conference. However, his peers did not believe his findings because he was dressed in traditional Turkish garb of a fez and a billowing set of shalwar pants. Years later, a Turkish dictator forces his subjects to wear European-style clothing under pain of death. This time, the Turkish astronomer presented his findings at the same conference but with a more European outfit. His peers then immediately believe his findings.\n\n\n12.2.3 Disinterestedness\n\nWhen it comes to science, researchers shouldn’t be motivated by commercial pursuits, political interests, or the desire for individual status and prestige. The norm of Disinterestedness is about individual motivations to do science, and that these should be for the sake of science itself. I see this more as Purity of Intention. Merton noted that the norm of Disinterestedness is supported by the fact that scientists are accountable to their peers. If you’re going to do partisan politics under the guise of science, your professional peers are going to point that out, and illustrate how its not science. In essence, your colleagues will largely see through you and understand what you are doing.\nThis is the most aspirational norm, if you ask me. Consider that in academia hiring and promotion is usually heavily influenced by how many papers you publish or how much grant money you can secure. In fact, the whole purpose of this course is to show how to do ethical programming and statistical analysis, with a number of illustrative cases where this is not the case. Additionally, consider the incentives of funders of research as well, especially private organizations that have their own motives and priority areas. While this norm is not especially realistic, it’s a good thing to strive for, and an important one when it comes to the interpretation of data.\n\n\n12.2.4 Organized Skepticism\n\nThe norm of Organized Skepticism is an institutional and methodological mandate, according to Merton. Basically, for science to work, this has to be part of the process. Science is concerned with a detached scrutiny of beliefs in terms of empirical and other logical criteria. This is related to the process of peer review, in which experts provide critical scrutiny for articles submitted to scientific journals. Merton points to the fact that this norm has often involved questioning the ways things are done, and established beliefs. This can sometimes lead to negative consequences for scientists that question the power assumptions of certain institutions.\n\n\n\nGalileo’s Organized Skepticism was not appreciated by the Catholic Church. Wikimedia Commons."
  },
  {
    "objectID": "replication2.html#organized-skepticism",
    "href": "replication2.html#organized-skepticism",
    "title": "7  Mertonian Norms",
    "section": "7.3 Organized Skepticism",
    "text": "7.3 Organized Skepticism\n\nThe norm of Organized Skepticism is an institutional and methodological mandate, according to Merton. Basically, for science to work, this has to be part of the process. Science is concerned with a detached scrutiny of beliefs in terms of empirical and other logical criteria. This is related to the process of peer review, in which experts provide critical scrutiny for articles submitted to scientific journals. Merton points to the fact that this norm has often involved questioning the ways things are done, and established beliefs. This can sometimes lead to negative consequences for scientists that question the power assumptions of certain institutions.\n\n\n\nGalileo’s Organized Skepticism was not appreciated by the Catholic Church. Wikimedia Commons.\n\n\n\n\n\n\n1. Cartwright N. Middle-range theory: Without it what could anyone do? THEORIA An International Journal for Theory, History and Foundations of Science. Published online September 2020. Accessed June 13, 2023. https://ojs.ehu.eus/index.php/THEORIA/article/view/21479\n\n\n2. Merton RK. Science and technology in a democratic order, reprinted as The normative structure of science. Published online 1942. https://sciencepolicy.colorado.edu/students/envs_5110/merton_sociology_science.pdf\n\n\n3. Huckvale K, Torous J, Larsen ME. Assessment of the Data Sharing and Privacy Practices of Smartphone Apps for Depression and Smoking Cessation. JAMA Network Open. 2019;2(4):e192542. doi:10.1001/jamanetworkopen.2019.2542\n\n\n4. de Saint-Exupéry A. Le petit prince [the little prince]. Verenigde State van Amerika: Reynal & Hitchkock (US), Gallimard (FR). Published online 1943."
  },
  {
    "objectID": "replication2.html#counter-norms",
    "href": "replication2.html#counter-norms",
    "title": "12  Mertonian Norms and Counter-Norms",
    "section": "12.3 Counter-Norms",
    "text": "12.3 Counter-Norms\nMerton’s norms have been quite influential in the sociology of science field, and later work by Mitroff produced a useful list of counter-norms to Merton’s norms.5 These counter-norms are the opposite of Merton’s norms, and nicely capture how science is often conducted in reality. We can refer to them when thinking about the conduct of science as falling on a spectrum with the poles being the norms and counter-norms.\n\n12.3.1 Particularism (in contrast with Universalism)\nThe idea here is that the acceptance or rejection of a claim depends in large part on the characteristics of who makes the claim. Recall the Turkish astronomer in the Petit Prince example, and how people accepted his claims only after he donned the clothing they deemed to be acceptable. If someone prejudges the claims of scientist from Harvard University as being automatically more credible and valid than those of someone from Jadavpur University in Kolkatta (where my dad went to college incidentally), India without actually seeing their claims, this would be a classic case of Particularism. In fact, they might be implicitly biased against the claimant from Jadavpur University even if they read their study.\nSadly, given what we know about human judgment and how often it relies on quick, mental shortcuts, this counter-norm appears quite realistic. The key then is to design systems of review (such as double-blind peer review) that acknowledge the possibility of Particularism.\n\n\n\n\n\n\nRacial Discrimination as a Form of Particularism\n\n\n\n\n\n\nProfessor David Blackwell - A Trailblazer in Statistics. The Bancroft Library, UC Berkeley Library.\n\n\nRacial discrimination is a commonly-seen form of Particularism whereby scientists are judged on the basis of their race, rather than on the credulity of their claims. David Blackwell was a very smart man, and made many important contributions to statistics and mathematics including to game theory, probability theory, and Bayesian statistics. He was true luminary in the field, and blazed a trail for others as the first African American elected to the National Academy of Sciences.\nBlackwell was a postdoc at the Institute of Advanced Study (IAS) in Princeton in 1941. Usually, an IAS postdoc would receive a Visiting Fellow appointment at Princeton University, but Blackwell was not afforded this because he was Black. The President of Princeton University was unhappy that IAS had admitted a Black man in the first place.\nIn 1942, Blackwell had interviewed for a position at UC Berkeley. His PhD advisor Joseph Doob told Jerzy Neyman, the founder of the UC Berkeley Department of Statistics, that among his good students “…Blackwell is the best. But of course he is black. And in spite of the fact that we are ina war that’s advancing the cause of democracy, it may not have spread throughout our own land.”6 This was a pretty spot-on observation by Doob, as Blackwell was not hired at UC Berkeley. The reason? The wife of the department head refused to have a Black man in her house because new faculty members customarily attended dinner at the department head’s house.\nBlackwell went on to teach at Howard University for 10 years, and also worked at the RAND corporation. In 1954, Blackwell was hired at UC Berkeley at last, and stayed there for the rest of his long and storied career.\n\n\n\n\n12.3.2 Solitariness (in contrast with Communalism)\nSecrecy is the name of the game with this counter-norm. Here, property rights are extended to scientific discoveries. This is actually what one commonly sees when business interests intersect with academic ones. Think about Big Tech companies. They have a strong financial incentive to deliver value to their shareholders through monetizing and protecting their intellectual property. That these are also scientific discoveries illustrates a deviation from the norm of Communalism.\nThe lines are not always so stark however, and my personal opinion is that there is room for scientific communalism as well as commercial privatization in a number of cases. For example, the founders of Google, Larry Page and Sergey Brin, authored a journal article in 1998 in which they break down their famous PageRank algorithm.7 This provided a valuable contribution to the computer science literature, while also forming the basis of Google Search, which still accounts for just over half of Google’s revenues.8\n\n\n12.3.3 Interestedness (in contrast with Disinterestedness)\nIn contrast with Disinterestedness, this counter-norm illustrates the temptation of self-interest and prestige as a chief motivating factor behind scientific discovery. Scientists then work not to for the sake of science, but to secure more funding, more renown, and better positions at more prestigious institutions. This is one of the more realistic counter-norms, in my opinion, as hiring and promotion tend to emphasize a scientist’s output, rather than a dedication to science for the sake of science. The culture of Publish or Perish means that you NEED to be cranking out papers to get a tenure-track position, and certainly to achieve tenure. Consider the implications of this incentive - the system will reward you for papers (be they fraudulent or improperly produced) rather than for producing transparent and robust unpublished work.\nThis incentive creeps into the academic process in a number of ways. Consider how the prestige of a scientific journal often hinges on how well-cited and influential its articles are, and how this encourages more sensational, novel papers rather than carefully-crafted replications or less sexy, though important, findings. Journals are more likely to publish findings that are statistically significant than findings that are not statistically significant, and this phenomenon is known as Publication Bias.9 This leads to all sorts of distortions of the evidence base which we shall discuss later in the course.\n\n\n\n\n\n\nSelf-Citations: Creating your own Fame\n\n\n\nIn 2018, a prominent professor of human development, Robert Sternberg, resigned as editor of the journal Perspectives on Psychological Science. His colleagues were not happy that he kept citing himself and publishing in the journal in which he was the Editor. In one of his articles, 10/17 citations (~59%) are his own.10 Other articles had similarly troubling levels of self-citations.11 He was also criticized for publishing several commentaries in the journal without peer review, and with the benefit of garnering multiple citations.\nCiting your own work is not a bad thing on its own, but when the majority of your citations are your own, that’s usually a problem! Consider how it may indicate an echo-chamber approach to science, where you are not actually doing the work of reviewing the literature. The case of Robert Sternberg illustrates how once a habit becomes entrenched, it can be very difficult to see the shortcomings and to stop doing it.\n\n\n\n\n12.3.4 Organized Dogmatism (in contrast with Organized Skepticism)\nUnder this counter-norm, scientists take all the credit for their discoveries, while placing the blame for any inadequacies on previous work by others. Far from doubting their own findings, under Organized Dogmatism scientists promote their own findings with a deep conviction, regardless of what others are saying or doing.\n\n\n\n\n\n\nDogmatism: The Ultimate Close-Minded Approach\n\n\n\nA useful definition of Dogmatism is provided by the late psychologist Milton Rokeach:\nA relatively closed cognitive organization of beliefs and disbeliefs about reality, organized around a central set of beliefs about absolute authority which, in turn, provides a framework for patterns of intolerance and qualified tolerance toward others.12\nIn sum, dogmatism sounds like this “I’m right. I know I’m right. Anyone who says otherwise is wrong.”\n\n\nImagine working on a specific area of research for most or all of your career. You become THE leading expert in the topic, and others defer to your authority…for a time. Just as you start to wane in fame, a young hotshot is putting out research that directly critiques your fundamental findings. Will you take an organized skeptical approach and re-assess your work (you should!), or will you instead fight back vociferously and vocally, in spite of mounting evidence against you? While I would love for scientists to be detached enough to scrutinize their own work even if they are the leading expert on it, unfortunately when one’s identity becomes intertwined with their work, it can be quite difficult to be objective.\nHow do you avoid Organized Dogmatism? Lead with intellectual humility. One excellent example of this is psychologist Julia Rohrer’s Loss of Confidence Project in which she and others admit mistakes that they have made in their previous research. This is a remarkable departure from the tempting counter-norms of Organized Dogmatism and Interestedness, because instead of lauding one’s own accomplishments, it publicly declares instances of one’s errors. Normalizing errors and reporting them is a noble effort, and one that can fight against the counter-norms impeding the progress of science.\nAs pointed out by Vox science and health editor Brian Resnick,13 there are at least three key challenges to humility (which I paraphrase):\n\nOur minds (even the geniuses among us) have blindspots and are more imperfect than we are willing to admit.\nWe need a culture that celebrates the words “I was wrong.”\nWe will never achieve perfect intellectual humility, so we need to be thoughtful with our convictions."
  },
  {
    "objectID": "replication2.html#conclusion",
    "href": "replication2.html#conclusion",
    "title": "12  Mertonian Norms and Counter-Norms",
    "section": "12.4 Conclusion",
    "text": "12.4 Conclusion\nIn short, Mertonian Norms are aspirational and admirable. We should strive to do science in accordance with these norms. However, let’s also be real and recognize that the counter-norms often motivate our behavior. We should be cognizant of them, and engage in habits and practices that actively minimize their influence on our behavior such as transparency and humility, and should strive to enact systems that are consonant with Mertonian Norms such as double-blind peer review, and keeping a lid on self-citations.\n\n\n\n\n1. Cartwright N. Middle-range theory: Without it what could anyone do? THEORIA An International Journal for Theory, History and Foundations of Science. Published online September 2020. Accessed June 13, 2023. https://ojs.ehu.eus/index.php/THEORIA/article/view/21479\n\n\n2. Merton RK. Science and technology in a democratic order, reprinted as The normative structure of science. Published online 1942. https://sciencepolicy.colorado.edu/students/envs_5110/merton_sociology_science.pdf\n\n\n3. Huckvale K, Torous J, Larsen ME. Assessment of the Data Sharing and Privacy Practices of Smartphone Apps for Depression and Smoking Cessation. JAMA Network Open. 2019;2(4):e192542. doi:10.1001/jamanetworkopen.2019.2542\n\n\n4. Saint-Exupéry A de. Le petit prince [The little prince]. Verenigde State van Amerika: Reynal & Hitchkock (US), Gallimard (FR). Published online 1943.\n\n\n5. Mitroff II. Norms and Counter-Norms in a Select Group of the Apollo Moon Scientists: A Case Study of the Ambivalence of Scientists. American Sociological Review. 1974;39(4):579-595. doi:10.2307/2094423\n\n\n6. Cattau D. David Blackwell, ’Superstar’. The University of Illinois Alumni Association – Illinois Alumni Magazine. Published online 2010.\n\n\n7. Brin S, Page L. The Anatomy of a Large-Scale Hypertextual Web Search Engine. Computer Networks. 1998;30:107-117. Accessed June 21, 2023. http://www-db.stanford.edu/~backrub/google.html\n\n\n8. Zaveri B. Google’s Revenue By Segment (2016-2023). Business Quant. Published online February 2020. Accessed June 21, 2023. https://businessquant.com/google-revenue-by-segment\n\n\n9. Easterbrook PJ, Gopalan R, Berlin J, Matthews DR. Publication bias in clinical research. The Lancet. 1991;337(8746):867-872.\n\n\n10. Sternberg RJ. \"Am I Famous Yet?\" Judging Scholarly Merit in Psychological Science: An Introduction. Perspectives on Psychological Science: A Journal of the Association for Psychological Science. 2016;11(6):877-881. doi:10.1177/1745691616661777\n\n\n11. Flaherty C. Revolt Over an Editor. Inside Higher Ed. Published online 2018. Accessed June 21, 2023. https://www.insidehighered.com/news/2018/04/30/prominent-psychologist-resigns-journal-editor-over-allegations-over-self-citation\n\n\n12. Rokeach M. The nature and meaning of dogmatism. Published online 1954.\n\n\n13. Resnick B. Intellectual humility: The importance of knowing you might be wrong. Vox. Published online January 2019. Accessed June 21, 2023. https://www.vox.com/science-and-health/2019/1/4/17989224/intellectual-humility-explained-psychology-replication"
  },
  {
    "objectID": "intro.html#exercises",
    "href": "intro.html#exercises",
    "title": "5  Introduction to R",
    "section": "5.8 Exercises",
    "text": "5.8 Exercises\nIt’s a good idea to attempt these right away after reading this section while the content is fresh. You can find the answers in Appendix A.\n\nCalculate \\(89+9/(4*5)^2\\) and assign the value to the object solution1. Then print solution1.\nWhat is the difference between R and R Studio?\nHow do you add a comment to a Script file?\nWhat are packages in R, and how do you install them?\nInstall and load the package rstudioapi.\nHow do you modify the appearance of R Studio?\nAssign the value of 365 to an object called year. Then, create another object called month and assign it the value year/12.\nCreate three variables named Cowboys, Giants, and Commanders and assign them all the value \"Inferior Team\" using multiple assignment operators."
  },
  {
    "objectID": "app1.html",
    "href": "app1.html",
    "title": "Appendix A — Answers for Section 3.8",
    "section": "",
    "text": "Calculate \\(89+9/(4*5)^2\\) and assign the value to the object solution1. Then print solution1.\n\n\nsolution1 &lt;- (89+9)/(4*5)^2 \nprint(solution1)\n\n\nWhat is the difference between R and R Studio?\n\nBase R refers to the statistical programming language and application installed on your computer to process the R programming language. RStudio is an Integrated Development Environment (IDE) that integrates with R to provide much more functionality. You can use base R without RStudio, but not the other way around.\n\nHow do you add a comment to a Script file?\n\n\n# Just add a hash/pound sign to the left. \n#### You can add more hashes for aesthetic purposes ####\n\n# Multi-line comments require a hash \n# starting on the left of each line.\n\n\nWhat are packages in R, and how do you install them?\n\nR packages are user-written collections of functions, compiled code, and sample data. There are over 9000+ packages in R and counting. We use packages for specific things we want to do that we cannot accomplish with the functions in base R, or to do things easier or more efficiently than base R functions.\nMost packages that have been vetted and checked are available on the Comprehensive R Archive Network (CRAN), which is the central R package repository.” “In most cases, installing a package in R is accomplished with the following code install.packages(\"name of package\").\n\nInstall and load the package rstudioapi.\n\n\ninstall.packages(\"rstudioapi\")\nlibrary(rstudioapi)\n\n\nHow do you modify the appearance of R Studio?\n\nTools –&gt; Global Options –&gt; Appearance –&gt; Editor Theme.\n\nAssign the value of 365 to an object called year. Then, create another object called months and assign it the value year * 0.032854884083862.\n\n\nyear &lt;- 365\nmonth &lt;- year * 0.032854884083862\n\n\nCreate three variables named Cowboys, Giants, and Commanders and assign them all the value \"Inferior Team\" using multiple assignment operators.\n\n\nCowboys &lt;- Giants &lt;- Commanders &lt;- \"Inferior Team\""
  },
  {
    "objectID": "datatypes.html",
    "href": "datatypes.html",
    "title": "8  Data Types",
    "section": "",
    "text": "Before we can dive into data management and cleaning, we need to know what types of data can be stored in R. There are five main types of objects that one can use to store data in R. These are summarized in Figure 8.1.\n\n\n\nFigure 8.1: Five types of data in R. Adapted from Grolemund (2014).1\n\n\n\n\n\n\n1. Grolemund G. Hands-on Programming with R: Write Your Own Functions and Simulations. \" O’Reilly Media, Inc.\"; 2014."
  },
  {
    "objectID": "datatypes.html#vectors",
    "href": "datatypes.html#vectors",
    "title": "4  Data Types",
    "section": "4.1 Vectors",
    "text": "4.1 Vectors\n\n4.1.1 Types\nA vector is a sequence of data elements of the same type. In R, there are two types of vectors - atomic vectors and lists. Within the category of atomic vectors, there are six types:\n\nLogical (TRUE, FALSE, NA)\nInteger (a type of numeric vector containing only integers)\nDouble (a type of numeric vector which is the default storage type for numbers in R; represents floating point numbers which can’t always be precisely represented by fixed memory)\nCharacter (a vector of strings, which are pieces of text. Each string is surrounded by quotes \" \".)\nComplex (a vector of elements that include complex numbers)\nRaw (a vector containing a ‘raw’ sequence of bytes; very unusual data type)\n\nYou should note that Doubles and Integers are both numeric vectors, but Doubles are far more common data structures that we typically deal with in social science research. Doubles are quite flexible in that they can be written in decimal, scientific, or hexadecimal form. They also have three unique values: Inf (Infinity), -Inf (Negative Infinity), and NaN (not a number). In practical usage, if you get a value corresponding to any of these three values, something has probably gone wrong. For instance, let’s see what happens when you try to divide certain values by 0 (which is, of course, undefined):\n\n54/0  \n\n[1] Inf\n\n-54/0  \n\n[1] -Inf\n\n0/0 \n\n[1] NaN\n\n\nIntegers cannot contain fractional values (i.e. no decimals), and are written like Doubles but are followed by an L as in 2L, 3L and so on. Generally, we won’t have to worry about these since we will usually be working with Doubles, Logical, or Character vectors.\nThe most common vectors we will deal with are Logical, Double, and Character vectors. Atomic vectors contain elements of the same type. You can always check the type of vector with the typeof() or class() commands. Additionally, you can see the number of elements in a given vector with the length() command.\n\n\n4.1.2 Creating Vectors\nAs mentioned, vectors can comprise strings (pieces of text), integers, or logical values. While they can also comprise a sequence of bytes and complex numbers, we will not discuss these as they are less relevant for our purposes. Let’s start with creating a numeric vector, which can be an integer or double (it doesn’t matter for our purposes). The most common way to create a vector is to use the concatenate or combine c() function with values in parentheses separated by commas. To create a character vector, you need to surround each element of the vector in quotes \" \" separated by commas. The paste() function can also be useful in combining vectors.\nYou can also create a vector comprising a sequence of numbers using the colon :.\n\n# Create two numeric vectors.\na &lt;- c(1, 2, 3, 4)\nb &lt;- c(5, 6, 7, 8)\n\n# You can do the same thing with a colon instead.\na &lt;- c(1:4)\nb &lt;- c(5:8)\n\n# Create a new vector c that combines vectors a and b. \nc &lt;- c(a,b)\nprint(c)\n\n[1] 1 2 3 4 5 6 7 8\n\n# Create two character vectors\nd &lt;- c(\"Nirvana\", \"Alice in Chains\")\ne &lt;- c(\"Soundgarden\", \"Pearl Jam\")\n\n# Create a new vector f that combines vectors d and e.\nf &lt;- c(d,e)\n\n# Create three character vectors.\ng &lt;- \"Four score and seven years ago our fathers brought forth on this continent,\"\nh &lt;- \"a new nation, conceived in Liberty, and dedicated to the\"\ni &lt;- \"proposition that all men are created equal.\"\n\n# Combine the character vectors using the paste() function.\nj &lt;- paste(g,h,i)\nprint(j)\n\n[1] \"Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal.\"\n\n\n\n\n4.1.3 Extracting Elements from a Vector\nTo extract an element from a vector, we use square brackets [] after the vector name, and write the position of the element(s) we want to extract. We can extract multiple elements by using the concatenate or combine c() function, such as vector1[c(1,4,5)]. To extract elements in a sequential range, we can use colon : between positions, such as vector1[c(1:5)].\nLet’s try an example with a visual summary. First, let’s create a character vector called catbreeds with the values \"American Bobtail\", \"Abyssinian\", \"Burmese\", \"Himalayan\", and \"Manx\". We will create the vector with our trusty concatenate or combine c() function. Remember that if you have strings, or pieces of text, these are surrounded by quotes \" \". You don’t need quotes if you are creating any of the other vectors mentioned.\n\n# I like one string per line to keep things organized and neat.\ncatbreeds &lt;- c(\"American Bobtail\",\n               \"Abyssinian\",\n               \"Burmese\",\n               \"Himalayan\",\n               \"Manx\")\n# Now I write the name of the vector to see its elements.\ncatbreeds\n\n[1] \"American Bobtail\" \"Abyssinian\"       \"Burmese\"          \"Himalayan\"       \n[5] \"Manx\"            \n\n\nOk, so now we have a nice character vector called catbreeds where the different strings (or pieces of text) correspond to different cat breeds. Now, how do we extract or print different elements of the vector? This is summarized in Figure 4.2.\n\n\n\nFigure 4.2: Retrieving different elements of a vector by position number.\n\n\nYou can see what this looks like in practice below.\n\n# Let's extract \"American Bobtail\" which is the first element in the vector. \ncatbreeds[1]\n\n[1] \"American Bobtail\"\n\n# Now let's extract three breeds - Abyssinian, Burmese, and Himalayan. \n# These are elements 2, 3, and 4 in the vector.\ncatbreeds[2:4]\n\n[1] \"Abyssinian\" \"Burmese\"    \"Himalayan\" \n\n# What if we want to extract multiple elements that are not sequential? \n# Let's do that with Burmese (element 3) and Manx (element 5).\ncatbreeds[c(3,5)]\n\n[1] \"Burmese\" \"Manx\"   \n\n\nYou can also extract only positive or only negative integers in a vector by specifying the relevant rule within square brackets after the vector name. For example, let’s create a new vector vectorbeta that comprises only the positive integers of vectoralpha. Then we’ll create another new vector vectorgamma that comprises only the negative integers of vectoralpha.\n\n# Create a numeric vector\nvectoralpha &lt;- c(29, 149, 217, -226, 55, 64, -103, -313, 368, 189)\n\n# Create a new vector vectorbeta that takes only positive integers of vectoralpha.  \n# This code says take all the integers greater than 0 in vectoralpha and assign \n# the values to a new vector called vectorbeta. \nvectorbeta &lt;- vectoralpha[vectoralpha &gt; 0] \nprint(vectorbeta)  \n\n[1]  29 149 217  55  64 368 189\n\n# Create a new vector vectorgamma that takes only the negative integers of vectoralpha. \n# This code says take all the integers less than 0 in vectoralpha and assign \n# the values to a new vector called vectorgamma. \nvectorgamma &lt;- vectoralpha[vectoralpha &lt; 0] \nprint(vectorgamma)\n\n[1] -226 -103 -313\n\n\n\n\n4.1.4 Vector Operations\nIn a numeric vector, you can perform a number of operations such as finding the mean mean(), median median(), minimum min(), and maximum max().\n\n# Create a numeric vector\n\nvectoralpha &lt;- c(29, 149, 217, -226, 55, 64, -103, -313, 368, 189)\n\n# Find the minimum and maximum values in the vector. \n\nmin(vectoralpha)\n\n[1] -313\n\nmax(vectoralpha)\n\n[1] 368\n\n# Find the mean and median of the vector.\n\nmean(vectoralpha)\n\n[1] 42.9\n\nmedian(vectoralpha)\n\n[1] 59.5\n\n\nYou can also perform arithmetic operations on vectors. Let’s create a couple of vectors and illustrate the four basic arithmetic operations. Each operation is performed element by element. This means that if both vectors are the same length (same number of elements), the operation will be conducted on the first element of both vectors, then the second element of both vectors, and so on. For example, in vectors a1 and b1 below, addition would involve the following steps: 1 + 2, 3 + 1, and 6 + 4 since these number comprise the first, second, and third elements of each vector, respectively.\n\n# Create two vectors. \na1 &lt;- c(1, 3, 6)\nb1 &lt;- c(2, 1, 4)\n\n# Vector Addition\na1 + b1\n\n[1]  3  4 10\n\n# Vector Subtraction\na1 - b1\n\n[1] -1  2  2\n\n# Vector Multiplication\na1 * b1\n\n[1]  2  3 24\n\n# Vector Division\na1 / b1\n\n[1] 0.5 3.0 1.5\n\n\n\n\n\n\n\n\nDon’t Just Keep Reading!\n\n\n\n\n\n\nManx cat wants you to try coding up a couple of vectors of your own before proceeding.\n\n\nTry creating your own vector and extracting the elements. Once you feel like you’ve got it, only then move on to Matrices. Try creating a numeric vector as well, and extracting its elements using the techniques illustrated above."
  },
  {
    "objectID": "datatypes.html#section",
    "href": "datatypes.html#section",
    "title": "8  Data Types",
    "section": "8.2 ",
    "text": "8.2 \n\n\n\n\n1. Grolemund G. Hands-on Programming with R: Write Your Own Functions and Simulations. \" O’Reilly Media, Inc.\"; 2014."
  },
  {
    "objectID": "datatypes.html#matrices",
    "href": "datatypes.html#matrices",
    "title": "4  Data Types",
    "section": "4.2 Matrices",
    "text": "4.2 Matrices\nA matrix is a two-dimensional vector (i.e. rows and columns.) All columns in matrix should have the same type (e.g. logical, character, numeric) and same length. This means that matrices are homogeneous data structures in that all elements must be of the same type.\nYou can create a matrix from vectors using the rbind() function to combine rows of data, and using the cbind() function to combine columns of data.\n\n# row bind \n\na &lt;- c(.94, .92, .95) \nb &lt;- c(.25, .56, .82) \nc &lt;- c(.65, .45, .37) \nd &lt;- rbind(a, b, c) \n\nprint(d)  \n\n  [,1] [,2] [,3]\na 0.94 0.92 0.95\nb 0.25 0.56 0.82\nc 0.65 0.45 0.37\n\n# column bind  \ne &lt;- c(34.5, 23.6) \nf &lt;- c(22.3, 13.2) \ng &lt;- cbind(e,f) \n\nprint(g)\n\n        e    f\n[1,] 34.5 22.3\n[2,] 23.6 13.2\n\n\nYou can also create a matrix using the matrix() function in which the main arguments are the data vector, the number of rows, and the number of columns. Here is a simple matrix with the numbers 1:9 spread over three rows and three columns:\n\nmatrix(1:9, nrow = 3, ncol = 3)\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\nYou can also change the names of the rows and columns of the matrix using the rownames() and colnames() functions. Let’s try using the matrix() function to input a correlation matrix into R. A correlation matrix is a type of matrix which displays correlation coefficients between variables. Essentially, it shows you the strength of a relationship between pairs of variables ranging from -1 to 1. There are many types of correlation coefficients, but typically Pearson’s \\(r\\) is used. When you look at a correlation matrix, you will notice that 1s are on the diagonal, because a variable is always perfectly correlated with itself. The upper triangle (above the diagonal) is a mirror image of the bottom triangle (below the diagonal), so you only need to focus on one. Personally, I like focusing on the lower triangle, but it doesn’t much matter.\nImagine a study on exam performance and study habits with the following variables:\n\nHours spent studying\nExam Score\nIQ Score\nHours spent sleeping\nSchool Rating\n\nNow let us examine the correlation matrix of the variables of this hypothetical study illustrated in ?fig-cormat1.\n\nIt looks like there is a strong correlation (\\(r = 0.82\\)) between hours spent studying and exam score, which makes sense. We also have smaller correlations between IQ and exam score \\((r = 0.33)\\) and school rating and exam score \\((r = 0.23)\\). All other correlations with exam score are negligible. Let’s now input this correlation matrix into R.\n\n# Input correlations \n\ncorrs &lt;- c(1.00, 0.82, 0.08, -0.22, 0.36, 0.82, \n           1.00, 0.33, -0.04, 0.23, 0.48, 0.33, \n           1.00, 0.06, 0.02, -0.22, -0.04, 0.06, \n           1.00, 0.12, 0.36, 0.23, 0.02, 0.12, \n           1.00)  \n\n# Use matrix() function and specify appropriate rows and columns \n\ncorrmat1 &lt;- matrix(corrs, nrow = 5, ncol = 5)  \n\n# Rename rows and columns \n\ncolnames(corrmat1) &lt;- c(\"Hours studying\", \n                        \"Exam Score\", \n                        \"IQ Score\", \n                        \"Hours sleeping\", \n                        \"School rating\") \n\nrownames(corrmat1) &lt;- c(\"Hours studying\", \n                        \"Exam Score\", \n                        \"IQ Score\", \n                        \"Hours sleeping\", \n                        \"School rating\")  \n\nprint(corrmat1) \n\n               Hours studying Exam Score IQ Score Hours sleeping School rating\nHours studying           1.00       0.82     0.48          -0.22          0.36\nExam Score               0.82       1.00     0.33          -0.04          0.23\nIQ Score                 0.08       0.33     1.00           0.06          0.02\nHours sleeping          -0.22      -0.04     0.06           1.00          0.12\nSchool rating            0.36       0.23     0.02           0.12          1.00\n\n\nTo retrieve an element of a matrix, write the matrix name, followed by square brackets in which the first argument is the row and the second is the column that you want. For example, if we want to retrieve the correlation between \"Hours spent studying\" and \"Exam Score\", we can write the matrix name corrmat1 followed by row 2 and column 1:\n\ncorrmat1[2,1]\n\n[1] 0.82\n\n\nThis retrieves the appropriate correlation of 0.82. Finally, let’s say you want to see the values for an entire column or an entire row. In this case, simply leave the row or column argument blank:\n\n# See the values for column 3 only \ncorrmat1[, 3]  \n\nHours studying     Exam Score       IQ Score Hours sleeping  School rating \n          0.48           0.33           1.00           0.06           0.02 \n\n# See the values for row 4 only \ncorrmat1[4, ] \n\nHours studying     Exam Score       IQ Score Hours sleeping  School rating \n         -0.22          -0.04           0.06           1.00           0.12"
  },
  {
    "objectID": "datatypes.html#dataframes",
    "href": "datatypes.html#dataframes",
    "title": "4  Data Types",
    "section": "4.3 Dataframes",
    "text": "4.3 Dataframes\nA dataframe is the most common type of data structure we typically deal with in social science. It is a two-dimensional labelled list of vectors, and can have columns of multiple data types. A dataframe’s vectors must be of the same length, which gives dataframes a rectangular structure. A dataframe has rownames() and colnames() just like matrices and lists. The variable names in a dataframe also must be different from one another, meaning that the two variables cannot have the exact same name.\nIf you want to explore R’s built-in dataframes to test out functions and analyses, you can run the data() command to see a list of dataframes. Let’s go ahead and use the built-in dataframe mtcars by assigning it to an object called df1 (dataframe 1). Then, let’s look at the dataframe. If you want to have a very quick preview of the first six rows of the dataframe, invoke the head() function, and similarly the last six rows of the dataframe can be previewed using the tail() function. In both cases, the name of the dataframe should be inside the parentheses.\nThe head() and tail() functions are useful when you have a very large dataset and cannot feasibly examine the whole thing. However, I find the View() function far more useful. This actually pulls up the entire dataframe in a separate tab.\n\n# Assign the built-in mtcars dataframe to an object called df1.\ndf1 &lt;- mtcars \n\n# Examine the first six rows of the dataframe.\nhead(df1)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n# Examine the last six rows of the dataframe.\ntail(df1)\n\n                mpg cyl  disp  hp drat    wt qsec vs am gear carb\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.7  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.9  1  1    5    2\nFord Pantera L 15.8   8 351.0 264 4.22 3.170 14.5  0  1    5    4\nFerrari Dino   19.7   6 145.0 175 3.62 2.770 15.5  0  1    5    6\nMaserati Bora  15.0   8 301.0 335 3.54 3.570 14.6  0  1    5    8\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.6  1  1    4    2\n\n# Have a much more detailed look at the dataframe.\nView(df1) \n\nYou can also click the little dataframe icon next to the dataframe in your Environment tab to view the dataframe.\n\nWhether you use the View() function or click the little icon, R will open another tab with the dataframe, and will look something like this:\n\nIf we want to know the dimensions of the dataframe (imagine it’s huge and we can’t easily View() the whole dataset), as well as the structure of the dataset, we can use the functions: nrow() (to see the number of rows), ncol() (to see the number of columns), and str() (see the structure of the dataframe).\n\n# See the number of rows in a dataframe  \nnrow(df1)    \n\n[1] 32\n\n# See the number of columns in a dataframe  \nncol(df1)    \n\n[1] 11\n\n# See the structure of the dataframe  \nstr(df1) \n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n\nNote that the str() function gives you a quick overview of the type of variables and some of their values in the dataframe. You can get this information using the View() function, which I tend to prefer, but if you want to quickly examine the structure of multiple dataframes, for instance, you can use the str() function.\nNotice how the dataframe comprises rows and columns, where the rows are names of cars and the columns are different attributes of those cars, such as miles per gallon, number of cylinders, displacement weight, and others. To retrieve a particular column in the dataframe, use the dollar sign $ operator after the name of the dataframe, followed by the column name. For instance, if you want to know the mean miles per gallon across all the cars, we can use the mean() function in the following manner:\n\n# Get the mean of the mpg variable in the df1 dataframe.\nmean(df1$mpg)\n\n[1] 20.09062\n\n\nFrom the mean() function used above, we see that across all the cars in the dataframe, the average fuel efficiency is about 20 miles per gallon.\nIn general, you will use the format dfname$variable to access or refer to any specific variable or column of a dataframe. You can also add a new variable to the dataframe using the same format. For example, if we wanted to add a new column to our dataset df1 in which we take the existing column qsec (the number of seconds it takes the car to travel one-fourth of a mile) and multiply this by four, to get an estimate of how long it takes the car to travel one mile, we can accomplish by with the following code.\n\n# Create new variable in df1 called qsec2 which takes the existing variable\n# qsec and multiplies it by four.\ndf1$qsec2 &lt;- df1$qsec*4\n\n# The mean of qsec2 is predictably four times the mean of qsec.\nmean(df1$qsec)\n\n[1] 17.84875\n\nmean(df1$qsec2)\n\n[1] 71.395\n\n\nTo get a quantitative summary of numeric variables in a dataframe, we can use the summary() function to obtain the minimum value, 25th percentile, median, 75th percentile, and maximum value. Let’s try that with the weight wt variable. Then, we’ll obtain the information for multiple variables in the dataframe using square brackets [], the concatenation operator c(), and single apostrophes ' ' around the variable names.\n\n# Summary statistics for only the weight variable \nsummary(df1$wt)  \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.513   2.581   3.325   3.217   3.610   5.424 \n\n# Summary statistics for multiple variables \nsummary(df1[c('mpg','wt', 'qsec')]) \n\n      mpg              wt             qsec      \n Min.   :10.40   Min.   :1.513   Min.   :14.50  \n 1st Qu.:15.43   1st Qu.:2.581   1st Qu.:16.89  \n Median :19.20   Median :3.325   Median :17.71  \n Mean   :20.09   Mean   :3.217   Mean   :17.85  \n 3rd Qu.:22.80   3rd Qu.:3.610   3rd Qu.:18.90  \n Max.   :33.90   Max.   :5.424   Max.   :22.90"
  },
  {
    "objectID": "datatypes.html#lists",
    "href": "datatypes.html#lists",
    "title": "4  Data Types",
    "section": "4.4 Lists",
    "text": "4.4 Lists\nWhile dataframes can contain data of different types (heterogeneous data), lists are even more flexible at doing this. A list is like a collection of different things that don’t have to play by the rules of a dataframe. Recall that though dataframes can contain heterogeneous data, the elements of a dataframe are subject to a few restrictions such as a) they have to be a labelled list of vectors, b) they have to be the same length, and c) the variables must not have the exact same name.\nList elements don’t have such restrictions, and you can think of them as collections stuff. For example, you can put the contents of a function, a dataframe, a piece of text, numbers, etc. Let’s look at an example of a list called stuff. We will create this list using the list() function and assigning a bunch of random stuff to it. You can even put lists in a list! That’s pretty meta. You can then View() the list to examine its contents.\n\n# Create a list called stuff comprising various random things.\nstuff &lt;- list(catbreed = \"manx\", \n              data = mtcars, \n              Euler = 2.71828, \n              logical = c(TRUE, FALSE, TRUE, NA),\n              random = list(alphabet = letters, \n                            greeting = \"Hello, World\", \n                            number = pi\n                            )\n              )\n\nOnce you View() the list, it will show you the contents as in ?fig-stufflist.\n\nYou can see that our list has a string (“Manx”), a dataframe (mtcars), a number (Euler), a logical vector (logical), and another list with three elements of its own (random). Lists are so flexible!\nYou can very easily extract elements of a list with the dollar sign $ operator if you know the name of the element, or using double square brackets [[ ]] if you know the position of the element. You can also use single square brackets [ ] which will give you a little more information than double square brackets, such as the name of the element.\n\n# Extract elements of a list by name using dollar sign operator.\nstuff$Euler\n\n[1] 2.71828\n\n# Extract elements of a list by using double square brackets.\nstuff[[3]]\n\n[1] 2.71828\n\n# Extract elements of a list by using single square brackets.\nstuff[3]\n\n$Euler\n[1] 2.71828\n\n\nAs you see, using single brackets [ ] to extract an element from a list gives you the element name Euler as well as the value, as opposed to only the value provided by the double square brackets [[ ]]. In general, if you have a collection of stuff, create a list for easy reference."
  },
  {
    "objectID": "datatypes.html#arrays",
    "href": "datatypes.html#arrays",
    "title": "4  Data Types",
    "section": "4.5 Arrays",
    "text": "4.5 Arrays\nThough we will not work with Arrays in this course, it may be worthwhile to know a thing or two about them. Recall that matrices and dataframes are two-dimensional data structures. An array goes beyond two dimensions, and can hold n-dimensional data, but the data have to be of the same type. I think arrays are easiest to understand when thinking about a collection of multiple matrices.\nFor example, let’s say we have two vectors of different lengths. One vector has three elements, while the other has six elements.\n\n# Create two vectors of different lengths (three and six).\nvector1 &lt;- c(1,3,4)\nvector2 &lt;- c(32, 4, 7, 19, 23, 43)\n\nNow, let’s create an array where we first create a matrix of three rows and three columns based on our vectors, and then duplicate this matrix four times. We can accomplish this using the array() function where the first argument will comprise our vectors, and the argument dim (or dimension) takes the row numbers, column numbers, and number of matrices we want. So if we want three rows, three columns, and four matrices, the values would be dim = c(3,3,4).\n\n# Use previous vectors to create array of four 3x3 matrices. \narray1 &lt;- array(c(vector1, vector2),\n                dim = c(3,3,4)\n                )\n\nprint(array1)\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1   32   19\n[2,]    3    4   23\n[3,]    4    7   43\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]    1   32   19\n[2,]    3    4   23\n[3,]    4    7   43\n\n, , 3\n\n     [,1] [,2] [,3]\n[1,]    1   32   19\n[2,]    3    4   23\n[3,]    4    7   43\n\n, , 4\n\n     [,1] [,2] [,3]\n[1,]    1   32   19\n[2,]    3    4   23\n[3,]    4    7   43\n\n\nSee how we now have four versions of the matrix we requested based on our two vectors? If we want to name our rows, columns and matrices, we can create vectors for those and then feed them into the dimnames (dimension names) argument of array(). The dimnames argument only takes lists, so we’ll stick our name vectors into a list.\n\n# Create row names.\nrownames &lt;- c(\"Row 1\", \n              \"Row 2\", \n              \"Row 3\")\n\n# Create column names.\ncolumnnames &lt;- c(\"Column 1\", \n                 \"Column 2\", \n                 \"Column 3\")\n\n# Create matrix names. \nmatrixnames &lt;- c(\"Matrix Alpha\", \n                 \"Matrix Beta\", \n                 \"Matrix Gamma\",\n                 \"Matrix Delta\")\n\n# Rename the array elements.\n\narray1 &lt;- array(c(vector1, vector2),\n                dim = c(3,3,4),\n                dimnames = list(rownames, \n                             columnnames,\n                             matrixnames)\n                )\n\n# And Voila!\nprint(array1)\n\n, , Matrix Alpha\n\n      Column 1 Column 2 Column 3\nRow 1        1       32       19\nRow 2        3        4       23\nRow 3        4        7       43\n\n, , Matrix Beta\n\n      Column 1 Column 2 Column 3\nRow 1        1       32       19\nRow 2        3        4       23\nRow 3        4        7       43\n\n, , Matrix Gamma\n\n      Column 1 Column 2 Column 3\nRow 1        1       32       19\nRow 2        3        4       23\nRow 3        4        7       43\n\n, , Matrix Delta\n\n      Column 1 Column 2 Column 3\nRow 1        1       32       19\nRow 2        3        4       23\nRow 3        4        7       43\n\n\nWhile more could be said about arrays, we will end here since we really don’t need to worry about them in this course."
  },
  {
    "objectID": "datatypes.html#exercises",
    "href": "datatypes.html#exercises",
    "title": "6  Data Types",
    "section": "6.6 Exercises",
    "text": "6.6 Exercises\nAs always, it’s a good idea to attempt these while the material is still fresh. You can find the answers in Appendix B.\n\nCreate a numeric vector called vec1 comprising the elements 3, -12, 532, 0, -100, 55, and -42. Then, find the median and lowest value in the vector.\nCreate a new vector vec2 which contains all the elements of vec1 that are positive integers. Then create a new vector vec3 that contains all the elements of vec1 that are negative integers. Then create a new vector vec4 which is the sum of vec2 and vec3.\nAssign the built-in dataframe mtcars to an object named after your favorite animal. Then calculate and print the median of the variable mpg.\nCreate a new variable and add to the object you created above (named after your favorite animal). This variable will take the variable Miles per Gallon mpg and convert it to Kilometers per Liter, which you will name kpl. This can be accomplished by taking mpg and dividing it by 2.352. Once you’ve created this new variable and added it to the object, calculate the median of kpl.\n\n\n\n\n\n1. Grolemund G. Hands-on Programming with R: Write Your Own Functions and Simulations. \" O’Reilly Media, Inc.\"; 2014."
  },
  {
    "objectID": "app2.html",
    "href": "app2.html",
    "title": "Appendix B — Answers for Section 4.6",
    "section": "",
    "text": "Create a numeric vector called vec1 comprising the elements 3, -12, 532, 0, -100, 55, and -42. Then, find the median and lowest value in the vector.\n\n\nvec1 &lt;- c(3, -12, 532, 0, -100, 55, -42) \n\nmedian(vec1)\n\n[1] 0\n\nmin(vec1)\n\n[1] -100\n\n\n\nCreate a new vector vec2 which contains all the elements of vec1 that are positive integers. Then create a new vector vec3 that contains all the elements of vec1 that are negative integers. Then create a new vector vec4 which is the sum of vec2 and vec3.\n\n\nvec2 &lt;- vec1[vec1 &gt; 0]\nvec3 &lt;- vec1[vec1 &lt; 0]\nvec4 &lt;- vec2 + vec3\n\n\nAssign the built-in dataframe mtcars to an object named after your favorite animal. Then calculate and print the median of the variable mpg.\n\n\n# I'm a big donkey guy, but you can put your favorite animal for the name of the object.\ndonkey &lt;- mtcars \n\nmedian(donkey$mpg)\n\n[1] 19.2\n\n\n\nCreate a new variable and add to the object you created above (named after your favorite animal). This variable will take the variable Miles per Gallon mpg and convert it to Kilometers per Liter, which you will name kpl. This can be accomplished by taking mpg and dividing it by 2.352. Once you’ve created this new variable and added it to the object, calculate the median of kpl.\n\n\ndonkey$kpl &lt;- donkey$mpg / 2.352\nmedian(donkey$kpl)\n\n[1] 8.163265"
  },
  {
    "objectID": "replication3.html",
    "href": "replication3.html",
    "title": "9  Crises of Replication",
    "section": "",
    "text": "Let’s remind ourselves what Reproducibility and Replicability mean. As you see in ?fig-rep3, Reproducibility involves literally reproducing the results from a study using original data, code, and materials. Replicability is about carrying out the study using the same procedures in different settings and achieving consistent, not identical, results.\n\nNext, let’s either remind ourselves (if you’ve taken a statistics course before) or learn for the first time what is meant by terms Type I Error, Type II Error, and Power which will become relevant for the stories in this chapter.\nWhen we talk about statistical hypothesis testing, we typically have two competing hypotheses:\n\nThe Null Hypothesis (\\(H_0\\)) typically suggests that there is no difference between two quantities or groups on some meaningful parameter. For example, if I’m running a study to test the efficacy of a new vaccine on a new disease, my null hypothesis is that is no effect of the vaccine on the disease.\nThe Alternative Hypothesis (\\(H_A\\)) is typically what we are trying to assert or explore. To use the above example, my alternative hypothesis would be that the new vaccine has a non-zero effect on the new disease.\n\nIf the sample data provide enough evidence against the null hypothesis, we say that we reject the null hypothesis in favor of the alternative hypothesis. If the sample data not provide enough evidence against the null hypothesis, we say that fail to reject the null hypothesis. Note that we can NEVER say that we accept the null hypothesis.\n\n\n\n\n\n\nWhy can’t we ever accept the null hypothesis?\n\n\n\nIn short, we assume the null is true and are trying to find evidence against it, so our decision is always framed in terms of rejecting or failing to reject the null hypothesis. On a more fundamental level, the idea here is that you cannot prove a negative. If your sample data do not support the null hypothesis, you have only shown that and just that. You have not shown that null hypothesis is true. It might be true (and indeed we assume it is when doing the tests), but your study is not exploring its verity, rather your study is trying to find evidence that contradicts the null. It is a sort of proof by contradiction.\n\n\nThe type of testing is extremely common, and is commonly referred to as Null Hypothesis Significance Testing (NHST).\nNow that we’ve got the definitional summary out of the way, let’s talk about what is often called The Replication Crisis in psychology."
  },
  {
    "objectID": "replication3.html#reproducibility-and-replication-definitions",
    "href": "replication3.html#reproducibility-and-replication-definitions",
    "title": "9  Crises of Replication",
    "section": "9.1 Reproducibility and Replication Definitions",
    "text": "9.1 Reproducibility and Replication Definitions\nLet’s remind ourselves what Reproducibility and Replicability mean. As you see in Figure 9.1, Reproducibility involves literally reproducing the results from a study using original data, code, and materials. Replicability is about carrying out the study using the same procedures in different settings and achieving consistent, not identical, results.\n\n\n\nFigure 9.1: Reproducibility and Replicability."
  },
  {
    "objectID": "replication3.html#null-hypothesis-significance-testing",
    "href": "replication3.html#null-hypothesis-significance-testing",
    "title": "13  Some Statistical Preliminaries",
    "section": "13.1 Null Hypothesis Significance Testing",
    "text": "13.1 Null Hypothesis Significance Testing\nNext, let’s either remind ourselves (if you’ve taken a statistics course before) what is commonly referred to as Null Hypothesis Significance Testing (NHST). NHST is a method of statistical inference in which you test the thing you think will happen in relation to a straw-man hypothesis usually corresponding to the notion “Nothing is going to happen.” Let’s break down how it works.\nWhen we talk about statistical hypothesis testing, we typically have two competing hypotheses:\n\nThe Null Hypothesis (H_0) typically suggests that there is no difference between two quantities or groups on some meaningful parameter. For example, if I’m running a study to test the efficacy of a new vaccine on a new disease, my null hypothesis is that is no effect of the vaccine on the disease.\nThe Alternative Hypothesis (H_A) is typically what we are trying to assert or explore. To use the above example, my alternative hypothesis would be that the new vaccine has a non-zero effect on the new disease.\n\nIf the sample data provide enough evidence against the null hypothesis, we say that we reject the null hypothesis in favor of the alternative hypothesis. If the sample data not provide enough evidence against the null hypothesis, we say that fail to reject the null hypothesis. Note that we can NEVER say that we accept the null hypothesis.\nHow do we judge if the data provide evidence for the hypothesis? The method of NHST involves the computation a test statistic, or number that quantifies how close the data match the distribution implied by the null hypothesis (this is commonly a normal or Gaussian distribution). The test statistic allows for the calculation of a p-value, or probability of observing a test statistic at least as extreme or more extreme as the one observed, assuming the null hypothesis true.1 2 If the p-value is less than a particular threshold (typically 0.05, or a one-in-twenty chance, and denoted by the Greek letter alpha \\alpha), then we can say that the p-value is statistically significant. This is illustrated in Figure 13.1.\n\n\n\nFigure 13.1: Null Hypothesis Significance Testing is akin to Proof by Contradiction.\n\n\n\n\n\n\n\n\nWhy can’t we ever accept the null hypothesis?\n\n\n\nIn short, we assume the null is true and are trying to find evidence against it, so our decision is always framed in terms of rejecting or failing to reject the null hypothesis. On a more fundamental level, the idea here is that you cannot prove a negative. If your sample data do not support the null hypothesis, you have only shown that and just that. You have not shown that null hypothesis is true. It might be true (and indeed we assume it is when doing the tests), but your study is not exploring its verity, rather your study is trying to find evidence that contradicts the null. It is a sort of proof by contradiction."
  },
  {
    "objectID": "Chapters/intro.html#what-exactly-is-r-and-why-is-it-called-r-and-not-something-more-informative",
    "href": "Chapters/intro.html#what-exactly-is-r-and-why-is-it-called-r-and-not-something-more-informative",
    "title": "Introduction to R",
    "section": "What exactly is R, and why is it called R and not something more informative?",
    "text": "What exactly is R, and why is it called R and not something more informative?\nR is a computer language and run-time environment which can be used to do many things, including statistical computing and data visualization. I know, the name is weird. How can it be named after a letter?!\nHere’s the deal. R was created by statisticians Ross Ihaka and Robert Gentleman in the early 1990s at the University of Auckland. They developed a coding language to teach introductory statistics based on the syntax of another language at the time called S (programmers loved single-letter names back in the day). Since both their first names (Robert and Ross) started with the letter R, that’s what they chose to call their new language. Seriously.\n\n\n\nRoss Ihaka (right) and Robert Gentleman (left) - the creatoRs.\n\n\nImportantly, Gentleman and Ihaka spoke with their colleagues who convinced them to release the language for free as part of a philosophy called the Free Software Movement. Under this philosophy, “users have the freedom to run, copy, distribute, study, change and improve the software” (see GNU for more details).\nThey created a mailing list to discuss all things R in 1994, which quickly became big enough to overwhelm them with many feature requests and bug reports. They then selected a core group of developers who would maintain R. They also enlisted the support of their colleagues Kurt Hornik and Fritz Leisch at the Technical University of Vienna who created the Comprehensive R Archive Network (CRAN), which is a repository of user contributions.\nToday, R is a thriving language that receives regular updates by the R Core Team, and boasts about 19,657 user-contributed packages on CRAN. R is free, flexible, powerful, and awesome.\n\n\n\nThis art was made in R by Danielle Navarro. That’s right, you can even make art in R."
  },
  {
    "objectID": "Chapters/intro.html#using-r-as-a-calculator",
    "href": "Chapters/intro.html#using-r-as-a-calculator",
    "title": "Introduction to R",
    "section": "Using R as a calculator",
    "text": "Using R as a calculator\nThe most basic use of R is as a calculator. You can execute calculations in a Script file or in the Console directly. In addition to using any real number, the following symbols and operations can also be used:\n\n\n\nCode\nMeaning\n\n\n\n\n+\nAddition\n\n\n-\nSubtraction\n\n\n*\nMultiplication\n\n\n/\nDivision\n\n\n^\nExponentiation\n\n\n( )\nParentheses/Brackets\n\n\nsqrt()\nSquare Root\n\n\nlog()\nNatural Logarithm\n\n\nexp()\nExponential Value\n\n\n\nJust enter the calculation in the Console (or Script File, though the Console is easier for quick calculations) following the appropriate order of operations (PEMDAS). As a matter of style, you want to include a space between arithmetic operators like +,-,*, and/. You also want to add a space after any commas, just like in English. This makes your code more readable.\n\n(6 * 6) + (12 / 2)\n\n[1] 42\n\nsqrt(81) * sqrt(9)\n\n[1] 27\n\n(2 / 3)^(4)\n\n[1] 0.1975309\n\nlog(100)\n\n[1] 4.60517\n\nexp(4.61)\n\n[1] 100.4841\n\n10^5\n\n[1] 1e+05\n\n\nAs you see, the answer is given following the [1], which just refers to the first position in a vector (to be discussed shortly). Note also that R uses scientific notation, so instead of printing 10^5=1000000, R will print 10^5 = 1e+05. The e here is completely unrelated to the number \\(e\\) (Euler’s number). It just means “10 to the power of.”\n\n\n\n\n\n\n“We talking about practice, man” - Allen Iverson\n\n\n\nGo ahead and try doing some calculations of your own in the Console! Especially if you’re new to R, the best way to learn R is by doing R over and over again."
  },
  {
    "objectID": "Chapters/intro.html#comments",
    "href": "Chapters/intro.html#comments",
    "title": "Introduction to R",
    "section": "Comments",
    "text": "Comments\nYou can and should always write comments in your R Script file using a single hash #. It’s a good idea to use - or = to also clearly break up the Script file into chunks. The easiest and cleanest way to do this is to use Section Headers. On a Windows, you can use Ctrl+Shift+R and on Mac, you can use Cmd+Shift+R to quickly add a section header. You can also go to Code –&gt; Insert Section. Then you can very neatly organize your Script File, like so:\n\n# Load Libraries --------------------------------------------------------\n\n# Load Data -------------------------------------------------------------\n\nIt’s a good idea to use comments to explain why you are doing something. For instance, if you are using a specific analytic approach, mention in a comment why you are doing that. I also recommend starting every Script File with the following information on separate commented lines: Title of file, Author, Date, Purpose. Here is an example:\n\n# Data Cleaning\n# By Shaon Lahiri \n# June 14, 2027 \n# This file imports the dataset \"piracy.csv\" and prepares it for data analysis. \n\nBy having clear meta-data for each Script file like this, someone else who reads your code can better understand its purpose, provenance, and format. Additionally, comments on the purpose of a particular line of code can also help you remember why you did something, as it can be easy to forget years later what you had in mind for a particular task.\n\n\n\n\n\n\nKeyboard Shortcuts\n\n\n\nIt can be very useful to know keyboard shortcuts for common tasks. You may want to bookmark this page which lists keyboard shortcuts in R Studio for Mac and Windows."
  },
  {
    "objectID": "Chapters/intro.html#functions",
    "href": "Chapters/intro.html#functions",
    "title": "Introduction to R",
    "section": "Functions",
    "text": "Functions\nA function is a block of code that runs only when it is called. Calling a function just means you are giving the computer a single instruction to do a particular thing. In R, every function comprise a word followed immediately by parentheses function(). We often have information we want to pass to a function, and these are called arguments. Essentially, the function does the thing we want, and the arguments specify how we want them done, or what information we want the function to use.\nLet’s try using the simple print() function to display the phrase “Hello World.”\n\nprint(\"Hello, World!\")\n\n[1] \"Hello, World!\"\n\n\nHere we called the print() function and passed it the argument \"Hello World!\". This resulted in the expected behavior of the value \"Hello, World!\" being printed (i.e. shown on screen).\n\n\n\n\n\n\nWhy “Hello, World!”?\n\n\n\nDid you know that writing code to print “Hello World” is a programming tradition? It’s often the first thing a student learns to write in a programming language, and comes to us from the 1970s in Bell Laboratories. Learn more here.\n\n\nBesides using functions to accomplish various tasks, we can also easily create our own functions in R. Creating your own function is often a way to automate a series of tasks to streamline your work. We will tackle this issue later. For now, it will be sufficient to be familiar with the terms function, argument, and call."
  },
  {
    "objectID": "Chapters/intro.html#r-packages",
    "href": "Chapters/intro.html#r-packages",
    "title": "Introduction to R",
    "section": "R Packages",
    "text": "R Packages\nPackages are user-written contributions which extend the functionality of R. They typically comprise a set of functions, code, documentation, and occasionally some datasets as well. Packages are stored in R as a ‘library’. They only have to installed once, but must be loaded each time you intend to use them. Typically, we only have need for a few packages at a time.\nInstalling a package typically involves communicating with CRAN, and is easily accomplished using the install.packages() function in which the name of the package you want to install should be placed in the parentheses with quotation marks \" \". Go ahead now and give it a shot with the tidyverse package (or set of packages rather) using the following code.\ninstall.packages(\"tidyverse\")\nIf the installation was successful, you should see a message that mentions the package has been successfully unpacked and located somewhere on your computer like this:\n\nSee how it says that the downloaded package is located on my computer? This type of message indicates that there were no problems with the installation. Now, if we want to use this package, it must be explicitly called using the library() function. This is ideally done in a dedicated section called “Load Libraries” or something similar.\n# Load libraries --------------------------\nlibrary(tidyverse)\nNow you know how to install and load packages from CRAN. This is what you will mostly do in R, as CRAN packages are all tested and meet its quality standards. To update a package, I find the easiest, cleanest approach is to go to your Output Pane in the bottom right of the screen, go to the Packages tab, and then hit the Update button.\n\nThat will bring up a menu which gives you several relevant pieces of information:\n\nWhich package(s) have an update available.\nWhat version of the package you have installed vs what version is available.\nWhat changes the update is making under ‘NEWS’.\n\n\nMaybe you don’t want to sit through the updates of EVERY package for which an update is available. This menu allows you to pick and choose the packages you want to update, and then click “Install Updates.” If you have a specific package you want to update and want to do it quickly, you can use the install.packages() function with the package name to be updated in quotes.\ninstall.packages(\"bayesplot\")\nFinally, say you want to just have all your packages up to date, and have some time to let R do its thing while you get a coffee, you can just type:\nupdate.packages(ask=FALSE)\nThis will update every package for which an update is available, and the ask=FALSE argument suppresses a prompt from appearing before every package update.\nFinally, you may be wondering which packages are essential to install and have ready to go. Typically, you know which packages to use based on a particular task you want to accomplish. Thus, I wouldn’t worry about this for now. Any R script file using packages will always have those listed in the file, and thus you will typically know what packages to install and load."
  },
  {
    "objectID": "Chapters/intro.html#assignments",
    "href": "Chapters/intro.html#assignments",
    "title": "Introduction to R",
    "section": "Assignments",
    "text": "Assignments\nIn R, everything is an object. An object is just some data that you have stored. It can be a number, a dataset, a function, or anything else. For example, let’s say we want to perform the following calculations:\n\n((sqrt(81) * sqrt(9)) / log(3.6)) + 6\n((sqrt(81) * sqrt(9)) / log(3.6)) - 12\n((sqrt(81) * sqrt(9)) / log(3.6)) * (2/3)\n\nWe may not want to keep writing ((sqrt(81) * sqrt(9)) / log(3.6)) as it’s cumbersome to work with and to look at. We can streamline the process by assigning ((sqrt(81) * sqrt(9)) / log(3.6)) to a simple name of our choice. Let’s say we assign it to an object called quantity1, the resulting computation would be much easier to write. We can do this by using the assignment operator &lt;- which is comprised of the symbol for less than followed immediately by a minus sign -. So, the code incorporating assignment of the quantity to an object called quantity1 would look like this:\n\nquantity1 &lt;- ((sqrt(81) * sqrt(9)) / log(3.6)) \n\nquantity1 + 6\nquantity1 - 12\nquantity1 * (2/3)\n\nAs you’ll notice, the name of the object always comes first, then the assignment operator &lt;-, and finally the value we are assigning to the object.\nYou can also assign text to an object if you enclose the text in quotes \" \".\n\nmyfavorite &lt;- \"Cat\"\nmyfavorite\n\n[1] \"Cat\"\n\n\nHere, I assigned the value \"Cat\" to an object called myfavorite. I then called that object, and this printed its value.\nOne you assign a value to an object, it will show up in your Environments Pane on the top-right of your screen. This allows you to keep track of all the objects you have created in your R session, along with their values.\n\nYou can also assign the same value to multiple variables using multiple assignment operators &lt;- &lt;- &lt;-. For example, if I want to assign the value male to three names, this is what that would look like.\n\n# Assign multiple variables the value male.\nBrendan &lt;- Mark &lt;- Tyrone &lt;- \"male\"\n\nprint(Brendan)\n\n[1] \"male\"\n\nprint(Mark)\n\n[1] \"male\"\n\nprint(Tyrone)\n\n[1] \"male\"\n\n\nIf you want to remove an object from your environment, you can use the rm() function with the name of your object in parentheses. For example, if I wanted to remove the object quantity1, I would write rm(quantity1). If you want to remove all objects from your environment, you can run the command rm(list=ls()).\n\n\n\n\n\n\nYou can’t name an object just anything!\n\n\n\nThere are some words in R that are reserved for particular tasks. These Reserved Words cannot be used to name an object. So, don’t ever use these words alone to name an object.\n\n\n\nbreak\nNA\n\n\nelse\nNaN\n\n\nFALSE\nnext\n\n\nTRUE\nrepeat\n\n\nfor\nreturn\n\n\nfunction\nwhile\n\n\nInf\nif"
  },
  {
    "objectID": "Chapters/intro.html#help",
    "href": "Chapters/intro.html#help",
    "title": "Introduction to R",
    "section": "Help",
    "text": "Help\nIf you ever need to know how a function works, such as what arguments it take, you can easily see documentation for it by adding a question mark ? before the function name. For example, if you can’t remember what the rm() function does, or what arguments it takes, you can write ?rm and see the documentation for it. For more complex problems or troubleshooting, you can and should become familiar with Googling the issue, and then seeing the best answer on Stack Overflow in particular. Stack Overflow is one the best places to get answers for troublesome R code. You should also note that everyone, and I mean absolutely, everyone Googles code issues at some point or another. This is because coding and data science are dynamic fields, and things are often changing. It’s sensible to keep up with how people are solving particular problems by seeing what people are saying on Stack Overflow or other fora."
  },
  {
    "objectID": "Chapters/intro.html#exercises",
    "href": "Chapters/intro.html#exercises",
    "title": "Introduction to R",
    "section": "Exercises",
    "text": "Exercises\nIt’s a good idea to attempt these right away after reading this section while the content is fresh. You can find the answers in sec-appendixa.\n\nCalculate \\(89+9/(4*5)^2\\) and assign the value to the object solution1. Then print solution1.\nWhat is the difference between R and R Studio?\nHow do you add a comment to a Script file?\nWhat are packages in R, and how do you install them?\nInstall and load the package rstudioapi.\nHow do you modify the appearance of R Studio?\nAssign the value of 365 to an object called year. Then, create another object called month and assign it the value year/12.\nCreate three variables named Cowboys, Giants, and Commanders and assign them all the value \"Inferior Team\" using multiple assignment operators."
  },
  {
    "objectID": "Chapters/syllabus.html#what-are-the-course-details",
    "href": "Chapters/syllabus.html#what-are-the-course-details",
    "title": "Syllabus",
    "section": "What are the course details?",
    "text": "What are the course details?\nCourse Title: Research in PPE: Research Transparency, Reproducibility and Basic Data Analysis in R\nCourse Instructor: Shaon Lahiri, PhD, MPH (he/him)\nCourse Instructor Email: shaonl@sas.upenn.edu\nCourse Number: PPE 4000-301\nCRN: 60098\nSchedule: Thursdays, 12pm - 3pm (August 29, 2023 - December 11, 2023)\nLocation: TBD"
  },
  {
    "objectID": "Chapters/syllabus.html#whats-this-course-about",
    "href": "Chapters/syllabus.html#whats-this-course-about",
    "title": "Syllabus",
    "section": "What’s this course about?",
    "text": "What’s this course about?\nA great question! Here’s the deal: across the sciences, there are often problems with published research and the way in which research is conducted. Some of these problems are pretty major, like fabricating data and images. Others are relatively minor but still problematic, such as not providing the full details of your study. These problems are bad for science. They hold us back from making progress in solving problems and understanding complex phenomena. They give us distorted or incorrect insights, that can then lead to bad policies and programs.\nIn this course we will discuss many issues related to the conduct of research, including bad or unethical approaches to research, how to think about replication and reproducibility, incentives to produce research, and solutions for making the research world a better place.\nBut this is not just about learning about research, but also about actually learning how to code to do research. We will learn basic data preparation, management, and analysis in R. We will learn how to implement transparent coding and data management practices in R. Finally, when it comes to data analysis, we will go over basic statistical analysis from descriptives, bivariate analyses, and to multivariable analyses staying mainly within the General Linear Model family of techniques."
  },
  {
    "objectID": "Chapters/syllabus.html#any-prerequisites-for-this-course",
    "href": "Chapters/syllabus.html#any-prerequisites-for-this-course",
    "title": "Syllabus",
    "section": "Any prerequisites for this course?",
    "text": "Any prerequisites for this course?\nIf you’ve never used R in your life, nor have even heard of RT2, that’s fine! I assume no prior knowledge of these concepts. It would be helpful to have completed at least a high school level course in statistics before taking this one. But if you haven’t, that’s no problem as we will go over the necessary statistical concepts.\nThe only thing you really need for this course is a modicum of motivation to learn R and RT2!"
  },
  {
    "objectID": "Chapters/syllabus.html#why-should-you-take-this-course",
    "href": "Chapters/syllabus.html#why-should-you-take-this-course",
    "title": "Syllabus",
    "section": "Why should you take this course?",
    "text": "Why should you take this course?\nMillions of people use the statistical programming language R to solve complex issues every day. If you have any interest in using data to understand the world, and visualize data in a way that communicates information succinctly and impactfully, this course is for you!\nNeed a programming skill to add to that resume to be more competitive on the job market? This course is for you!\nBesides being a highly marketable skill, learning R is incredibly useful to undertake quantitative research. If you have any interest in doing research, having R in your aRsenal can help you quickly and efficiently write reproducible code to import, clean, analyze, and interpret data in various forms. Finally, doing good research means doing ethical research. I will teach you how to avoid bad practices and malignant influences in order to conduct research in a reproducible, ethical, and rigorous manner.\n\n\n\n\n\n\nIf you want to impress potential romantic partners, R is also useful to have in your back pocket. I once went on a date where the subject of R came up (I brought it up), and my date asked how much it cost. When I said it was free, she was shocked and intrigued. I then explained all the things R can do. The relationship never went anywhere, but still…she was impressed."
  },
  {
    "objectID": "Chapters/syllabus.html#what-are-the-requirements-of-this-course",
    "href": "Chapters/syllabus.html#what-are-the-requirements-of-this-course",
    "title": "Syllabus",
    "section": "What are the requirements of this course?",
    "text": "What are the requirements of this course?\nThe pie chart below breaks down the grade distribution for this course. Each component is described in detail below.\n\n\nClass Participation (20%)\nTo receive an ‘A’ for this component, you must not miss more than two (2) classes during the semester. If you miss class due to religious holiday, illness, or an emergency, those do not count toward your two missable classes. You must also complete any homework by the deadline, and be ready to engage in discussion, ask questions, engage in activities, and generally be ready to contribute. I will assign all homework on an ad-hoc basis on Canvas with at least one week’s notice. Homework will typically involve reading journal articles, watching videos, or completing tasks in R. If you feel sick, please do not come to class. Instead, please email me and we can set up a time to meet if you have any questions about the course material. No documentation is required by me for illness or emergencies. Beyond the two excused absences, each missed class will decrease your Class Participation grade by 10%.\n\n\nClass Presentations (20%)\nOver the course of the semester, each student will deliver two presentations to the class lasting approximately 15 minutes, followed by five minutes of Q&A. The first of these presentations will cover a recent or emerging topic within the world of research transparency and reproducibility. The second presentation will be an introduction and demonstration of a recent R package. I’m happy to help you come up with topics or troubleshoot for any of the presentations. All presentation topics must be approved by me before you proceed with the preparation.\n\n\nMonthly Exams (10% each; 40% total)\nFour exams will be administered in class toward the end of each month. Each exam will include a section on research transparency & reproducibility (Section 1), and a section involving completing tasks in R (Section 2). Section 1 is ‘closed-book’ and will be administered on paper (no notes or digital media will be allowed during the completion of this section). Students will then complete Section 2 on their laptops, and will be permitted to use their notes and the Internet (including Chat GPT or similar generative chatbots). Each exam will cover only the content discussed in the previous month, and after the last exam. For example, the content for Exam 2 will cover the content discussed in February, and will not cover content discussed in Exam 1. Each exam is worth 10%, for a total of 40% of one’s grade for the course. There are no make-up exams except in case of emergency. All exams must be completed alone.\n\n\nWeekly Discussion Board Posts (20%)\nPrior to class each week, students are required to post a question, comment, or reflection on the Discussion Board about a topic related to research transparency & reproducibility, OR about R (either is fine with me). The post should be made on the appropriate Discussion Board on Canvas by 11am on Mondays we have class. Posts made after this time will not count toward the student’s grade. This is essentially a chance for you to explore, critique, or reflect on the class content or any related content from outside the class.\nThe content of Discussion Board posts should be reasonably substantive. For example, a post saying “this was really hard to do, and I think there should be a better way” is not very substantive, whereas a post saying “this was challenging, but I found some other code online that accomplishes this task in a better way. Here it is:. . .” is a much more substantive post. Use your judgment and always feel free to check with me if you have any questions or doubts. Multiple posts are permitted. Students may miss posting two (2) weeks of posts (i.e. two posts) without penalty. Outside of this allowance, missed discussion posts will subtract 10% of a student’s grade per week of missed posts."
  },
  {
    "objectID": "Chapters/syllabus.html#what-is-the-grading-scale-for-this-course",
    "href": "Chapters/syllabus.html#what-is-the-grading-scale-for-this-course",
    "title": "Syllabus",
    "section": "What is the grading scale for this course?",
    "text": "What is the grading scale for this course?\n\n\n\n\n\n\n\n\nPercentage\nLetter Grade\nInterpretation\n\n\n\n\n93-100%\nA\nOutstanding work. Meets all expectations of the assignment with excellent command of material.\n\n\n90-92%\nA-\nHigh-quality work. Meets nearly all expectations of the assignment, but is lacking in 1-2 areas.\n\n\n87-89%\nB+\nGood work. Meets most expectations of the assignment, but is lacking in 3 or more areas.\n\n\n83-86%\nB\nAdequate work. Meets the majority of expectations of the assignment, but is weak in 1-2 areas.\n\n\n80-82%\nB-\nBelow average work. Meets some expectations of the assignment, but is weak in 3 or more areas.\n\n\n77-79%\nC+\nDid not meet the expectations of the assignment. Please meet with me.\n\n\n73-76%\nC\nDid not meet the expectations of the assignment. Please meet with me.\n\n\n70-72%\nC-\nDid not meet the expectations of the assignment. Please meet with me.\n\n\n67-69%\nD\nDid not meet the expectations of the assignment. Please meet with me.\n\n\n&lt;67%\nF\nDid not meet the expectations of the assignment. Please meet with me.\n\n\n\nGrades will be calculated using standard rounding rules, in which a number in the tenths decimal place will be rounded down to the nearest integer if it is between 0-4, and rounded up to the nearest integer if it is between 5-9. For example, a grade of 89.4% will be rounded to 89%, and a grade of 89.5% will be rounded to 90%. For all quizzes, I provide an answer key. For all assignments, I provide a rubric."
  },
  {
    "objectID": "Chapters/syllabus.html#what-materials-do-i-need-to-buy-for-this-course",
    "href": "Chapters/syllabus.html#what-materials-do-i-need-to-buy-for-this-course",
    "title": "Syllabus",
    "section": "What materials do I need to buy for this course?",
    "text": "What materials do I need to buy for this course?\nNothing beyond what you use normally for other courses at Penn (i.e. a personal computer or laptop, internet access, word processing software, and pdf reading software). All readings will be provided on Canvas. Please make sure R and RStudio are downloaded and installed on your laptop before the first class following the directions in sec-installation."
  },
  {
    "objectID": "Chapters/syllabus.html#do-you-have-an-email-policy",
    "href": "Chapters/syllabus.html#do-you-have-an-email-policy",
    "title": "Syllabus",
    "section": "Do you have an “Email Policy”?",
    "text": "Do you have an “Email Policy”?\nI do! Please allow me up to two business days (i.e. M-F except holidays) to get back to your emails. In most cases, I will get back to you as soon as possible. If I haven’t replied to you in two business days, please follow-up. I stick to traditional work hours (i.e 9am - 5pm) when responding to emails. I request that you to begin all emails to me with “Hi Shaon,” because I prefer personalized emails addressing me by name. If you have any questions about course material, or if there is anything I can do to help you succeed in the course, please email me. I am here to support your growth and nurture your interests. Additionally, if anything at all in the course does not sit right with you at any point, please get in touch."
  },
  {
    "objectID": "Chapters/syllabus.html#what-are-you-doing-to-make-the-course-more-accessible-and-inclusive",
    "href": "Chapters/syllabus.html#what-are-you-doing-to-make-the-course-more-accessible-and-inclusive",
    "title": "Syllabus",
    "section": "What are you doing to make the course more accessible and inclusive?",
    "text": "What are you doing to make the course more accessible and inclusive?\nI strive to create a welcoming environment for all, and have a zero tolerance policy regarding bullying, harassment, or threatening behavior. Part of creating an inclusive environment can involve occasionally hearing a viewpoint contrary to your own. I expect all of us to engage in respectful civil discourse, especially when disagreeing with one another. This is usually not an issue in my courses, but as we sometimes deal with sensitive subject matter, please remember it is encouraged to share your thoughts and opinions, but please accord the same respect to others as you would have them accord to you. If you feel at any point that I or someone in the class has violated that trust, please contact me as soon as possible.\nPart of inclusion means supporting our classmates who are breastfeeding or have gaps in child-care. Babies are always welcome in the class, and I ask students who bring babies to sit near a door in case you need to step out briefly to provide special attention to your little one. For young children, while not a long-term child care solution, occasionally bringing a child to class is perfectly fine. I ask that all students work together with me to support our classmates who bring babies or young children to class. While I hold all students to the same high-quality standards for the work submitted in this course, if you are a student-parent, I will be happy to work with you to support you however I can.\nService animals are always welcome in the classroom.\nIf you require any other accommodations, please let me know and I will do my best to make them happen."
  },
  {
    "objectID": "Chapters/syllabus.html#is-cheating-in-the-course-ok",
    "href": "Chapters/syllabus.html#is-cheating-in-the-course-ok",
    "title": "Syllabus",
    "section": "Is cheating in the course ok?",
    "text": "Is cheating in the course ok?\nNo! Please familiarize yourself with Penn’s Code of Academic Integrity if you have not already done so here. I have a zero tolerance policy for cheating and plagiarism. If you are ever unsure what the line is between plagiarism and appropriate citation/reference, please contact me BEFORE submitting your work! If you feel that cannot succeed in this course without cheating, please contact me and we can speak confidentially. I will never punish you for the temptation, and we can develop a plan together to figure out a plan for avoiding the temptation. Please always work alone on all quizzes and written assignments in this course, and never copy a sentence or phrase from another work without appropriate citations and references."
  },
  {
    "objectID": "Chapters/outline.html",
    "href": "Chapters/outline.html",
    "title": "Outline of Classes",
    "section": "",
    "text": "Date\nTopics and Events\n\n\n\n\nAugust 31\nIntroduction to Base R and RStudio (operations, objects, data types, IDE).\nIntroduction to RT2 (history of replication and Mertonian Norms).\n\n\nSeptember 7\n\n\n\nSeptember 14\n\n\n\nSeptember 21\n\n\n\nSeptember 28\n\n\n\nOctober 5\n\n\n\nOctober 12\nFall Break (No Class)\n\n\nOctober 19\n\n\n\nOctober 26\n\n\n\nNovember 2\n\n\n\nNovember 9\nStudent Presentations - Round 1.\n\n\nNovember 16\n\n\n\nNovember 21 (Tuesday)\nThur-Fri class schedule on Tue-Wed.\n\n\nNovember 30\n\n\n\nDecember 7\nLast Day of Class.\nStudent Presentations - Round 2."
  },
  {
    "objectID": "Chapters/installation.html",
    "href": "Chapters/installation.html",
    "title": "Installing R and RStudio",
    "section": "",
    "text": "Let’s make sure we understand what R and RStudio are before proceeding. Firstly, R is a programming language that is commonly used for statistical analysis and data visualization. RStudio is an Integrated Development Environment (IDE) which sits on top of R. It provides a very nice graphical user interface that allows us to use R along with many other useful features.\nR is the engine, and RStudio is the car frame. You can use R without RStudio (known as Base R), but you cannot use RStudio without R.\nBefore we meet for the first class, ensure you have downloaded and installed the following:\n\nR\n\nDownload from here: https://cran.r-project.org\n\nRStudio Desktop\n\nDownload from here: https://posit.co/download/rstudio-desktop/\n\n\nIf you’re using a Mac and you’re having trouble, try switching browsers from Safari to Chrome. Sometimes Safari has some extra security provisions which prevent you from installing software successfully, and this can be remedied by using a different browser like Chrome.\nPlease email me if you have any trouble installing R and R Studio. If you’ve successfully installed both, head over to sec-started ."
  },
  {
    "objectID": "Chapters/getstarted.html",
    "href": "Chapters/getstarted.html",
    "title": "Getting Accustomed to R Studio",
    "section": "",
    "text": "Once you’ve successfully installed R and RStudio, go ahead and open RStudio. It should look something like this:\n\nLooks pretty plain right? No razzle dazzle?\nBefore we do anything, let’s first customize the way RStudio looks by going to Tools –&gt; Global Options:\n\nNext, go to Appearance and select an Editor Theme (personally I prefer dark themes like Cobalt). You can also modify the Editor font size in case you want to zoom in on the code. Click Apply to try out different themes, and Ok once you’ve settled on a theme.\n\nYou can change the Editor font size to something larger if you have trouble seeing the code at size 10. You can also do this anytime in RStudio by going to View and then clicking Zoom In or Zoom Out:\n\nNow that you’ve settled on a theme and Editor font size, let’s see what each of the panels in RStudio mean:\n\n\nThis area is called the Console, and this is where code is executed.\nThis area is the Environments Pane. This is where you can see the objects stored in your R session, such as data, functions, variables, and other things.\nThis area is the Output Pane. This is where you can see files in your Working Directory (under the tab Files), see a preview Plots you might have created, see your list of packages, and more.\n\nThere is also a fourth pane, which is usually created by clicking File –&gt; New File –&gt; R Script. This will create a setup that looks like this:\n\n\nThis is the Source Pane. This is where you can view and edit various code files. By default, we will use this pane to write, edit, and execute R Script files. These files tell R what code to run and in what order to run it.\n\nSo there you have it. Those are the four main panels that we will use in R Studio to write and execute R Code. Now, let’s learn some R starting with sec-intro !"
  },
  {
    "objectID": "Chapters/replication1.html#what-is-research-transparency-reproducibility-and-replication",
    "href": "Chapters/replication1.html#what-is-research-transparency-reproducibility-and-replication",
    "title": "A Very Brief History of Replication",
    "section": "What is Research Transparency, Reproducibility, and Replication?",
    "text": "What is Research Transparency, Reproducibility, and Replication?\nBy now, you should be swimming along in understanding the preliminaries of R. Hopefully, you are practicing and ensuring you have nailed down all the basics.\nWhile it can be tempting to continue along this road, and immediately jump to the next topic in R, this is actually a good time to slow things down, and understand what we are doing in this course. This is not a course in how to code in R, at least not entirely. This course is also about the very consequential problems and solutions involved in Research Transparency & Reproducibility (RT2). Let’s begin with some definitions of these terms from the United States National Academies of Sciences, Engineering, and Medicine.1\n\n\n\n\n\n\nDefinitions\n\n\n\nReproducibility - Obtaining consistent computational results using the sample input data, computational steps, methods, code, and conditions of analysis.\nReplicability - Obtaining consistent results across studies aimed at answering the same scientific question, each of which has obtained its own data.\n\n\nThink of reproducibility as checking the results of the original study. If a researcher gave you their code and data, could you reproduce or recreate their analyses and have them match? That’s the hope anyway, but you would be surprised how often that does not happen.\nThink of replication as conducting multiple studies, using the same methods and procedures as the original study. If you arrive at similar results, that’s good and shows that the underlying theory seems to hold. If not, then it could be a) a problem with the underlying theory, b) a problem with the methods or procedures, or c) both.\nFinally, as stated by the National Academies, “Reproducibility is strongly associated with transparency; a study’s data and code have to be available in order for others to reproduce and confirm results.”1 Thus, transparency typically refers to availability of data, code, and materials used to produce the results of a study. However, the term conveys more than that. Another definition of research transparency is as the obligation to “make data, analysis, methods, and interpretive choices underlying their claims visible in a way that allows others to evaluation them - as a fundamental ethical obligation.”2 This extends beyond mere availability of data and code to a fundamental way of doing ethical research.\nThe definitions are illustrated in the Figure fig-rt2.\n\n\n\nFigure 1: Transparency is ideal for replication efforts, but often materials are not provided by authors.\n\n\nNow that we’ve got definitions for the terms, let’s have a look at some key (and in some cases oft-forgotten) players in the history of replication and reproducibility."
  },
  {
    "objectID": "Chapters/replication1.html#replication-origins",
    "href": "Chapters/replication1.html#replication-origins",
    "title": "A Very Brief History of Replication",
    "section": "Replication Origins",
    "text": "Replication Origins\nThe history of replication is closely tied to the history of the Scientific Method. Many individuals over time likely contributed to it, but let’s have a look at a few key figures throughout history. The focus here is not on the Scientific Method per se, but rather on the use of replication or repeated experimentation.\n\nIbn al Haytham\n\n\n\nPortrait of Ibn Al Haytham by Zargar Zahoor.\n\n\nOne of the earliest proponents of an approach we later would know as the Scientific Method was Ibn al-Haytham, a mathematician, astronomer, physicist, and all-around scholar who lived from 965-1040 CE. Born in Basra, modern-day Iraq, Al Haytham is known as the Father of Optics for his pioneering work on the subject in his famous work كتاب المناظر (Kitab al-Manazir or Book of Optics), which provided empirical evidence that vision occurred when light entered the eyes (intromission), rather than light being emitted by the eyes (extramission), which was argued by Euclid in Optica.\nBesides his many scientific contributions, particularly to the field of Optics, al Haytham made use of repeated systematic observation of natural phenomena to argue his points. Even in his magnum opus Book of Optics, he often ends a description of an experiment with the phrase “this is always found to be so, with no variation or change”, emphasizing that repetition was a central argument of his experimental findings.3\nWhile this may not seem that impressive to us today, consider that Al Haytham was working in his way before we had the term Scientific Method, and about 500 years before Galileo started looking through telescopes! The important takeaway from Al Haytham is that accidental conditions might distort one’s observation of a phenomenon, and we’re better off making multiple observations before making any grand claims.\n\n\nAbu Rayhan Al Biruni\n\n\n\nAl Biruni on the cover of the UNESCO Courier magazine in 1974.\n\n\nAbu Rayhan Muhammad ibn Ahmad al-Biruni was a Muslim polymath who lived from 973 - 1050 CE. He was known to have authored 146 books, with the majority written on mathematics, astronomy, and related subjects. He was also one of the earliest scholars to offer a formal refutation of astrology in contradistinction to astronomy, which he considered a legitimate science based on empiricism. He made contributions to many fields such as physics, astronomy, geography, and Indology (the study of India).\nImportantly, al Biruni contributed to the development of the scientific method, and was one of the first documented scientists who thought about how to separate measurement of a phenomenon from errors in the measurement device. This was remarkably foresighted, as the issue of measurement error is unavoidable in the natural and social sciences. Al Biruni noted that measurement errors can come from different sources such as measurement tools and human judgment, and can compound.4 To address the issue, he recommended taking multiple measurements of a phenomenon and then quantitatively combining them to arrive at a common-sense estimate. This is an approach we still take today with a number of applications, but especially in the use of multiple survey questions to study latent or unobserved phenomena like attitudes, emotions, and perceptions.\n\n\nRoger Bacon\n\n\n\nBacon in his observatory in Merton College, Oxford by Ernest Board.\n\n\nPicking up where Ibn al Haytham left off, Roger Bacon was a philosopher and Franciscan friar from England who lived from 1219 - 1292 CE. He took al Haytham’s scientific method and applied it to works by Aristotle. He was the first person in Europe to describe the formula for gunpowder (it was invented and known about in China already). Crucial for our purposes, he was an early proponent of a method of observation, hypothesis, experimentation, and the need for independent verification as noted in his Opus Tertium, published in 1267. Regarding the latter, he kept very meticulous notes about his experiments, permitting reproducibility by others. This is exactly what we try to do today!\n\n\nAndries van Wesel\n\n\n\nAndries van Wesel revolutionized our understanding of human anatomy.\n\n\nAndries van Wesel (also known as Andreas Vesalius) as a Flemish anatomist who lived from 1514-1564 CE. He is most famous for creating detailed drawings of human anatomy that provided new insights into the subject, correcting a number of errors made by the Greek physician Galen (129-216 CE), and followed by physicians for centuries.\nIn the summer of 1542, he published his magnum opus De humani coporis fabrica libri septem (On the Fabric of the Human Body in Seven Books), which was based on a collection of his lectures delivered at the University of Padua, where he was a professor. Based on his own experiences dissecting corpses (not a common practice among physicians at the time), he was able to show that the Galen never dissected a human corpse because he made many errors in human anatomy.5 Galen instead had relied on dissections of animal corpses, and assumed humans had the same anatomy. Van Wesel was able to show that Galen made 300 errors in human anatomy. Finally, van Wesel was able to create detailed visualizations leveraging newer techniques of printing with woodcut engravings. This was huge, because it allowed other physicians to dissect corpses and verify what van Wesel was claiming. This is one of the earliest examples that I’ve seen of evidence-based medicine triumphing over eminence-based medicine. This meant that you didn’t have to take his word for it, you could actually dissect a corpse and check if what you saw lined up with his illustrations. This is very much in line with how we think about replication today!\n\n\n\n\n\n\nBans on (Dead) Bodies\n\n\n\nDid you know that dissecting human corpses was strictly prohibited by the Catholic Church in the 16th century? Andries van Wesel had to secretly ‘steal’ the bodies of executed criminals to perform his dissections. This allowed him to correct many of Galen’s errors of anatomy. Have a look at his illustrations here.\n\n\n\n\nFrancis Bacon\n\n\n\nPortrait of Francis Bacon by Paul van Somer I.\n\n\nSir Francis Bacon was an English philosopher and statesman who lived from 1561 - 1626 CE. He is sometimes called the father of empiricism, but of course we know that he stood on the shoulders of giants who came before him. He put forward a method of inductive reasoning in his magnum opus Novum Organum (New Method) which stressed the importance of making systematic observations of phenomena, and then to generalize the observations to a few axioms. He also noted that it was important to have a skeptical and methodological approach, so that scientists would not mislead themselves, and to not generalize beyond what the facts demonstrate. The latter sentiment is particularly relevant today as authors of scientific papers frequently discuss broad-reaching implications of their findings, which really cannot generalize beyond their samples. In fact, some have suggested adding a “Constraints on Generality” section to the discussion section of every paper, in which authors clearly state the specific population to which their findings apply.6 I feel like Sir Francis would approve.\nIn addition to his many contributions to the formation of the Scientific Method, Bacon also stressed the need for repeated experimentation to create an increasingly complex knowledge base that is always supported by observable facts. This is highly in line with how we think about replication today, as we aim to support or reject theories of how things work. Finally, Bacon also listed several “idols of the mind”, which he noted as obscuring the path of correct scientific reasoning. These include the human tendency to perceive more order in a system than actually exists, confusion arising from the scientific use of certain words compared with their common usage, and the tendency to follow dogma and not ask questions about the world. These are what we would today refer to as cognitive biases, and they have an important role in inhibiting research transparency and replication, which we will discuss later.\nIn short, Sir Francis reaffirmed the need for repeated experimentation, careful observation, and meticulous record-keeping in the conduct of science.\nWhile many others contributed to the refinement of the Scientific Method over the next 500 years or so, for our purposes to will skip to the 20th century.\n\n\n\n\n1. National Academies of Sciences E. Reproducibility and Replicability in Science.; 2019. doi:10.17226/25303\n\n\n2. Moravcsik A. Transparency in Qualitative Research. SAGE Publications Limited; 2020.\n\n\n3. Steinle F. Stability and Replication of Experimental Results: A Historical Perspective. In: Reproducibility : Principles, Problems, Practices, and Prospects. John Wiley & Sons, Incorporated; 2016. http://ebookcentral.proquest.com/lib/upenn-ebooks/detail.action?docID=4547409\n\n\n4. Glick TF, Livesey SJ, Wallis F. Routledge Revivals: Medieval Science, Technology and Medicine (2006): An Encyclopedia. Taylor & Francis; 2017.\n\n\n5. Zampieri F, ElMaghawry M, Zanatta A, Thiene G. Andreas Vesalius: Celebrating 500 years of dissecting nature. Global Cardiology Science & Practice. 2015;2015(5):66. doi:10.5339/gcsp.2015.66\n\n\n6. Simons DJ, Shoda Y, Lindsay DS. Constraints on Generality (COG): A Proposed Addition to All Empirical Papers. Perspectives on Psychological Science. 2017;12(6):1123-1128. doi:10.1177/1745691617708630"
  },
  {
    "objectID": "Chapters/replication2.html#background",
    "href": "Chapters/replication2.html#background",
    "title": "Mertonian Norms and Counter-Norms",
    "section": "Background",
    "text": "Background\nRobert K. Merton lived from 1910 - 2003 and made a number of major contributions to sociology, criminology, and the sociology of science. Unlike some of the early figures in the history of replication we saw in sec-rep1, Merton advocated for theorizing to begin with middle-range theory, or starting with aspects of a phenomenon that are clearly understood, constructed with observed data. He intended this to be a middle ground between a grand theory and a description of a phenomenon. This is remarkably contemporary, as a great deal of middle range theory is about underlying mechanisms that give rise to phenomena.1\nReflecting on the history of science, Merton noted that scientists could no longer isolate themselves from the concerns of society.2 He could have been a poet, because his descriptions of this observation are loaded with powerful imagery that summarize his position nicely. Let’s look at a few of these quotes with my crude interpretations interspersed.\n\nMerton, please forgive me my crude interpretations of your dense, though mellifluous, prose.\n\n\n\n\n\n\nMerton’s Quote\nMy Crude Interpretation\n\n\n\n\nA tower of ivory becomes untenable when its walls are under prolonged assault.\nHey scientists, you can only pretend society doesn’t exist for so long, until you can’t.\n\n\nA frontal assault on the autonomy of science was required to convert this sanguine isolationism into realistic participation in the revolutionary conflict of cultures.\nHey scientists, when bombs start falling, you better get in the game!\n\n\nScience is a deceptively inclusive word which refers to a variety of distinct though interrelated items.\nLike, what even is Science?\n\n\nThe ethos of science is that affectively toned complex of values and norms which is held to be binding on the man of science. The norms are expressed in the form of prescriptions, proscriptions, preferences, and permissions. They are legitimized in terms of institutional values.\nScience is done a particular way, and you can see that in a bunch of places.\n\n\n\nAs you will note, Merton was interested in the way science was done, and with its explicit and implicit beliefs, norms, and aspirations. In short, he was interested in spelling out an ethos of science, or the character of ideal scientific inquiry. While Merton made many valuable contributions to social science, we are primarily concerned with his notion of Mertonian Norms, which are given by the acronym CUDOS."
  },
  {
    "objectID": "Chapters/replication2.html#cudos",
    "href": "Chapters/replication2.html#cudos",
    "title": "Mertonian Norms and Counter-Norms",
    "section": "CUDOS",
    "text": "CUDOS\n\nMertonian Norms of Science\n\n\n\n\n\n\nComponent\nDescription\n\n\n\n\nCommunalism (called Communism by Merton)\nPublic ownership of scientific goods.\n\n\nUniversalism\nValidity of findings as independent of the status of human subjects.\n\n\nDisinterestedness\nScientific institutions act for the benefit of a common scientific enterprise, and not for personal gain.\n\n\nOrganized Skepticism\nScientific claims should undergo critical scrutiny before being accepted.\n\n\n\nLet’s make sure we have a good sense of each of his norms (i.e. CUDOS) before proceeding.\n\nCommunalism\n\nMerton believed that science should produce goods under common ownership of the community (he called it Communism but scientists instead often call this Communalism because the former has…certain connotations unrelated to science). The idea here is that a major discovery in science in not the exclusive property of the discoverer. You might get something named after you (known as Eponymy, such as Boyle’s Law or Schrödinger’s cat), and maybe some recognition, but you don’t get to own the discovery as this contradicts the scientific ethic. Secrecy in the antithesis of this norm, and “full and open communication” is related to putting the norm into practice. Merton notes that this norm is incompatible with the definition of technology as private property.\n\n\n\n\n\n\nReal-World Applications: Communalism\n\n\n\nA study investigated 36 top ranked apps for depression and smoking cessation for Android and iOS in the US and Australia in 2018.3 The study found that of the 36 apps, 29 shared user data to Google and Facebook, and only 12 of these accurately disclosed this information in a privacy policy. How does sharing findings on users with commercial entities align with the Mertonian norm of Communalism?\n\n\n\n\nUniversalism\n\nThe acceptance or rejection of particular research claims should be impersonal. It should not depend on the author’s race, gender, nationality, alma mater, or any personal characteristic. Merton believed that the impersonal character of science was deep and rooted in Universalism. He was also realistic and understood that the larger culture within which science is housed can be antagonistic to the norm of Universalism. In particular, nationalistic bias and related forces can create the situation where “the man of science may be converted into a man of way - and act accordingly.”2 Knowledge cannot be advanced and furthered if scientific careers are restricted on grounds other than competence, according to Merton. It shouldn’t matter who you are as long your work is competent and adheres to the standard of quality in a field. Unfortunately, implicit and explicit can creep in during process of hiring researchers, the peer review process, and the consumption of research. This is why many journals operate using a double-blind process, whereby neither the authors nor the reviewers know the identity of the other.\n\n\n\n\n\n\nLess than Universal - A Personal Anecdote\n\n\n\nWhen I was an undergraduate research assistant at the University of Michigan in the US, I once brought a paper to the attention of my supervisor, who was a young PhD student educated in the US. When they asked what journal the paper came from, I replied “the Indian Journal of Psychology.” Without looking at the paper, they said “Hmm, see if you can find a paper from a better journal.” I dutifully went looking for another paper, but not before reflecting “Why didn’t they look at the study before making a decision on its quality?” As an Indian studying in the US, this experience illustrated the mental shortcuts frequently made in academia, illustrating a stark deviation from the norm of Universalism.\n\n\nWhen I first read about the Mertonian norm of Universalism, my first thought was of a specific scene in Antoine de Saint-Exupéry’s classic Le Petit Prince [The Little Prince].4 The scene is presented in Figure fig-turk .\n\n\n\nFigure 1: Le Petit Prince (1943). Reynal & Hitchcock. No one believed the astronomer when he wore non-European clothing. But when he donned a three-piece suit, they listened then. A violation of the norm of Universalism.\n\n\nIn this scene, a Turkish astronomer who discovered an asteroid and tried to present the results of his discovery to his peers at an international astronomy conference. However, his peers did not believe his findings because he was dressed in traditional Turkish garb of a fez and a billowing set of shalwar pants. Years later, a Turkish dictator forces his subjects to wear European-style clothing under pain of death. This time, the Turkish astronomer presented his findings at the same conference but with a more European outfit. His peers then immediately believe his findings.\n\n\nDisinterestedness\n\nWhen it comes to science, researchers shouldn’t be motivated by commercial pursuits, political interests, or the desire for individual status and prestige. The norm of Disinterestedness is about individual motivations to do science, and that these should be for the sake of science itself. I see this more as Purity of Intention. Merton noted that the norm of Disinterestedness is supported by the fact that scientists are accountable to their peers. If you’re going to do partisan politics under the guise of science, your professional peers are going to point that out, and illustrate how its not science. In essence, your colleagues will largely see through you and understand what you are doing.\nThis is the most aspirational norm, if you ask me. Consider that in academia hiring and promotion is usually heavily influenced by how many papers you publish or how much grant money you can secure. In fact, the whole purpose of this course is to show how to do ethical programming and statistical analysis, with a number of illustrative cases where this is not the case. Additionally, consider the incentives of funders of research as well, especially private organizations that have their own motives and priority areas. While this norm is not especially realistic, it’s a good thing to strive for, and an important one when it comes to the interpretation of data.\n\n\nOrganized Skepticism\n\nThe norm of Organized Skepticism is an institutional and methodological mandate, according to Merton. Basically, for science to work, this has to be part of the process. Science is concerned with a detached scrutiny of beliefs in terms of empirical and other logical criteria. This is related to the process of peer review, in which experts provide critical scrutiny for articles submitted to scientific journals. Merton points to the fact that this norm has often involved questioning the ways things are done, and established beliefs. This can sometimes lead to negative consequences for scientists that question the power assumptions of certain institutions.\n\n\n\nGalileo’s Organized Skepticism was not appreciated by the Catholic Church. Wikimedia Commons."
  },
  {
    "objectID": "Chapters/replication2.html#counter-norms",
    "href": "Chapters/replication2.html#counter-norms",
    "title": "Mertonian Norms and Counter-Norms",
    "section": "Counter-Norms",
    "text": "Counter-Norms\nMerton’s norms have been quite influential in the sociology of science field, and later work by Mitroff produced a useful list of counter-norms to Merton’s norms.5 These counter-norms are the opposite of Merton’s norms, and nicely capture how science is often conducted in reality. We can refer to them when thinking about the conduct of science as falling on a spectrum with the poles being the norms and counter-norms.\n\nParticularism (in contrast with Universalism)\nThe idea here is that the acceptance or rejection of a claim depends in large part on the characteristics of who makes the claim. Recall the Turkish astronomer in the Petit Prince example, and how people accepted his claims only after he donned the clothing they deemed to be acceptable. If someone prejudges the claims of scientist from Harvard University as being automatically more credible and valid than those of someone from Jadavpur University in Kolkatta (where my dad went to college incidentally), India without actually seeing their claims, this would be a classic case of Particularism. In fact, they might be implicitly biased against the claimant from Jadavpur University even if they read their study.\nSadly, given what we know about human judgment and how often it relies on quick, mental shortcuts, this counter-norm appears quite realistic. The key then is to design systems of review (such as double-blind peer review) that acknowledge the possibility of Particularism.\n\n\n\n\n\n\nRacial Discrimination as a Form of Particularism\n\n\n\n\n\n\nProfessor David Blackwell - A Trailblazer in Statistics. The Bancroft Library, UC Berkeley Library.\n\n\nRacial discrimination is a commonly-seen form of Particularism whereby scientists are judged on the basis of their race, rather than on the credulity of their claims. David Blackwell was a very smart man, and made many important contributions to statistics and mathematics including to game theory, probability theory, and Bayesian statistics. He was true luminary in the field, and blazed a trail for others as the first African American elected to the National Academy of Sciences.\nBlackwell was a postdoc at the Institute of Advanced Study (IAS) in Princeton in 1941. Usually, an IAS postdoc would receive a Visiting Fellow appointment at Princeton University, but Blackwell was not afforded this because he was Black. The President of Princeton University was unhappy that IAS had admitted a Black man in the first place.\nIn 1942, Blackwell had interviewed for a position at UC Berkeley. His PhD advisor Joseph Doob told Jerzy Neyman, the founder of the UC Berkeley Department of Statistics, that among his good students “…Blackwell is the best. But of course he is black. And in spite of the fact that we are ina war that’s advancing the cause of democracy, it may not have spread throughout our own land.”6 This was a pretty spot-on observation by Doob, as Blackwell was not hired at UC Berkeley. The reason? The wife of the department head refused to have a Black man in her house because new faculty members customarily attended dinner at the department head’s house.\nBlackwell went on to teach at Howard University for 10 years, and also worked at the RAND corporation. In 1954, Blackwell was hired at UC Berkeley at last, and stayed there for the rest of his long and storied career.\n\n\n\n\nSolitariness (in contrast with Communalism)\nSecrecy is the name of the game with this counter-norm. Here, property rights are extended to scientific discoveries. This is actually what one commonly sees when business interests intersect with academic ones. Think about Big Tech companies. They have a strong financial incentive to deliver value to their shareholders through monetizing and protecting their intellectual property. That these are also scientific discoveries illustrates a deviation from the norm of Communalism.\nThe lines are not always so stark however, and my personal opinion is that there is room for scientific communalism as well as commercial privatization in a number of cases. For example, the founders of Google, Larry Page and Sergey Brin, authored a journal article in 1998 in which they break down their famous PageRank algorithm.7 This provided a valuable contribution to the computer science literature, while also forming the basis of Google Search, which still accounts for just over half of Google’s revenues.8\n\n\nInterestedness (in contrast with Disinterestedness)\nIn contrast with Disinterestedness, this counter-norm illustrates the temptation of self-interest and prestige as a chief motivating factor behind scientific discovery. Scientists then work not to for the sake of science, but to secure more funding, more renown, and better positions at more prestigious institutions. This is one of the more realistic counter-norms, in my opinion, as hiring and promotion tend to emphasize a scientist’s output, rather than a dedication to science for the sake of science. The culture of Publish or Perish means that you NEED to be cranking out papers to get a tenure-track position, and certainly to achieve tenure. Consider the implications of this incentive - the system will reward you for papers (be they fraudulent or improperly produced) rather than for producing transparent and robust unpublished work.\nThis incentive creeps into the academic process in a number of ways. Consider how the prestige of a scientific journal often hinges on how well-cited and influential its articles are, and how this encourages more sensational, novel papers rather than carefully-crafted replications or less sexy, though important, findings. Journals are more likely to publish findings that are statistically significant than findings that are not statistically significant, and this phenomenon is known as Publication Bias.9 This leads to all sorts of distortions of the evidence base which we shall discuss later in the course.\n\n\n\n\n\n\nSelf-Citations: Creating your own Fame\n\n\n\nIn 2018, a prominent professor of human development, Robert Sternberg, resigned as editor of the journal Perspectives on Psychological Science. His colleagues were not happy that he kept citing himself and publishing in the journal in which he was the Editor. In one of his articles, 10/17 citations (~59%) are his own.10 Other articles had similarly troubling levels of self-citations.11 He was also criticized for publishing several commentaries in the journal without peer review, and with the benefit of garnering multiple citations.\nCiting your own work is not a bad thing on its own, but when the majority of your citations are your own, that’s usually a problem! Consider how it may indicate an echo-chamber approach to science, where you are not actually doing the work of reviewing the literature. The case of Robert Sternberg illustrates how once a habit becomes entrenched, it can be very difficult to see the shortcomings and to stop doing it.\n\n\n\n\nOrganized Dogmatism (in contrast with Organized Skepticism)\nUnder this counter-norm, scientists take all the credit for their discoveries, while placing the blame for any inadequacies on previous work by others. Far from doubting their own findings, under Organized Dogmatism scientists promote their own findings with a deep conviction, regardless of what others are saying or doing.\n\n\n\n\n\n\nDogmatism: The Ultimate Close-Minded Approach\n\n\n\nA useful definition of Dogmatism is provided by the late psychologist Milton Rokeach:\nA relatively closed cognitive organization of beliefs and disbeliefs about reality, organized around a central set of beliefs about absolute authority which, in turn, provides a framework for patterns of intolerance and qualified tolerance toward others.12\nIn sum, dogmatism sounds like this “I’m right. I know I’m right. Anyone who says otherwise is wrong.”\n\n\nImagine working on a specific area of research for most or all of your career. You become THE leading expert in the topic, and others defer to your authority…for a time. Just as you start to wane in fame, a young hotshot is putting out research that directly critiques your fundamental findings. Will you take an organized skeptical approach and re-assess your work (you should!), or will you instead fight back vociferously and vocally, in spite of mounting evidence against you? While I would love for scientists to be detached enough to scrutinize their own work even if they are the leading expert on it, unfortunately when one’s identity becomes intertwined with their work, it can be quite difficult to be objective.\nHow do you avoid Organized Dogmatism? Lead with intellectual humility. One excellent example of this is psychologist Julia Rohrer’s Loss of Confidence Project in which she and others admit mistakes that they have made in their previous research. This is a remarkable departure from the tempting counter-norms of Organized Dogmatism and Interestedness, because instead of lauding one’s own accomplishments, it publicly declares instances of one’s errors. Normalizing errors and reporting them is a noble effort, and one that can fight against the counter-norms impeding the progress of science.\nAs pointed out by Vox science and health editor Brian Resnick,13 there are at least three key challenges to humility (which I paraphrase):\n\nOur minds (even the geniuses among us) have blindspots and are more imperfect than we are willing to admit.\nWe need a culture that celebrates the words “I was wrong.”\nWe will never achieve perfect intellectual humility, so we need to be thoughtful with our convictions."
  },
  {
    "objectID": "Chapters/replication2.html#conclusion",
    "href": "Chapters/replication2.html#conclusion",
    "title": "Mertonian Norms and Counter-Norms",
    "section": "Conclusion",
    "text": "Conclusion\nIn short, Mertonian Norms are aspirational and admirable. We should strive to do science in accordance with these norms. However, let’s also be real and recognize that the counter-norms often motivate our behavior. We should be cognizant of them, and engage in habits and practices that actively minimize their influence on our behavior such as transparency and humility, and should strive to enact systems that are consonant with Mertonian Norms such as double-blind peer review, and keeping a lid on self-citations.\n\n\n\n\n1. Cartwright N. Middle-range theory: Without it what could anyone do? THEORIA An International Journal for Theory, History and Foundations of Science. Published online September 2020. Accessed June 13, 2023. https://ojs.ehu.eus/index.php/THEORIA/article/view/21479\n\n\n2. Merton RK. Science and technology in a democratic order, reprinted as The normative structure of science. Published online 1942. https://sciencepolicy.colorado.edu/students/envs_5110/merton_sociology_science.pdf\n\n\n3. Huckvale K, Torous J, Larsen ME. Assessment of the Data Sharing and Privacy Practices of Smartphone Apps for Depression and Smoking Cessation. JAMA Network Open. 2019;2(4):e192542. doi:10.1001/jamanetworkopen.2019.2542\n\n\n4. Saint-Exupéry A de. Le petit prince [The little prince]. Verenigde State van Amerika: Reynal & Hitchkock (US), Gallimard (FR). Published online 1943.\n\n\n5. Mitroff II. Norms and Counter-Norms in a Select Group of the Apollo Moon Scientists: A Case Study of the Ambivalence of Scientists. American Sociological Review. 1974;39(4):579-595. doi:10.2307/2094423\n\n\n6. Cattau D. David Blackwell, ’Superstar’. The University of Illinois Alumni Association – Illinois Alumni Magazine. Published online 2010.\n\n\n7. Brin S, Page L. The Anatomy of a Large-Scale Hypertextual Web Search Engine. Computer Networks. 1998;30:107-117. Accessed June 21, 2023. http://www-db.stanford.edu/~backrub/google.html\n\n\n8. Zaveri B. Google’s Revenue By Segment (2016-2023). Business Quant. Published online February 2020. Accessed June 21, 2023. https://businessquant.com/google-revenue-by-segment\n\n\n9. Easterbrook PJ, Gopalan R, Berlin J, Matthews DR. Publication bias in clinical research. The Lancet. 1991;337(8746):867-872.\n\n\n10. Sternberg RJ. \"Am I Famous Yet?\" Judging Scholarly Merit in Psychological Science: An Introduction. Perspectives on Psychological Science: A Journal of the Association for Psychological Science. 2016;11(6):877-881. doi:10.1177/1745691616661777\n\n\n11. Flaherty C. Revolt Over an Editor. Inside Higher Ed. Published online 2018. Accessed June 21, 2023. https://www.insidehighered.com/news/2018/04/30/prominent-psychologist-resigns-journal-editor-over-allegations-over-self-citation\n\n\n12. Rokeach M. The nature and meaning of dogmatism. Published online 1954.\n\n\n13. Resnick B. Intellectual humility: The importance of knowing you might be wrong. Vox. Published online January 2019. Accessed June 21, 2023. https://www.vox.com/science-and-health/2019/1/4/17989224/intellectual-humility-explained-psychology-replication"
  },
  {
    "objectID": "Chapters/datatypes.html#vectors",
    "href": "Chapters/datatypes.html#vectors",
    "title": "Data Types",
    "section": "Vectors",
    "text": "Vectors\n\nTypes\nA vector is a sequence of data elements of the same type. In R, there are two types of vectors - atomic vectors and lists. Within the category of atomic vectors, there are six types:\n\nLogical (TRUE, FALSE, NA)\nInteger (a type of numeric vector containing only integers)\nDouble (a type of numeric vector which is the default storage type for numbers in R; represents floating point numbers which can’t always be precisely represented by fixed memory)\nCharacter (a vector of strings, which are pieces of text. Each string is surrounded by quotes \" \".)\nComplex (a vector of elements that include complex numbers)\nRaw (a vector containing a ‘raw’ sequence of bytes; very unusual data type)\n\nYou should note that Doubles and Integers are both numeric vectors, but Doubles are far more common data structures that we typically deal with in social science research. Doubles are quite flexible in that they can be written in decimal, scientific, or hexadecimal form. They also have three unique values: Inf (Infinity), -Inf (Negative Infinity), and NaN (not a number). In practical usage, if you get a value corresponding to any of these three values, something has probably gone wrong. For instance, let’s see what happens when you try to divide certain values by 0 (which is, of course, undefined):\n\n54/0  \n\n[1] Inf\n\n-54/0  \n\n[1] -Inf\n\n0/0 \n\n[1] NaN\n\n\nIntegers cannot contain fractional values (i.e. no decimals), and are written like Doubles but are followed by an L as in 2L, 3L and so on. Generally, we won’t have to worry about these since we will usually be working with Doubles, Logical, or Character vectors.\nThe most common vectors we will deal with are Logical, Double, and Character vectors. Atomic vectors contain elements of the same type. You can always check the type of vector with the typeof() or class() commands. Additionally, you can see the number of elements in a given vector with the length() command.\n\n\nCreating Vectors\nAs mentioned, vectors can comprise strings (pieces of text), integers, or logical values. While they can also comprise a sequence of bytes and complex numbers, we will not discuss these as they are less relevant for our purposes. Let’s start with creating a numeric vector, which can be an integer or double (it doesn’t matter for our purposes). The most common way to create a vector is to use the concatenate or combine c() function with values in parentheses separated by commas. To create a character vector, you need to surround each element of the vector in quotes \" \" separated by commas. The paste() function can also be useful in combining vectors.\nYou can also create a vector comprising a sequence of numbers using the colon :.\n\n# Create two numeric vectors.\na &lt;- c(1, 2, 3, 4)\nb &lt;- c(5, 6, 7, 8)\n\n# You can do the same thing with a colon instead.\na &lt;- c(1:4)\nb &lt;- c(5:8)\n\n# Create a new vector c that combines vectors a and b. \nc &lt;- c(a,b)\nprint(c)\n\n[1] 1 2 3 4 5 6 7 8\n\n# Create two character vectors\nd &lt;- c(\"Nirvana\", \"Alice in Chains\")\ne &lt;- c(\"Soundgarden\", \"Pearl Jam\")\n\n# Create a new vector f that combines vectors d and e.\nf &lt;- c(d,e)\n\n# Create three character vectors.\ng &lt;- \"Four score and seven years ago our fathers brought forth on this continent,\"\nh &lt;- \"a new nation, conceived in Liberty, and dedicated to the\"\ni &lt;- \"proposition that all men are created equal.\"\n\n# Combine the character vectors using the paste() function.\nj &lt;- paste(g,h,i)\nprint(j)\n\n[1] \"Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal.\"\n\n\n\n\nExtracting Elements from a Vector\nTo extract an element from a vector, we use square brackets [] after the vector name, and write the position of the element(s) we want to extract. We can extract multiple elements by using the concatenate or combine c() function, such as vector1[c(1,4,5)]. To extract elements in a sequential range, we can use colon : between positions, such as vector1[c(1:5)].\nLet’s try an example with a visual summary. First, let’s create a character vector called catbreeds with the values \"American Bobtail\", \"Abyssinian\", \"Burmese\", \"Himalayan\", and \"Manx\". We will create the vector with our trusty concatenate or combine c() function. Remember that if you have strings, or pieces of text, these are surrounded by quotes \" \". You don’t need quotes if you are creating any of the other vectors mentioned.\n\n# I like one string per line to keep things organized and neat.\ncatbreeds &lt;- c(\"American Bobtail\",\n               \"Abyssinian\",\n               \"Burmese\",\n               \"Himalayan\",\n               \"Manx\")\n# Now I write the name of the vector to see its elements.\ncatbreeds\n\n[1] \"American Bobtail\" \"Abyssinian\"       \"Burmese\"          \"Himalayan\"       \n[5] \"Manx\"            \n\n\nOk, so now we have a nice character vector called catbreeds where the different strings (or pieces of text) correspond to different cat breeds. Now, how do we extract or print different elements of the vector? This is summarized in Figure fig-catvec.\n\n\n\nFigure 2: Retrieving different elements of a vector by position number.\n\n\nYou can see what this looks like in practice below.\n\n# Let's extract \"American Bobtail\" which is the first element in the vector. \ncatbreeds[1]\n\n[1] \"American Bobtail\"\n\n# Now let's extract three breeds - Abyssinian, Burmese, and Himalayan. \n# These are elements 2, 3, and 4 in the vector.\ncatbreeds[2:4]\n\n[1] \"Abyssinian\" \"Burmese\"    \"Himalayan\" \n\n# What if we want to extract multiple elements that are not sequential? \n# Let's do that with Burmese (element 3) and Manx (element 5).\ncatbreeds[c(3,5)]\n\n[1] \"Burmese\" \"Manx\"   \n\n\nYou can also extract only positive or only negative integers in a vector by specifying the relevant rule within square brackets after the vector name. For example, let’s create a new vector vectorbeta that comprises only the positive integers of vectoralpha. Then we’ll create another new vector vectorgamma that comprises only the negative integers of vectoralpha.\n\n# Create a numeric vector\nvectoralpha &lt;- c(29, 149, 217, -226, 55, 64, -103, -313, 368, 189)\n\n# Create a new vector vectorbeta that takes only positive integers of vectoralpha.  \n# This code says take all the integers greater than 0 in vectoralpha and assign \n# the values to a new vector called vectorbeta. \nvectorbeta &lt;- vectoralpha[vectoralpha &gt; 0] \nprint(vectorbeta)  \n\n[1]  29 149 217  55  64 368 189\n\n# Create a new vector vectorgamma that takes only the negative integers of vectoralpha. \n# This code says take all the integers less than 0 in vectoralpha and assign \n# the values to a new vector called vectorgamma. \nvectorgamma &lt;- vectoralpha[vectoralpha &lt; 0] \nprint(vectorgamma)\n\n[1] -226 -103 -313\n\n\n\n\nVector Operations\nIn a numeric vector, you can perform a number of operations such as finding the mean mean(), median median(), minimum min(), and maximum max().\n\n# Create a numeric vector\n\nvectoralpha &lt;- c(29, 149, 217, -226, 55, 64, -103, -313, 368, 189)\n\n# Find the minimum and maximum values in the vector. \n\nmin(vectoralpha)\n\n[1] -313\n\nmax(vectoralpha)\n\n[1] 368\n\n# Find the mean and median of the vector.\n\nmean(vectoralpha)\n\n[1] 42.9\n\nmedian(vectoralpha)\n\n[1] 59.5\n\n\nYou can also perform arithmetic operations on vectors. Let’s create a couple of vectors and illustrate the four basic arithmetic operations. Each operation is performed element by element. This means that if both vectors are the same length (same number of elements), the operation will be conducted on the first element of both vectors, then the second element of both vectors, and so on. For example, in vectors a1 and b1 below, addition would involve the following steps: 1 + 2, 3 + 1, and 6 + 4 since these number comprise the first, second, and third elements of each vector, respectively.\n\n# Create two vectors. \na1 &lt;- c(1, 3, 6)\nb1 &lt;- c(2, 1, 4)\n\n# Vector Addition\na1 + b1\n\n[1]  3  4 10\n\n# Vector Subtraction\na1 - b1\n\n[1] -1  2  2\n\n# Vector Multiplication\na1 * b1\n\n[1]  2  3 24\n\n# Vector Division\na1 / b1\n\n[1] 0.5 3.0 1.5\n\n\n\n\n\n\n\n\nDon’t Just Keep Reading!\n\n\n\n\n\n\nManx cat wants you to try coding up a couple of vectors of your own before proceeding.\n\n\nTry creating your own vector and extracting the elements. Once you feel like you’ve got it, only then move on to Matrices. Try creating a numeric vector as well, and extracting its elements using the techniques illustrated above."
  },
  {
    "objectID": "Chapters/datatypes.html#matrices",
    "href": "Chapters/datatypes.html#matrices",
    "title": "Data Types",
    "section": "Matrices",
    "text": "Matrices\nA matrix is a two-dimensional vector (i.e. rows and columns.) All columns in matrix should have the same type (e.g. logical, character, numeric) and same length. This means that matrices are homogeneous data structures in that all elements must be of the same type.\nYou can create a matrix from vectors using the rbind() function to combine rows of data, and using the cbind() function to combine columns of data.\n\n# row bind \n\na &lt;- c(.94, .92, .95) \nb &lt;- c(.25, .56, .82) \nc &lt;- c(.65, .45, .37) \nd &lt;- rbind(a, b, c) \n\nprint(d)  \n\n  [,1] [,2] [,3]\na 0.94 0.92 0.95\nb 0.25 0.56 0.82\nc 0.65 0.45 0.37\n\n# column bind  \ne &lt;- c(34.5, 23.6) \nf &lt;- c(22.3, 13.2) \ng &lt;- cbind(e,f) \n\nprint(g)\n\n        e    f\n[1,] 34.5 22.3\n[2,] 23.6 13.2\n\n\nYou can also create a matrix using the matrix() function in which the main arguments are the data vector, the number of rows, and the number of columns. Here is a simple matrix with the numbers 1:9 spread over three rows and three columns:\n\nmatrix(1:9, nrow = 3, ncol = 3)\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\nYou can also change the names of the rows and columns of the matrix using the rownames() and colnames() functions. Let’s try using the matrix() function to input a correlation matrix into R. A correlation matrix is a type of matrix which displays correlation coefficients between variables. Essentially, it shows you the strength of a relationship between pairs of variables ranging from -1 to 1. There are many types of correlation coefficients, but typically Pearson’s \\(r\\) is used. When you look at a correlation matrix, you will notice that 1s are on the diagonal, because a variable is always perfectly correlated with itself. The upper triangle (above the diagonal) is a mirror image of the bottom triangle (below the diagonal), so you only need to focus on one. Personally, I like focusing on the lower triangle, but it doesn’t much matter.\nImagine a study on exam performance and study habits with the following variables:\n\nHours spent studying\nExam Score\nIQ Score\nHours spent sleeping\nSchool Rating\n\nNow let us examine the correlation matrix of the variables of this hypothetical study illustrated in Figure fig-cormat1.\n\nIt looks like there is a strong correlation (\\(r = 0.82\\)) between hours spent studying and exam score, which makes sense. We also have smaller correlations between IQ and exam score \\((r = 0.33)\\) and school rating and exam score \\((r = 0.23)\\). All other correlations with exam score are negligible. Let’s now input this correlation matrix into R.\n\n# Input correlations \n\ncorrs &lt;- c(1.00, 0.82, 0.08, -0.22, 0.36, 0.82, \n           1.00, 0.33, -0.04, 0.23, 0.48, 0.33, \n           1.00, 0.06, 0.02, -0.22, -0.04, 0.06, \n           1.00, 0.12, 0.36, 0.23, 0.02, 0.12, \n           1.00)  \n\n# Use matrix() function and specify appropriate rows and columns \n\ncorrmat1 &lt;- matrix(corrs, nrow = 5, ncol = 5)  \n\n# Rename rows and columns \n\ncolnames(corrmat1) &lt;- c(\"Hours studying\", \n                        \"Exam Score\", \n                        \"IQ Score\", \n                        \"Hours sleeping\", \n                        \"School rating\") \n\nrownames(corrmat1) &lt;- c(\"Hours studying\", \n                        \"Exam Score\", \n                        \"IQ Score\", \n                        \"Hours sleeping\", \n                        \"School rating\")  \n\nprint(corrmat1) \n\n               Hours studying Exam Score IQ Score Hours sleeping School rating\nHours studying           1.00       0.82     0.48          -0.22          0.36\nExam Score               0.82       1.00     0.33          -0.04          0.23\nIQ Score                 0.08       0.33     1.00           0.06          0.02\nHours sleeping          -0.22      -0.04     0.06           1.00          0.12\nSchool rating            0.36       0.23     0.02           0.12          1.00\n\n\nTo retrieve an element of a matrix, write the matrix name, followed by square brackets in which the first argument is the row and the second is the column that you want. For example, if we want to retrieve the correlation between \"Hours spent studying\" and \"Exam Score\", we can write the matrix name corrmat1 followed by row 2 and column 1:\n\ncorrmat1[2,1]\n\n[1] 0.82\n\n\nThis retrieves the appropriate correlation of 0.82. Finally, let’s say you want to see the values for an entire column or an entire row. In this case, simply leave the row or column argument blank:\n\n# See the values for column 3 only \ncorrmat1[, 3]  \n\nHours studying     Exam Score       IQ Score Hours sleeping  School rating \n          0.48           0.33           1.00           0.06           0.02 \n\n# See the values for row 4 only \ncorrmat1[4, ] \n\nHours studying     Exam Score       IQ Score Hours sleeping  School rating \n         -0.22          -0.04           0.06           1.00           0.12"
  },
  {
    "objectID": "Chapters/datatypes.html#dataframes",
    "href": "Chapters/datatypes.html#dataframes",
    "title": "Data Types",
    "section": "Dataframes",
    "text": "Dataframes\nA dataframe is the most common type of data structure we typically deal with in social science. It is a two-dimensional labelled list of vectors, and can have columns of multiple data types. A dataframe’s vectors must be of the same length, which gives dataframes a rectangular structure. A dataframe has rownames() and colnames() just like matrices and lists. The variable names in a dataframe also must be different from one another, meaning that the two variables cannot have the exact same name.\nIf you want to explore R’s built-in dataframes to test out functions and analyses, you can run the data() command to see a list of dataframes. Let’s go ahead and use the built-in dataframe mtcars by assigning it to an object called df1 (dataframe 1). Then, let’s look at the dataframe. If you want to have a very quick preview of the first six rows of the dataframe, invoke the head() function, and similarly the last six rows of the dataframe can be previewed using the tail() function. In both cases, the name of the dataframe should be inside the parentheses.\nThe head() and tail() functions are useful when you have a very large dataset and cannot feasibly examine the whole thing. However, I find the View() function far more useful. This actually pulls up the entire dataframe in a separate tab.\n\n# Assign the built-in mtcars dataframe to an object called df1.\ndf1 &lt;- mtcars \n\n# Examine the first six rows of the dataframe.\nhead(df1)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n# Examine the last six rows of the dataframe.\ntail(df1)\n\n                mpg cyl  disp  hp drat    wt qsec vs am gear carb\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.7  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.9  1  1    5    2\nFord Pantera L 15.8   8 351.0 264 4.22 3.170 14.5  0  1    5    4\nFerrari Dino   19.7   6 145.0 175 3.62 2.770 15.5  0  1    5    6\nMaserati Bora  15.0   8 301.0 335 3.54 3.570 14.6  0  1    5    8\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.6  1  1    4    2\n\n# Have a much more detailed look at the dataframe.\nView(df1) \n\nYou can also click the little dataframe icon next to the dataframe in your Environment tab to view the dataframe.\n\nWhether you use the View() function or click the little icon, R will open another tab with the dataframe, and will look something like this:\n\nIf we want to know the dimensions of the dataframe (imagine it’s huge and we can’t easily View() the whole dataset), as well as the structure of the dataset, we can use the functions: nrow() (to see the number of rows), ncol() (to see the number of columns), and str() (see the structure of the dataframe).\n\n# See the number of rows in a dataframe  \nnrow(df1)    \n\n[1] 32\n\n# See the number of columns in a dataframe  \nncol(df1)    \n\n[1] 11\n\n# See the structure of the dataframe  \nstr(df1) \n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n\nNote that the str() function gives you a quick overview of the type of variables and some of their values in the dataframe. You can get this information using the View() function, which I tend to prefer, but if you want to quickly examine the structure of multiple dataframes, for instance, you can use the str() function.\nNotice how the dataframe comprises rows and columns, where the rows are names of cars and the columns are different attributes of those cars, such as miles per gallon, number of cylinders, displacement weight, and others. To retrieve a particular column in the dataframe, use the dollar sign $ operator after the name of the dataframe, followed by the column name. For instance, if you want to know the mean miles per gallon across all the cars, we can use the mean() function in the following manner:\n\n# Get the mean of the mpg variable in the df1 dataframe.\nmean(df1$mpg)\n\n[1] 20.09062\n\n\nFrom the mean() function used above, we see that across all the cars in the dataframe, the average fuel efficiency is about 20 miles per gallon.\nIn general, you will use the format dfname$variable to access or refer to any specific variable or column of a dataframe. You can also add a new variable to the dataframe using the same format. For example, if we wanted to add a new column to our dataset df1 in which we take the existing column qsec (the number of seconds it takes the car to travel one-fourth of a mile) and multiply this by four, to get an estimate of how long it takes the car to travel one mile, we can accomplish by with the following code.\n\n# Create new variable in df1 called qsec2 which takes the existing variable\n# qsec and multiplies it by four.\ndf1$qsec2 &lt;- df1$qsec*4\n\n# The mean of qsec2 is predictably four times the mean of qsec.\nmean(df1$qsec)\n\n[1] 17.84875\n\nmean(df1$qsec2)\n\n[1] 71.395\n\n\nTo get a quantitative summary of numeric variables in a dataframe, we can use the summary() function to obtain the minimum value, 25th percentile, median, 75th percentile, and maximum value. Let’s try that with the weight wt variable. Then, we’ll obtain the information for multiple variables in the dataframe using square brackets [], the concatenation operator c(), and single apostrophes ' ' around the variable names.\n\n# Summary statistics for only the weight variable \nsummary(df1$wt)  \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.513   2.581   3.325   3.217   3.610   5.424 \n\n# Summary statistics for multiple variables \nsummary(df1[c('mpg','wt', 'qsec')]) \n\n      mpg              wt             qsec      \n Min.   :10.40   Min.   :1.513   Min.   :14.50  \n 1st Qu.:15.43   1st Qu.:2.581   1st Qu.:16.89  \n Median :19.20   Median :3.325   Median :17.71  \n Mean   :20.09   Mean   :3.217   Mean   :17.85  \n 3rd Qu.:22.80   3rd Qu.:3.610   3rd Qu.:18.90  \n Max.   :33.90   Max.   :5.424   Max.   :22.90"
  },
  {
    "objectID": "Chapters/datatypes.html#lists",
    "href": "Chapters/datatypes.html#lists",
    "title": "Data Types",
    "section": "Lists",
    "text": "Lists\nWhile dataframes can contain data of different types (heterogeneous data), lists are even more flexible at doing this. A list is like a collection of different things that don’t have to play by the rules of a dataframe. Recall that though dataframes can contain heterogeneous data, the elements of a dataframe are subject to a few restrictions such as a) they have to be a labelled list of vectors, b) they have to be the same length, and c) the variables must not have the exact same name.\nList elements don’t have such restrictions, and you can think of them as collections stuff. For example, you can put the contents of a function, a dataframe, a piece of text, numbers, etc. Let’s look at an example of a list called stuff. We will create this list using the list() function and assigning a bunch of random stuff to it. You can even put lists in a list! That’s pretty meta. You can then View() the list to examine its contents.\n\n# Create a list called stuff comprising various random things.\nstuff &lt;- list(catbreed = \"manx\", \n              data = mtcars, \n              Euler = 2.71828, \n              logical = c(TRUE, FALSE, TRUE, NA),\n              random = list(alphabet = letters, \n                            greeting = \"Hello, World\", \n                            number = pi\n                            )\n              )\n\nOnce you View() the list, it will show you the contents as in Figure fig-stufflist.\n\nYou can see that our list has a string (“Manx”), a dataframe (mtcars), a number (Euler), a logical vector (logical), and another list with three elements of its own (random). Lists are so flexible!\nYou can very easily extract elements of a list with the dollar sign $ operator if you know the name of the element, or using double square brackets [[ ]] if you know the position of the element. You can also use single square brackets [ ] which will give you a little more information than double square brackets, such as the name of the element.\n\n# Extract elements of a list by name using dollar sign operator.\nstuff$Euler\n\n[1] 2.71828\n\n# Extract elements of a list by using double square brackets.\nstuff[[3]]\n\n[1] 2.71828\n\n# Extract elements of a list by using single square brackets.\nstuff[3]\n\n$Euler\n[1] 2.71828\n\n\nAs you see, using single brackets [ ] to extract an element from a list gives you the element name Euler as well as the value, as opposed to only the value provided by the double square brackets [[ ]]. In general, if you have a collection of stuff, create a list for easy reference."
  },
  {
    "objectID": "Chapters/datatypes.html#arrays",
    "href": "Chapters/datatypes.html#arrays",
    "title": "Data Types",
    "section": "Arrays",
    "text": "Arrays\nThough we will not work with Arrays in this course, it may be worthwhile to know a thing or two about them. Recall that matrices and dataframes are two-dimensional data structures. An array goes beyond two dimensions, and can hold n-dimensional data, but the data have to be of the same type. I think arrays are easiest to understand when thinking about a collection of multiple matrices.\nFor example, let’s say we have two vectors of different lengths. One vector has three elements, while the other has six elements.\n\n# Create two vectors of different lengths (three and six).\nvector1 &lt;- c(1,3,4)\nvector2 &lt;- c(32, 4, 7, 19, 23, 43)\n\nNow, let’s create an array where we first create a matrix of three rows and three columns based on our vectors, and then duplicate this matrix four times. We can accomplish this using the array() function where the first argument will comprise our vectors, and the argument dim (or dimension) takes the row numbers, column numbers, and number of matrices we want. So if we want three rows, three columns, and four matrices, the values would be dim = c(3,3,4).\n\n# Use previous vectors to create array of four 3x3 matrices. \narray1 &lt;- array(c(vector1, vector2),\n                dim = c(3,3,4)\n                )\n\nprint(array1)\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1   32   19\n[2,]    3    4   23\n[3,]    4    7   43\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]    1   32   19\n[2,]    3    4   23\n[3,]    4    7   43\n\n, , 3\n\n     [,1] [,2] [,3]\n[1,]    1   32   19\n[2,]    3    4   23\n[3,]    4    7   43\n\n, , 4\n\n     [,1] [,2] [,3]\n[1,]    1   32   19\n[2,]    3    4   23\n[3,]    4    7   43\n\n\nSee how we now have four versions of the matrix we requested based on our two vectors? If we want to name our rows, columns and matrices, we can create vectors for those and then feed them into the dimnames (dimension names) argument of array(). The dimnames argument only takes lists, so we’ll stick our name vectors into a list.\n\n# Create row names.\nrownames &lt;- c(\"Row 1\", \n              \"Row 2\", \n              \"Row 3\")\n\n# Create column names.\ncolumnnames &lt;- c(\"Column 1\", \n                 \"Column 2\", \n                 \"Column 3\")\n\n# Create matrix names. \nmatrixnames &lt;- c(\"Matrix Alpha\", \n                 \"Matrix Beta\", \n                 \"Matrix Gamma\",\n                 \"Matrix Delta\")\n\n# Rename the array elements.\n\narray1 &lt;- array(c(vector1, vector2),\n                dim = c(3,3,4),\n                dimnames = list(rownames, \n                             columnnames,\n                             matrixnames)\n                )\n\n# And Voila!\nprint(array1)\n\n, , Matrix Alpha\n\n      Column 1 Column 2 Column 3\nRow 1        1       32       19\nRow 2        3        4       23\nRow 3        4        7       43\n\n, , Matrix Beta\n\n      Column 1 Column 2 Column 3\nRow 1        1       32       19\nRow 2        3        4       23\nRow 3        4        7       43\n\n, , Matrix Gamma\n\n      Column 1 Column 2 Column 3\nRow 1        1       32       19\nRow 2        3        4       23\nRow 3        4        7       43\n\n, , Matrix Delta\n\n      Column 1 Column 2 Column 3\nRow 1        1       32       19\nRow 2        3        4       23\nRow 3        4        7       43\n\n\nWhile more could be said about arrays, we will end here since we really don’t need to worry about them in this course. The data structure we will work with most is dataframes, so make sure you practice working with those!"
  },
  {
    "objectID": "Chapters/datatypes.html#exercises",
    "href": "Chapters/datatypes.html#exercises",
    "title": "Data Types",
    "section": "Exercises",
    "text": "Exercises\nAs always, it’s a good idea to attempt these while the material is still fresh. You can find the answers in sec-appendixb.\n\nCreate a numeric vector called vec1 comprising the elements 3, -12, 532, 0, -100, 55, and -42. Then, find the median and lowest value in the vector.\nCreate a new vector vec2 which contains all the elements of vec1 that are positive integers. Then create a new vector vec3 that contains all the elements of vec1 that are negative integers. Then create a new vector vec4 which is the sum of vec2 and vec3.\nCreate a variable called MarySue1 with the value \"Dr Mary Sue Coleman, former president of the University of Michigan once said\". Then, create another variable called MarySue2 with the value \"For today, goodbye. For tomorrow, good luck. And Forever, Go Blue!\". Then find the number of characters in each variable using the nchar() function. Then, check if the letter ‘r’ is present in each variable by using the grepl() function, in which the first argument is thing you want to find (i.e. 'r'), and the second argument is the variable name (i.e. MarySue1 and MarySue2).\nCreate a variable called MarySue3 whose value is a concatenation (combination) of MarySue1 and MarySue2 using the paste() function, in which the arguments are the variables separated by a comma. Then, print the value for MarySue3.\nAssign the built-in dataframe mtcars to an object named after your favorite animal. Then calculate and print the median of the variable mpg.\nCreate a new variable and add to the object you created above (named after your favorite animal). This variable will take the variable Miles per Gallon mpg and convert it to Kilometers per Liter, which you will name kpl. This can be accomplished by taking mpg and dividing it by 2.352. Once you’ve created this new variable and added it to the object, calculate the median of kpl.\n\n\n\n\n\n1. Grolemund G. Hands-on Programming with R: Write Your Own Functions and Simulations. \" O’Reilly Media, Inc.\"; 2014."
  },
  {
    "objectID": "Chapters/replication3.html#reproducibility-and-replication-definitions",
    "href": "Chapters/replication3.html#reproducibility-and-replication-definitions",
    "title": "Crises of Replication",
    "section": "Reproducibility and Replication Definitions",
    "text": "Reproducibility and Replication Definitions\nLet’s remind ourselves what Reproducibility and Replicability mean. As you see in Figure fig-rep3, Reproducibility involves literally reproducing the results from a study using original data, code, and materials. Replicability is about carrying out the study using the same procedures in different settings and achieving consistent, not identical, results."
  },
  {
    "objectID": "Chapters/replication3.html#null-hypothesis-significance-testing",
    "href": "Chapters/replication3.html#null-hypothesis-significance-testing",
    "title": "Crises of Replication",
    "section": "Null Hypothesis Significance Testing",
    "text": "Null Hypothesis Significance Testing\nNext, let’s either remind ourselves (if you’ve taken a statistics course before) what is commonly referred to as Null Hypothesis Significance Testing (NHST). NHST is a method of statistical inference in which you test the thing you think will happen in relation to a straw-man hypothesis usually corresponding to the notion “Nothing is going to happen.” Let’s break down how it works.\nWhen we talk about statistical hypothesis testing, we typically have two competing hypotheses:\n\nThe Null Hypothesis (\\(H_0\\)) typically suggests that there is no difference between two quantities or groups on some meaningful parameter. For example, if I’m running a study to test the efficacy of a new vaccine on a new disease, my null hypothesis is that is no effect of the vaccine on the disease.\nThe Alternative Hypothesis (\\(H_A\\)) is typically what we are trying to assert or explore. To use the above example, my alternative hypothesis would be that the new vaccine has a non-zero effect on the new disease.\n\nIf the sample data provide enough evidence against the null hypothesis, we say that we reject the null hypothesis in favor of the alternative hypothesis. If the sample data not provide enough evidence against the null hypothesis, we say that fail to reject the null hypothesis. Note that we can NEVER say that we accept the null hypothesis.\n\n\n\n\n\n\nWhy can’t we ever accept the null hypothesis?\n\n\n\nIn short, we assume the null is true and are trying to find evidence against it, so our decision is always framed in terms of rejecting or failing to reject the null hypothesis. On a more fundamental level, the idea here is that you cannot prove a negative. If your sample data do not support the null hypothesis, you have only shown that and just that. You have not shown that null hypothesis is true. It might be true (and indeed we assume it is when doing the tests), but your study is not exploring its verity, rather your study is trying to find evidence that contradicts the null. It is a sort of proof by contradiction.\n\n\nNow that we’ve got the definitional summary out of the way, let’s talk about what is often called The Replication Crisis in psychology."
  },
  {
    "objectID": "Chapters/references.html",
    "href": "Chapters/references.html",
    "title": "References",
    "section": "",
    "text": "1. National Academies of Sciences E.\nReproducibility and Replicability in\nScience.; 2019. doi:10.17226/25303\n\n\n2. Simons DJ, Shoda Y, Lindsay DS. Constraints on\nGenerality (COG): A\nProposed Addition to All\nEmpirical Papers. Perspectives on\nPsychological Science. 2017;12(6):1123-1128. doi:10.1177/1745691617708630\n\n\n3. Moravcsik A. Transparency in Qualitative\nResearch. SAGE Publications Limited; 2020.\n\n\n4. Steinle F. Stability and\nReplication of Experimental\nResults: A Historical\nPerspective. In: Reproducibility :\nPrinciples, Problems, Practices,\nand Prospects. John Wiley & Sons, Incorporated;\n2016. http://ebookcentral.proquest.com/lib/upenn-ebooks/detail.action?docID=4547409\n\n\n5. Glick TF, Livesey SJ, Wallis F. Routledge\nRevivals: Medieval Science,\nTechnology and Medicine (2006):\nAn Encyclopedia. Taylor & Francis;\n2017.\n\n\n6. Zampieri F, ElMaghawry M, Zanatta A, Thiene G.\nAndreas Vesalius: Celebrating 500 years of\ndissecting nature. Global Cardiology Science & Practice.\n2015;2015(5):66. doi:10.5339/gcsp.2015.66\n\n\n7. Cartwright N. Middle-range theory:\nWithout it what could anyone do? THEORIA An\nInternational Journal for Theory, History and Foundations of\nScience. Published online September 2020. Accessed June 13, 2023.\nhttps://ojs.ehu.eus/index.php/THEORIA/article/view/21479\n\n\n8. Merton RK. Science and technology in a\ndemocratic order, reprinted as The normative structure of\nscience. Published online 1942. https://sciencepolicy.colorado.edu/students/envs_5110/merton_sociology_science.pdf\n\n\n9. Huckvale K, Torous J, Larsen ME. Assessment of\nthe Data Sharing and Privacy\nPractices of Smartphone Apps for\nDepression and Smoking Cessation.\nJAMA Network Open. 2019;2(4):e192542. doi:10.1001/jamanetworkopen.2019.2542\n\n\n10. Saint-Exupéry A de. Le petit prince\n[The little prince]. Verenigde State van Amerika:\nReynal & Hitchkock (US), Gallimard (FR). Published online\n1943.\n\n\n11. Mitroff II. Norms and\nCounter-Norms in a Select\nGroup of the Apollo Moon\nScientists: A Case\nStudy of the Ambivalence of\nScientists. American Sociological Review.\n1974;39(4):579-595. doi:10.2307/2094423\n\n\n12. Cattau D. David Blackwell,\n’Superstar’. The University of Illinois Alumni\nAssociation – Illinois Alumni Magazine. Published online\n2010.\n\n\n13. Brin S, Page L. The Anatomy of a\nLarge-Scale Hypertextual\nWeb Search Engine. Computer\nNetworks. 1998;30:107-117. Accessed June 21, 2023. http://www-db.stanford.edu/~backrub/google.html\n\n\n14. Zaveri B. Google’s Revenue\nBy Segment (2016-2023). Business\nQuant. Published online February 2020. Accessed June 21, 2023. https://businessquant.com/google-revenue-by-segment\n\n\n15. Flaherty C. Revolt Over an\nEditor. Inside Higher Ed. Published online 2018.\nAccessed June 21, 2023. https://www.insidehighered.com/news/2018/04/30/prominent-psychologist-resigns-journal-editor-over-allegations-over-self-citation\n\n\n16. Easterbrook PJ, Gopalan R, Berlin J, Matthews\nDR. Publication bias in clinical research. The Lancet.\n1991;337(8746):867-872.\n\n\n17. Sternberg RJ. \"Am I\nFamous Yet?\" Judging\nScholarly Merit in Psychological\nScience: An Introduction.\nPerspectives on Psychological Science: A Journal of the Association\nfor Psychological Science. 2016;11(6):877-881. doi:10.1177/1745691616661777\n\n\n18. Rokeach M. The nature and meaning of dogmatism.\nPublished online 1954.\n\n\n19. Resnick B. Intellectual humility: The\nimportance of knowing you might be wrong. Vox. Published online\nJanuary 2019. Accessed June 21, 2023. https://www.vox.com/science-and-health/2019/1/4/17989224/intellectual-humility-explained-psychology-replication\n\n\n20. Grolemund G. Hands-on Programming with\nR: Write Your Own Functions and\nSimulations. \" O’Reilly Media, Inc.\"; 2014."
  },
  {
    "objectID": "Chapters/app1.html",
    "href": "Chapters/app1.html",
    "title": "Exercise 5.8 Answers.",
    "section": "",
    "text": "Calculate \\(89+9/(4*5)^2\\) and assign the value to the object solution1. Then print solution1.\n\n\nsolution1 &lt;- (89+9)/(4*5)^2 \nprint(solution1)\n\n\nWhat is the difference between R and R Studio?\n\nBase R refers to the statistical programming language and application installed on your computer to process the R programming language. RStudio is an Integrated Development Environment (IDE) that integrates with R to provide much more functionality. You can use base R without RStudio, but not the other way around.\n\nHow do you add a comment to a Script file?\n\n\n# Just add a hash/pound sign to the left. \n#### You can add more hashes for aesthetic purposes ####\n\n# Multi-line comments require a hash \n# starting on the left of each line.\n\n\nWhat are packages in R, and how do you install them?\n\nR packages are user-written collections of functions, compiled code, and sample data. There are over 9000+ packages in R and counting. We use packages for specific things we want to do that we cannot accomplish with the functions in base R, or to do things easier or more efficiently than base R functions.\nMost packages that have been vetted and checked are available on the Comprehensive R Archive Network (CRAN), which is the central R package repository.” “In most cases, installing a package in R is accomplished with the following code install.packages(\"name of package\").\n\nInstall and load the package rstudioapi.\n\n\ninstall.packages(\"rstudioapi\")\n\n\nHow do you modify the appearance of R Studio?\n\nTools –&gt; Global Options –&gt; Appearance –&gt; Editor Theme.\n\nAssign the value of 365 to an object called year. Then, create another object called month and assign it the value year/12.\n\n\nyear &lt;- 365\nmonth &lt;- year/12\n\n\nCreate three variables named Cowboys, Giants, and Commanders and assign them all the value \"Inferior Team\" using multiple assignment operators.\n\n\nCowboys &lt;- Giants &lt;- Commanders &lt;- \"Inferior Team\""
  },
  {
    "objectID": "Chapters/app2.html",
    "href": "Chapters/app2.html",
    "title": "Exercise 8.6 Answers.",
    "section": "",
    "text": "Create a numeric vector called vec1 comprising the elements 3, -12, 532, 0, -100, 55, and -42. Then, find the median and lowest value in the vector.\n\n\nvec1 &lt;- c(3, -12, 532, 0, -100, 55, -42) \n\nmedian(vec1)\n\n[1] 0\n\nmin(vec1)\n\n[1] -100\n\n\n\nCreate a new vector vec2 which contains all the elements of vec1 that are positive integers. Then create a new vector vec3 that contains all the elements of vec1 that are negative integers. Then create a new vector vec4 which is the sum of vec2 and vec3.\n\n\nvec2 &lt;- vec1[vec1 &gt; 0]\nvec3 &lt;- vec1[vec1 &lt; 0]\nvec4 &lt;- vec2 + vec3\n\n\nCreate a variable called MarySue1 with the value \"Dr Mary Sue Coleman, former president of the University of Michigan once said\". Then, create another variable called MarySue2 with the value \"For today, goodbye. For tomorrow, good luck. And Forever, Go Blue!\". Then find the number of characters in each variable using the nchar() function. Then, check if the letter ‘r’ is present in each variable by using the grepl() function, in which the first argument is thing you want to find (i.e. 'r'), and the second argument is the variable name (i.e. MarySue1 and MarySue2).\n\n\nMarySue1 &lt;- \"Dr Mary Sue Coleman, former president of the University of Michigan once said\"\n\nMarySue2 &lt;- \"For today, goodbye. For tomorrow, good luck. And Forever, Go Blue!\"\n\nnchar(MarySue1)\n\n[1] 77\n\nnchar(MarySue2)\n\n[1] 66\n\ngrepl('r', MarySue1)\n\n[1] TRUE\n\ngrepl('r', MarySue2)\n\n[1] TRUE\n\n\n\nCreate a variable called MarySue3 whose value is a concatenation (combination) of MarySue1 and MarySue2 using the paste() function, in which the arguments are the variables separated by a comma. Then, print the value for MarySue3.\n\n\nMarySue3 &lt;- paste(MarySue1, MarySue2)\nprint(MarySue3)\n\n[1] \"Dr Mary Sue Coleman, former president of the University of Michigan once said For today, goodbye. For tomorrow, good luck. And Forever, Go Blue!\"\n\n\n\nAssign the built-in dataframe mtcars to an object named after your favorite animal. Then calculate and print the median of the variable mpg.\n\n\n# I'm a big donkey guy, but you can put your favorite animal for the name of the object.\ndonkey &lt;- mtcars \n\nmedian(donkey$mpg)\n\n[1] 19.2\n\n\n\nCreate a new variable and add to the object you created above (named after your favorite animal). This variable will take the variable Miles per Gallon mpg and convert it to Kilometers per Liter, which you will name kpl. This can be accomplished by taking mpg and dividing it by 2.352. Once you’ve created this new variable and added it to the object, calculate the median of kpl.\n\n\ndonkey$kpl &lt;- donkey$mpg / 2.352\nmedian(donkey$kpl)\n\n[1] 8.163265"
  },
  {
    "objectID": "replication3.html#p-value",
    "href": "replication3.html#p-value",
    "title": "13  Some Statistical Preliminaries",
    "section": "13.2 P-Value",
    "text": "13.2 P-Value\nIt’s important that we have a VERY clear definition of a p-value, as this is commonly misunderstood. Let’s use the American Statistical Association’s definition of the p-value.\n\n\n\n\n\n\nDefinition of the p-value from the American Statistical Association\n\n\n\nInformally, a p-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value.3\n\n\nDoes that sound confusing to you? If so, you’re not alone! Defining the p-value accurately doesn’t make it very clear. I think a more intuitive definition of a p-value is an index of surprise. So, if the p-value is very low, that means your results are surprising assuming the null hypothesis is true, and worthy of a second-look. Something may be going on, but the p-value does not give you definitive proof of anything. In fact, that’s how p-values were originally intended to be used, over a century ago (Are my results worthy of a second look?), and this idea has been lost over time.4 Another idea that has been lost over time is the null hypothesis doesn’t have to be a straw man hypothesis of zero effect or association between variables, but could actually be any test hypothesis.5 It should probably not have been called null, as that’s likely why people took it to mean zero or nothing.\nAs important as it is to know what the correct definition of a p-value is, it’s also important to understand what is p-value is NOT.\n\nP-Values do not measure the probability that the hypothesis under study is true. So, don’t base your conclusions solely on whether an association is statistically significant or not.\nP-Values do not measure the probability that the data were produced by chance alone. So, don’t think that the p-value gives you the probability that the observed association was produced by chance alone.\nP-Values do not measure the size of an effect, nor the importance of a result. So, don’t base important decisions on whether a test of an association or effect passes an arbitrary threshold \\alpha.\nBy themselves, p-values do not provide a good measure of evidence for a model or hypothesis. So, don’t believe an association or effect isn’t there just because you didn’t find it.\n\nThe p-value is also affected by the sample size and effect size. Generally, a larger sample size have a higher likelihood of detecting a statistically significant result if there is one. Additionally, a large effect is more likely to be detected than a smaller one.\nOk, so that’s a lot of DON’Ts. You might be wondering, Well what should I do then? To answer this question, we can use the helpful acronym ATOM: Accept uncertainty, be Thoughtful, Open, and Modest.4 Basically, use the p-value but know its limitations."
  },
  {
    "objectID": "replication3.html#power",
    "href": "replication3.html#power",
    "title": "13  Some Statistical Preliminaries",
    "section": "13.3 Power",
    "text": "13.3 Power\nWhen you perform NHST, there are certain conditions of your hypothesis test that have a strong bearing on the p-values obtained, which can affect the interpretation of your results.\nWith any NHST, the following terms apply:\n\nType I error (\\alpha) is the probability of incorrectly rejecting the null hypothesis when it is true. This is commonly known as a false positive. This is typically set at 5%.\nType II error (\\beta) is the probability of failing to reject the null hypothesis when it is false. This is commonly known as a false negative. This is typically set at 20%.\nPower (1 - \\beta) is the pre-study probability of rejecting the null hypothesis, or observing an effect in the sample if there is one. In the social sciences, one usually tries to use a minimum sample size that achieves 80% power.\n\nStatistical power calculations are performed before a study is performed, and typically to determine an appropriate sample size to detect an effect or association of interest. Besides the quantities specified above, one typically needs to also specify an effect size, or minimum detectable effect to be expected. This is usually specified as a 10-15% difference between groups.\nPositive Predictive Value (PPV) is the probability that a positive research finding reflects a true effect (that finding is positive). Probability of research finding being true effect depends on 1) prior probability of it being true (before doing study), 2) statistical power of study, and 3) level of statistical significance. PPV is defined formally as:\n PPV = \\frac{(1 - \\beta) * R} {(1 - \\beta) * R + \\alpha}  \nwhere: 1 - \\beta is statistical power (probability of detecting an effect if there is one), \\beta is the probability of a Type II error (false negative), \\alpha is the probability of a Type I error (false positive), and R is the pre-study probability of observing a non-zero effect.\nThe lower the power and the higher the Type I error, the lower the PPV. If a study has lower power, it can only detect an effect (if there is one) when the size of that effect is large."
  },
  {
    "objectID": "replication4.html#reproducibility-and-replication-definitions",
    "href": "replication4.html#reproducibility-and-replication-definitions",
    "title": "15  Crises of Replication",
    "section": "15.1 Reproducibility and Replication Definitions",
    "text": "15.1 Reproducibility and Replication Definitions\nLet’s remind ourselves what Reproducibility and Replicability mean. As you see in Figure 15.1, Reproducibility involves literally reproducing the results from a study using original data, code, and materials. Replicability is about carrying out the study using the same procedures in different settings and achieving consistent, not identical, results.\n\n\n\nFigure 15.1: Reproducibility and Replicability."
  },
  {
    "objectID": "replication3.html#confidence-intervals",
    "href": "replication3.html#confidence-intervals",
    "title": "13  Some Statistical Preliminaries",
    "section": "13.4 Confidence Intervals",
    "text": "13.4 Confidence Intervals\nNote that when we commonly deal with estimates when running statistical tests. We commonly estimate a population parameter (such as the mean), which is a descriptive measure for an entire population (such as Residents of Philadelphia). Parameters can be many things. Two of the most common population parameters we try to estimate are the mean of some variable, denoted by the Greek letter Mu \\mu, and the Standard Deviation (a measure how spread out the data are) of some variable denoted by the Greek letter Sigma \\sigma.\nSince we typically cannot collect data from all members of a population, we have to take samples from the population, and calculate sample statistic which involve the same kinds of parameters as for the population, but just for the sample. Thus, a sample statistic is a characteristic of the sample. If you draw a number from the population, the sample’s characteristics will typically be similar to the population parameters. If you draw enough random samples, the sample statistics will converge on the population parameters, which is known as the Law of Large Numbers. Note that this only applies to the average of a variable converging to the Expected Value of the parameter in the population.\nOk, so why does all this matter? Well, the idea is that there is a degree of uncertainty associated with every estimate. The degree of uncertainty matters. If an estimate has a high degree of uncertainty, we might trust it less than an estimate with a low degree of uncertainty. How do we know how much uncertainty is associated with an estimate? Enter, the Confidence Interval, or a range of estimates for a parameter. The confidence level of the interval represents the proportion of confidence intervals in the long run that contain the true population parameter. For example, a confidence level of 95% (which is typical) suggests that if one were to construct many confidence intervals over and over again, 95% of them would contain the true population parameter.\n\n\n\n\n1. Harrell F. Statistical Thinking - A Litany of Problems With p-values. Published online February 2017. Accessed June 27, 2023. https://www.fharrell.com/post/pval-litany/\n\n\n2. Lakens D. The Practical Alternative to the p Value Is the Correctly Used p Value. Perspectives on Psychological Science. 2021;16(3):639-648. doi:10.1177/1745691620958012\n\n\n3. Wasserstein RL, Lazar NA. The ASA Statement on p-Values: Context, Process, and Purpose. The American Statistician. 2016;70(2):129-133. doi:10.1080/00031305.2016.1154108\n\n\n4. Wasserstein RL, Schirm AL, Lazar NA. Moving to a World Beyond “ p &lt; 0.05.” The American Statistician. 2019;73(sup1):1-19. doi:10.1080/00031305.2019.1583913\n\n\n5. Greenland S, Senn SJ, Rothman KJ, et al. Statistical tests, P values, confidence intervals, and power: A guide to misinterpretations. European Journal of Epidemiology. 2016;31(4):337-350. doi:10.1007/s10654-016-0149-3"
  },
  {
    "objectID": "replication4.html#stirrings",
    "href": "replication4.html#stirrings",
    "title": "10  Crises of Replication",
    "section": "10.2 Stirrings",
    "text": "10.2 Stirrings\nFor a long time, there was concern about the replicability and reproducibility of findings in the social sciences. In 2005, physician-scientist John Ioannidis published a provocative and worrying essay entitled Why Most Published Research Findings are False, which outlined several methodological shortcomings plaguing most published research using statistics, including an over-reliance on p-values, low statistical power, and biases in study design, data collection, and analysis.\n\n\n\nIoannidis laid out several factors why most published research is false.1\n\n\nHe noted that several biases in design, data analysis, and presentation factors influence the production of research findings. With greater bias in the conduct of a study, the lower the chances of the research finding being true. Additionally, the likelihood of a finding being true depends a great deal on the pre-study odds of it being true, as well as the statistical power of a study. He also points out that when a study is replicated by others in different contexts, the effect size is also likely to be smaller, and the likelihood of the finding being true is diminished.\nThis obviously made a lot of folks nervous, and become a highly downloaded paper. However, the concerns about replication did not truly become mainstream until one particular psychologist from Cornell published a study suggesting that humans have psychic ability…"
  },
  {
    "objectID": "replication4.html#the-dominoes-begin-to-fall",
    "href": "replication4.html#the-dominoes-begin-to-fall",
    "title": "10  Crises of Replication",
    "section": "10.3 The Dominoes Begin to Fall",
    "text": "10.3 The Dominoes Begin to Fall\nIn 2011, a study authored by Cornell psychologist Daryl Bem (a very respected psychologist) was published in the Journal for Personality and Social Psychology (a very respected psychology journal). The study appeared to shake the foundations of what we know about human beings. It suggested, through a series of nine experiments comprising more than 1,000 participants over ten years that humans have psychic ability!\n\n\n\nFormer Cornell professor of psychology Daryl Bem was very interested in studying the paranormal. Perhaps he wanted to find evidence for the paranormal a little too much.\n\n\nHere’s what happened. Bem administered College students came to a computer lab, and had to look at a computer screen. He administered many experiments to see if people have an innate psychic ability to predict the future. In one experiment, participants were presented with two curtains on a screen (represented in Figure Figure 10.2 below).\n\n\n\nFigure 10.2: Participants had to guess which one curtain concealed an image. After they made their guess, the image was randomly assigned to one curtain or another.\n\n\nThey had to then guess which curtain was concealing an image. Unbeknownst to them, the image would be randomly allocated to a curtain after they had made their choice. So, if they were able to guess which curtain held the image better than a 50-50 chance (for example, say they correctly guessed 60% of the time), this would be taken as evidence of an extra-sensory perception that allows one to Feel the Future (the admittedly catchy title of the study). Notably, some of the images were ‘erotic’ in nature, while others were neutral. In eight of nine studies reported in the paper, participants were significantly likely to predict which curtain contained the image. The findings in totality provided Bem ample evidence for the anomalous phenomenon of precognition, or psychic ability to see the future.\nThe study became HUGE. In general, if more than ten people read your research, I would call that a win. But Bem’s study was a darling of the media. The New York Times ran a front-page story on it, and Bem even appeared on the popular comedy program The Colbert Report. Those who were already convinced of the highly questionable field of Psi research (research on paranormal phenomena) were thrilled at Bem’s study. It seemed to provide strong evidence for psychic ability using widely accepted methods in a respected journal by a respected Ivy-league professor. Bem had large enough sample sizes to be accepted by the journal (he had been the Editor previously so he knew his methods were up to snuff), used basic one sample t-tests that one learns in introductory statistics classes, and made sure his stimuli had been randomized correctly. Many began to question whether humans had psychic ability based on Bem’s findings, and I don’t blame them at all. In the words of Slate author Daniel Engber, Bem’s findings were both methodologically sound and logically insane.2\nSo, do humans actually have psychic ability? No. At least, we don’t actually have any convincing evidence for that notion. Others tried to replicate Bem’s findings and did not find the same effects. In fact, one large replication with 3,289 participants across seven experiments found an effect size of \\(d = 0.04\\), which is statistically indistinguishable from zero. In another very thorough replication study involving 2,115 participants and ten labs from nine different countries, the researchers were unable to replicate Bem’s parapsychological effects, and found support for the notion that Bem’s statistically significant findings were pure bias in that significant findings were due to researcher and publications biases, as well as a lack of methodological rigor.3\n\n\n\n\n1. Ioannidis JPA. Why Most Published Research Findings Are False. PLOS Medicine. 2005;2(8):e124. doi:10.1371/journal.pmed.0020124\n\n\n2. Engber D. Daryl Bem Proved ESP Is Real. Slate. Published online June 2017. Accessed July 13, 2023. https://slate.com/health-and-science/2017/06/daryl-bem-proved-esp-is-real-showed-science-is-broken.html\n\n\n3. Kekecs Z, Palfi B, Szaszi B, et al. Raising the value of research studies in psychological science by increasing the credibility of research reports: The transparent Psi project. Royal Society Open Science. 2023;10(2):191375. doi:10.1098/rsos.191375"
  },
  {
    "objectID": "replication4.html#stirrings-of-a-crisis",
    "href": "replication4.html#stirrings-of-a-crisis",
    "title": "15  Crises of Replication",
    "section": "15.2 Stirrings of a Crisis",
    "text": "15.2 Stirrings of a Crisis\nFor a long time, there was concern about the replicability and reproducibility of findings in the social sciences. In 2005, physician-scientist John Ioannidis published a provocative and worrying essay entitled Why Most Published Research Findings are False, which outlined several methodological shortcomings plaguing most published research using statistics, including an over-reliance on p-values, low statistical power, and biases in study design, data collection, and analysis.\n\n\n\nIoannidis laid out several factors why most published research is false.1\n\n\nHe noted that several biases in design, data analysis, and presentation factors influence the production of research findings. With greater bias in the conduct of a study, the lower the chances of the research finding being true. Additionally, the likelihood of a finding being true depends a great deal on the pre-study odds of it being true, as well as the statistical power of a study. He also points out that when a study is replicated by others in different contexts, the effect size is also likely to be smaller, and the likelihood of the finding being true is diminished.\nThis obviously made a lot of folks nervous, and become a highly downloaded paper. However, the concerns about replication did not truly become mainstream until one particular psychologist from Cornell published a study suggesting that humans have psychic ability…"
  },
  {
    "objectID": "replication4.html#feeling-the-future---the-crisis-unfolds",
    "href": "replication4.html#feeling-the-future---the-crisis-unfolds",
    "title": "10  Crises of Replication",
    "section": "10.3 Feeling the Future - The Crisis Unfolds",
    "text": "10.3 Feeling the Future - The Crisis Unfolds\nIn 2011, a study authored by Cornell psychologist Daryl Bem (a very respected psychologist) was published in the Journal for Personality and Social Psychology (a very respected psychology journal). The study appeared to shake the foundations of what we know about human beings. It suggested, through a series of nine experiments comprising more than 1,000 participants over ten years that humans have psychic ability!\n\n\n\nFormer Cornell professor of psychology Daryl Bem was very interested in studying the paranormal. Perhaps he wanted to find evidence for the paranormal a little too much.\n\n\nHere’s what happened. Bem administered College students came to a computer lab, and had to look at a computer screen. He administered many experiments to see if people have an innate psychic ability to predict the future. In one experiment, participants were presented with two curtains on a screen (represented in Figure Figure 10.2 below).\n\n\n\nFigure 10.2: Participants had to guess which one curtain concealed an image. After they made their guess, the image was randomly assigned to one curtain or another.\n\n\nThey had to then guess which curtain was concealing an image. Unbeknownst to them, the image would be randomly allocated to a curtain after they had made their choice. So, if they were able to guess which curtain held the image better than a 50-50 chance (for example, say they correctly guessed 60% of the time), this would be taken as evidence of an extra-sensory perception that allows one to Feel the Future (the admittedly catchy title of the study). Notably, some of the images were ‘erotic’ in nature, while others were neutral. In eight of nine studies reported in the paper, participants were significantly likely to predict which curtain contained the image. The findings in totality provided Bem ample evidence for the anomalous phenomenon of precognition, or psychic ability to see the future.\nThe study became HUGE. In general, if more than ten people read your research, I would call that a win. But Bem’s study was a darling of the media. The New York Times ran a front-page story on it, and Bem even appeared on the popular comedy program The Colbert Report. Those who were already convinced of the highly questionable field of Psi research (research on paranormal phenomena) were thrilled at Bem’s study. It seemed to provide strong evidence for psychic ability using widely accepted methods in a respected journal by a respected Ivy-league professor. Bem had large enough sample sizes to be accepted by the journal (he had been the Editor previously so he knew his methods were up to snuff), used basic one sample t-tests that one learns in introductory statistics classes, and made sure his stimuli had been randomized correctly. Many began to question whether humans had psychic ability based on Bem’s findings, and I don’t blame them at all. In the words of Slate author Daniel Engber, Bem’s findings were both methodologically sound and logically insane.2\nSo, do humans actually have psychic ability? No. At least, we don’t actually have any convincing evidence for that notion. Others tried to replicate Bem’s findings and did not find the same effects. In fact, one large replication with 3,289 participants across seven experiments found an effect size of \\(d = 0.04\\), which is statistically indistinguishable from zero. In another very thorough replication study involving 2,115 participants and ten labs from nine different countries, the researchers were unable to replicate Bem’s parapsychological effects, and found support for the notion that Bem’s statistically significant findings were pure bias in that significant findings were due to researcher and publications biases, as well as a lack of methodological rigor.3\n\n\n\n\n\n\nBem Fires Back.\n\n\n\nI should note that Bem tried to dispute the findings of the skeptics with a meta-analysis of 90 studies from 33 labs across 14 countries suggesting that this findings are in fact real.4 However, given the myriad problems in the conduct and reporting of studies observed in the original study by Bem, I think it’s fair to say most academics in the field didn’t really buy these updated findings. Indeed, a breakdown of the statistics in the meta-analysis raises more questions than it answers.5. Given the replication work by non-Bem affiliated labs, and the fact that they are more recent, it’s safe to conclude that the original findings are not supported (however much some might want them to be).\n\n\nOk, so you may be wondering, “Well what did Bem do wrong?” He used widely accepted statistical methods, used large sample sizes, performed multiple experiments across years, and went through peer-review in a top-tier journal. It would seem he did everything right! The lesson is instructive, because you can do all of that, and still produce junk science. Bem fell prey to what we now call Researcher Degrees of Freedom- the flexibility inherent in the many decisions researchers have to make in data collection, analysis, and reporting that can result in selective practices producing spurious results.6 Engaging in researcher degrees of freedom increases the chance of false positives, or Type I errors, as well as biases in selectively reporting only findings that confirm one’s pre-existing conceptions. Here are some researcher degrees of freedom that very likely led Bem to his spurious findings:\n\nBem made many tweaks to his experiments over the years in order to yield statistically significant results. He did not document many of them. If you tweak your experiments without reporting that you did so, you make it seem like you just got the result magically, and not as a result of careful manipulation. This manipulation can results in false positives, but it also weakens your conceptual theory. For example, if a study did not show evidence of psychic ability, and Bem modified the experiment, and then DID find evidence of psychic ability, what does this say about his theory of psychic ability. In this way, notice how you can get very flexible with your theory if you’re making undocumented tweaks.\nThere were no pre-registered replications of experiments in Bem’s original study. While we’ll get to pre-registration a bit more in detail later, the idea is simple: 1) Before you do your study, say what your going to do, why, and how; 2) publish this pre-registration in the public domain; 3) do your study and report back on the things you had planned to do. Nowadays, this is a common rigorous practice, but was not really a thing in Bem’s time. This means that after doing one experiment, Bem was free to plan, adapt, abandon, and modify his hypotheses and experimental protocols in order to yield statistically significant results. Thus, if he ran a study, didn’t find want he wanted and then abandoned the study, this would not be recorded. Selective reporting means that you could run 100 studies, and only report and publish 10 of those that are statistically significant, severely distorting the evidence base and hiding your 90% null results rate.\n\n\n\n\n\n\n1. Ioannidis JPA. Why Most Published Research Findings Are False. PLOS Medicine. 2005;2(8):e124. doi:10.1371/journal.pmed.0020124\n\n\n2. Engber D. Daryl Bem Proved ESP Is Real. Slate. Published online June 2017. Accessed July 13, 2023. https://slate.com/health-and-science/2017/06/daryl-bem-proved-esp-is-real-showed-science-is-broken.html\n\n\n3. Kekecs Z, Palfi B, Szaszi B, et al. Raising the value of research studies in psychological science by increasing the credibility of research reports: The transparent Psi project. Royal Society Open Science. 2023;10(2):191375. doi:10.1098/rsos.191375\n\n\n4. Bem D, Tressoldi P, Rabeyron T, Duggan M. Feeling the future: A meta-analysis of 90 experiments on the anomalous anticipation of random future events. F1000Research. 2016;4:1188. doi:10.12688/f1000research.7177.2\n\n\n5. Lakens D. The 20% Statistician: A pre-publication peer-review of the ’Feeling The Future’ meta-analysis. The 20% Statistician. Published online May 2014. Accessed July 14, 2023. http://daniellakens.blogspot.com/2014/05/a-pre-publication-peer-review-of-meta.html\n\n\n6. Simmons JP, Nelson LD, Simonsohn U. False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant. Psychological Science. 2011;22(11):1359-1366. doi:10.1177/0956797611417632"
  },
  {
    "objectID": "replication4.html#feeling-the-future",
    "href": "replication4.html#feeling-the-future",
    "title": "15  Crises of Replication",
    "section": "15.3 Feeling the Future",
    "text": "15.3 Feeling the Future\n\n15.3.1 An Unintentional Crisis Unfolds\nIn 2011, a study authored by Cornell psychologist Daryl Bem (a very respected psychologist) was published in the Journal for Personality and Social Psychology (a very respected psychology journal). The study appeared to shake the foundations of what we know about human beings. It suggested, through a series of nine experiments comprising more than 1,000 participants over ten years that humans have psychic ability!\n\n\n\nFormer Cornell professor of psychology Daryl Bem was very interested in studying the paranormal. Perhaps he wanted to find evidence for the paranormal a little too much.\n\n\nHere’s what happened. Bem administered College students came to a computer lab, and had to look at a computer screen. He administered many experiments to see if people have an innate psychic ability to predict the future. In one experiment, participants were presented with two curtains on a screen (represented in Figure Figure 15.2 below).\n\n\n\nFigure 15.2: Participants had to guess which one curtain concealed an image. After they made their guess, the image was randomly assigned to one curtain or another.\n\n\nThey had to then guess which curtain was concealing an image. Unbeknownst to them, the image would be randomly allocated to a curtain after they had made their choice. So, if they were able to guess which curtain held the image better than a 50-50 chance (for example, say they correctly guessed 60% of the time), this would be taken as evidence of an extra-sensory perception that allows one to Feel the Future (the admittedly catchy title of the study). Notably, some of the images were ‘erotic’ in nature, while others were neutral. In eight of nine studies reported in the paper, participants were significantly likely to predict which curtain contained the image. The findings in totality provided Bem ample evidence for the anomalous phenomenon of precognition, or psychic ability to see the future.\nThe study became HUGE. In general, if more than ten people read your research, I would call that a win. But Bem’s study was a darling of the media. The New York Times ran a front-page story on it, and Bem even appeared on the popular comedy program The Colbert Report. Those who were already convinced of the highly questionable field of Psi research (research on paranormal phenomena) were thrilled at Bem’s study. It seemed to provide strong evidence for psychic ability using widely accepted methods in a respected journal by a respected Ivy-league professor. Bem had large enough sample sizes to be accepted by the journal (he had been the Editor previously so he knew his methods were up to snuff), used basic one sample t-tests that one learns in introductory statistics classes, and made sure his stimuli had been randomized correctly. Many began to question whether humans had psychic ability based on Bem’s findings, and I don’t blame them at all. In the words of Slate author Daniel Engber, Bem’s findings were both methodologically sound and logically insane.2\n\n\n\nBem’s ESP study riled up the field of social psychology…and beyond!\n\n\nSo, do humans actually have psychic ability? No. At least, we don’t actually have any convincing evidence for that notion. Others tried to replicate Bem’s findings and did not find the same effects. In fact, one large replication with 3,289 participants across seven experiments found an effect size of d = 0.04, which is statistically indistinguishable from zero. In another very thorough replication study involving 2,115 participants and ten labs from nine different countries, the researchers were unable to replicate Bem’s parapsychological effects, and found support for the notion that Bem’s statistically significant findings were pure bias in that significant findings were due to researcher and publications biases, as well as a lack of methodological rigor.3\n\n\n\n\n\n\nBem fires back.\n\n\n\nI should note that Bem tried to dispute the findings of the skeptics with a meta-analysis of 90 studies from 33 labs across 14 countries suggesting that this findings are in fact real.4 However, given the myriad problems in the conduct and reporting of studies observed in the original study by Bem, I think it’s fair to say most academics in the field didn’t really buy these updated findings. Indeed, a breakdown of the statistics in the meta-analysis raises more questions than it answers.5 Given the replication work by non-Bem affiliated labs, and the fact that they are more recent, it’s safe to conclude that the original findings are not supported (however much some might want them to be).\n\n\n\n\n15.3.2 Where Bem Went Wrong\nOk, so you may be wondering, “Well what did Bem do wrong?” He used widely accepted statistical methods, used large sample sizes, performed multiple experiments across years, and went through peer-review in a top-tier journal. It would seem he did everything right! The lesson is instructive, because you can do all of that, and still produce junk science. Bem fell prey to what we now call Researcher Degrees of Freedom - the flexibility inherent in the many decisions researchers have to make in data collection, analysis, and reporting that can result in selective practices producing spurious results.6 Engaging in researcher degrees of freedom increases the chance of false positives, or Type I errors, as well as biases in selectively reporting only findings that confirm one’s pre-existing conceptions. Here are just two researcher degrees of freedom that very likely led Bem to his spurious findings:\n\nBem made many tweaks to his experiments over the years in order to yield statistically significant results. He did not document many of them. If you tweak your experiments without reporting that you did so, you make it seem like you just got the result magically, and not as a result of careful manipulation. This manipulation can results in false positives, but it also weakens your conceptual theory. For example, if a study did not show evidence of psychic ability, and Bem modified the experiment, and then DID find evidence of psychic ability, what does this say about his theory of psychic ability. In this way, notice how you can get very flexible with your theory if you’re making undocumented tweaks.\nThere were no pre-registered replications of experiments in Bem’s original study. While we’ll get to pre-registration a bit more in detail later, the idea is simple: 1) Before you do your study, say what your going to do, why, and how; 2) publish this pre-registration in the public domain; 3) do your study and report back on the things you had planned to do. Nowadays, this is a common rigorous practice, but was not really a thing in Bem’s time. This means that after doing one experiment, Bem was free to plan, adapt, abandon, and modify his hypotheses and experimental protocols in order to yield statistically significant results. Thus, if he ran a study, didn’t find what he wanted and then abandoned the study, this would not be recorded. Selective reporting means that you could run 100 studies, and only report and publish 10 of those that are statistically significant, severely distorting the evidence base and hiding your 90% null results rate.\n\nBem’s own words in an interview with Slate magazine in 2017 are quite instructive:\n\n\n\n\n\n\nBem in his own words.\n\n\n\n“I would start one [experiment], and if it just wasn’t going anywhere, I would abandon it and restart it with changes,” Bem told me recently. Some of these changes were reported in the article; others weren’t. “I didn’t keep very close track of which ones I had discarded and which ones I hadn’t,” he said. Given that the studies spanned a decade, Bem can’t remember all the details of the early work. “I was probably very sloppy at the beginning,” he said. “I think probably some of the criticism could well be valid. I was never dishonest, but on the other hand, the critics were correct.”2"
  },
  {
    "objectID": "replication4.html#the-domino-effect",
    "href": "replication4.html#the-domino-effect",
    "title": "15  Crises of Replication",
    "section": "15.4 The Domino Effect",
    "text": "15.4 The Domino Effect\nAfter the publication of Bem’s ESP paper, many began questioning the foundations of the entire field of social psychology. Researchers began trying and failing to replicate many classic findings in psychology. The subfield of priming within social psychology was one of the first fields to undergo extensive critical scrutiny, with some dire results. Many priming studies failed to replicate, and it seemed that faulty methods, lack of statistical power, researcher biases, and publication biases were responsible.\n\n\n\n\n\n\nA Prime Candidate for Replication: The Elderly Priming Study.\n\n\n\nFor instance, a classic study in the field of subliminal priming led by Yale psychologist John Bargh involved one experiment in which undergraduates in a lab worked on a scrambled-sentence task. That is, they were given 30 sets of five word combinations which they had to use to construct a sentence. In the treatment group, the scrambled words contained words previously identified to be related to stereotypes of the elderly: Florida, old, lonely, grey, sentimental, wise, etc. (these are from the actual study). The hidden outcome of this study was the time it took for participants to walk down a corridor to leave the study area. The results showed that those exposed to the elderly prime condition walked almost one second slower than those who were exposed to neutral words (a statistically significant result).7\n\n\n\n\n\nSounds pretty cool right? That’s certainly what the authors of a psychology textbook I read in college thought when they presented the results of this study as fact. Unfortunately for Bargh and his colleagues, the unintentional crisis of replication set in motion by Bem led a team to try and replicate the classic elderly prime findings. The team followed the same protocol as Bargh and his colleagues, and found no difference in walking times between the elderly and neutral prime conditions.8\nNotably, the replication team used infrared sensors to automate the timing process, so it didn’t depend on a human’s use of a stopwatch. They speculated that the people manually timing the participants may have been a source of bias in the original study. They ran another experiment in which they used people to manually time participants with a stopwatch (I’ll call them the timers). They told the timers in one group that participants in the treatment group would walk faster, and they told the other half the participants in the treatment group would walk slower. Unbeknownst to the timers, infrared sensors were also measuring participants’ objective walking speeds. The results showed that when experimenters were led to believe participants would walk slower as a result of the intervention, the walking times were significantly higher in the Prime condition compared to the Neutral condition. Interestingly, when experimenters were led to believe that participants would walk faster as a result of the intervention, the walking times were significantly lower in the Prime condition compared to the Neutral condition. The results suggest that priming effects may reflect experimenter bias, rather than an actual induced effect of a prime stimulus on participants.\nI should note that some of Bargh’s other priming studies have also failed to replicate.9 For his part, Bargh dismissed the replication of his elderly prime study, and responded to the replicators with many objections to their replications as well as a scathing personal attack on them.10\n\n\nIt seems that 2011 was a watershed moment in the history of reproducibility and replication. A few key moments are worth mentioning. First, just about two months after The New York Times reported on Bem’s ESP study, a group of researchers submitted a study for publication entitled False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant.6\nThis paper demonstrated with real experimental data how easy it was to achieve statistical significance for an impossible hypothesis. In their study, the authors experimentally demonstrate that listening to the song When I’m Sixty-Four by the Beatles, compared to the song Kalimba which came free with Windows 7, actually MADE participants younger. Think about that for a second. Obviously, listening to any song might make you feel younger or older, but the outcome I mean here is actual chronological age measured by one’s birth date. Even though the premise that listening to the Beatles will make you younger is obviously false, the authors show, with standard statistical methods, that it is possible to achieve a statistically significant result supporting the impossible claim at p &lt; .05. There was no magic associated with this finding, it was achieved deliberately by engaging in researcher degrees of freedom such as using multiple dependent variables but reporting only the covariates that resulted in statistical significance, analyzing data before all data were collected, and not using a rule for when to stop collecting data.\n\n\n\nListening to the Beatles can make you younger! Not really, but with so much analytic flexibility, you can make an impossible hypothesis appear statistically significant.\n\n\nAs the field of priming research began to come into question, prominent psychologist Daniel Kahneman wrote an open letter to Bargh and others in the field of priming studies asking them to clean up their act. The entire letter can be found in Appendix G. The letter is prescient and instructive. Kahenman correctly predicting a train wreck looming which became the Replication Crisis, and his proposal to have a daisy chain setup of different labs replicating the same effect foreshadowed many large replication efforts to come.\nKahneman was not and is not a priming researcher, but he had some skin in the game vis-à-vis a popular book he published in 2011 entitled Thinking Fast and Slow which went on to become a New York Times bestseller. The book was a popular psychology piece in which Kahneman reviewed some of his major findings, as well as those of others, some of whom included priming researchers. As the replication crisis unfolded, it became apparent that many of the chapters referenced studies which later failed to replicate, his fourth chapter representing the chapter most riddled with references to spurious studies.11\nTo his credit, Kahneman provided a thoughtful response to a blog scrutinizing the shaky studies he cited in his book. Kahneman replied as a comment to the blog admitting that he placed too much faith in underpowered studies, despite having published a paper previously about how researchers are often reliant on underpowered studies.\n\n\n\nKahneman lauded a number of priming studies in his bestselling book that turned out to not replicate. To his credit, he admitted this publicly. Retraction Watch.\n\n\nThe case of Kahneman, a prominent researcher who knew well enough the dangers of relying on small sample sizes, falling prey to the very thing he had critiqued is illustrative of the importance of evidence-based rather than eminence-based practice. It also showed the extent of the problem. Even Nobel Prize winners could make mistakes in promoting weakly supported or spurious findings.\nThe replication crisis was not confined to priming studies, nor to the field of psychology alone. The crisis became evident in fields such as economics, cancer biology, finance, artificial intelligence, nutrition, and more. Indeed, it continues to this day, as more and more fields are critically scrutinizing key findings."
  },
  {
    "objectID": "replication4.html#gauging-the-extent-of-the-damage",
    "href": "replication4.html#gauging-the-extent-of-the-damage",
    "title": "15  Crises of Replication",
    "section": "15.5 Gauging the Extent of the Damage",
    "text": "15.5 Gauging the Extent of the Damage\nThe replication crisis has resulted in major efforts to replicate key findings across different fields. This process continues to unfold, but we can examine some salient examples.\n\n15.5.1 The Open Science Collaboration\nA huge replication study was convened by a group called the Open Science Collaboration involving 270 scientists from 17 countries, who selected 100 studies published in 2008 from top-tier psychology journals.12 Of the 100 studies, 97 had a finding that was statistically significant at the 5% level (p &lt; .05), and of the replications 35 of 97 studies had a finding at p &lt; .05. The replication study effects were also about half as big as the original study effects. The distribution of original study effect sizes and replicated study effect sizes in presented in Figure 15.3.\n\n\n\nFigure 15.3: The diagonal line represents the case where the original effect size = the replicated effect size. Red dots represent non-significant results (p &gt; .05), and green dots represent significant results (p &lt; .05). The dotted line represents zero effect, and points below represent replicated effects in the opposite direction of the original study.12\n\n\nThis was a massive effort that took four years to painstakingly gather data and replicate these experiments. The fact that about two-thirds of findings published in top-tier psychology journals did not replicate was concerning to many who took these findings as well-established. What did it mean for other findings? What did it mean for the entire field of psychology? Could other fields be affected?\n\n\n15.5.2 Questionable Research Practices in Psychology\nTo better understand the extent of Questionable Research Practices (QRPs; researcher degrees of freedom) in psychology, a group of researchers surveyed over 2,000 academic psychologists at major US universities.13 Respondents were asked about a) whether they had engaged in a number of QRPs (self-admission rate), b) the percentage of other psychologists they believed had engaged in the QRP (prevalence estimate), and c) the percentage of psychologists committing QRP who would admit to doing so. The main results are presented in Figure 15.4.\n\n\n\nFigure 15.4: For each QRP, the authors present the self-admission rate, the perceived prevalence of the QRP among other psychologists, and a prevalence estimate derived by the researchers taking the self-admission rate and dividing it by the estimate of how likely other psychologists would admit to engaging in QRPs.13\n\n\nAs we see, even with a select sample of psychologists, many admitted to engaging in QRPs. We can imagine that the self-admission rate represents a potential underestimate of the true extent of the problem.\n\n\n15.5.3 Many Labs 2\nImagine you replicate a study and find that the effects are different from the original. Immediately, the original authors may say that your protocol deviated from the original study, explaining the discrepancy in effect. To address this issue, a team carried out replications of 28 findings in psychology using protocols that were peer reviewed in advance.14 Each protocol was administered on about 15,305 participants from 36 countries and territories. So, this was a series of high-powered replications. Remember that statistical power relates to having the ability to detect an effect if it’s actually there.\nThe team found that just 15 of 28 findings replicated in the same direction as the original at p &lt; .05. Of these, 75% of the replicated findings were smaller than the original study effect. The authors concluded that the variation in effect sizes had more to do with the effect being studied than the sample or setting of the study.\n\n\n15.5.4 Replicating Social Science Experiments from Nature and Science\nMoving from the field of psychology to social science broadly, a team replicated 21 experimental studies published in the top journals Nature and Science between 2010 and 2015.15 By being published in these journals, one would assume that these experiments are rigorous and well conducted, as these journals are supposed to be at the forefront of science. The main results are presented in Figure 15.5.\n\n\n\nFigure 15.5: Standardized effect sizes presented such that 1 equals the original effect size. Yellow diamonds represent a zero effect, and green diamonds represent a statistically significant effect. Thirteen of 21 effects replicated in the same direction as the original study.15\n\n\nDespite being published in the BEST journals, eight of 21 studies did not replicate in the original direction, and the studies that did replicate were, on average, half the size of the original effect. Remember, getting published in these journals is very hard, and typically we expect robust effects to be found in their pages. The fact that a full eight of the 21 studies did not replicate, and that the replicated effects are much smaller than the original tells us that journal quality is not a failsafe for robust replicated findings. The authors conclude that both false positives, and inflated effect sizes of true positives contribute to imperfect reproducibility."
  },
  {
    "objectID": "app3.html#the-letter-in-full",
    "href": "app3.html#the-letter-in-full",
    "title": "Appendix F — Kahneman’s Open Letter to Priming Researchers",
    "section": "F.2 The Letter in Full",
    "text": "F.2 The Letter in Full\nFrom: Daniel Kahneman\nSent: Wednesday, September 26, 2012 9:32 AM\nSubject: A proposal to deal with questions about priming effects\nDear colleagues,\nI write this letter to a collection of people who were described to me (mostly by John Bargh) as students of social priming. There were names on the list that I could not match to an email. Please pass it on to anyone else you think might be relevant.\nAs all of you know, of course, questions have been raised about the robustness of priming results. The storm of doubts is fed by several sources, including the recent exposure of fraudulent researchers, general concerns with replicability that affect many disciplines, multiple reported failures to replicate salient results in the priming literature, and the growing belief in the existence of a pervasive file drawer problem that undermines two methodological pillars of your field: the preference for conceptual over literal replication and the use of meta-analysis. Objective observers will point out that the problem could well be more severe in your field than in other branches of experimental psychology, because every priming study involves the invention of a new experimental situation.\nFor all these reasons, right or wrong, your field is now the poster child for doubts about the integrity of psychological research. Your problem is not with the few people who have actively challenged the validity of some priming results. It is with the much larger population of colleagues who in the past accepted your surprising results as facts when they were published. These people have now attached a question mark to the field, and it is your responsibility to remove it.\nI am not a member of your community, and all I have personally at stake is that I recently wrote a book that emphasizes priming research as a new approach to the study of associative memory – the core of what dual-system theorists call System 1. Count me as a general believer. I also believe in a point that John Bargh made in his response to Cleeremans, that priming effects are subtle and that their design requires high-level skills. I am skeptical about replications by investigators new to priming research, who may not be attuned to the subtlety of the conditions under which priming effects are observed, or to the ease with which these effects can be undermined.\nMy reason for writing this letter is that I see a train wreck looming. I expect the first victims to be young people on the job market. Being associated with a controversial and suspicious field will put them at a severe disadvantage in the competition for positions. Because of the high visibility of the issue, you may already expect the coming crop of graduates to encounter problems. Another reason for writing is that I am old enough to remember two fields that went into a prolonged eclipse after similar outsider attacks on the replicability of findings: subliminal perception and dissonance reduction.\nI believe that you should collectively do something about this mess. To deal effectively with the doubts you should acknowledge their existence and confront them straight on, because a posture of defiant denial is self-defeating. Specifically, I believe that you should have an association, with a board that might include prominent social psychologists from other field. The first mission of the board would be to organize an effort to examine the replicability of priming results, following a protocol that avoids the questions that have been raised and guarantees credibility among colleagues outside the field.\nThe following is just an example of such a protocol:\n\nAssemble a group of five labs, where the leading investigators have an established reputation (tenure should perhaps be a requirement). Substantial labs with several students are the most desirable participants.\nEach lab selects a recent demonstration of a priming effect, which they consider robust and most likely to replicate.\nThe board makes a public commitment to these five specific effects\nSet up a daisy chain of labs A-B-C-D-E-A, where each lab will replicate the study selected by its neighbor: B replicates A, C replicates B etc.\nHave the replicating lab send someone to see how subjects are run (hence the emphasis on recency – the experiments should be in the active repertoire of the original lab, so that additional subjects can be run with confidence that the same procedure is followed).\nHave the replicated lab send someone to vet the procedure of the replicating lab as it starts its work\nRun enough subjects to guarantee power (probably more than in the original study)\nUse technology (e.g. video) to ensure that every detail of the method is documented and can be copied by others.\nPre-commit to publish the results, letting the chips fall where they may, and make all data available for analysis by others.\n\nThis is something you could do quickly, and relatively cheaply. The main costs are 10 trips, and funds to cover these costs would be easy to get (I have checked). You would have to be careful in selecting laboratories and results to maximize credibility, and every step of the procedure should be open and documented. The unusually high openness to scrutiny may be annoying and even offensive, but it is a small price to pay for the big prize of restored credibility.\nSuccess (say, replication of four of the five positive priming results) would immediately rehabilitate the field. Importantly, success would also provide an effective challenge to the adequacy of outsiders’ replications. A publicly announced and open effort would be credible among colleagues at large, because it would show that you are sufficiently confident in your results to take a risk.\nMore ambiguous results would be painful, of course, but they would still protect the reputations of scholars who sincerely believe in their work – even if they are sometimes wrong.\nThe protocol I outlined is just an example of something you might do. The main point of my letter is that you should do something, and that you must do it collectively. No single individual will be able to overcome the doubts, but if you act as a group and avoid defensiveness you will be credible.\nAll best,"
  },
  {
    "objectID": "app3.html#sec-appendixc",
    "href": "app3.html#sec-appendixc",
    "title": "Appendix C — Kahneman’s Open Letter to Priming Researchers",
    "section": "C.1 Background",
    "text": "C.1 Background\nThis letter has been archived by Nature, and can be currently found here. If the link stops working, please email me and I’ll fix it.\nAs mentioned in Chapter 10 , this letter was sent by psychologist Daniel Kahneman to psychologist John Bargh and others in the field of priming research. The letter was sent at a time where the entire field of not only priming, but psychology in general came under intense scrutiny in the form of a Replication Crisis, which since encouraged other fields to deal with their own crises of reproducibility, transparency, and replication. Today, the STEM fields as well are dealing with their own similar crises, particularly around fabrication or manipulation of images, which have become easier to detect with image recognition software and keen scientific integrity consultants like the formidable Dr Elisabeth Bik.\nThe letter sent by Kahneman is instructive and prescient. He correctly predicted the train wreck that would become the Replication Crisis, and while his daisy chain proposal may not have been realized among labs studying priming, it was reflected in similar large replication efforts such as Many Labs and Many Economists that continue to this day. Below, you will find the letter in full."
  },
  {
    "objectID": "app3.html",
    "href": "app3.html",
    "title": "Appendix C — Kahneman’s Open Letter to Priming Researchers",
    "section": "",
    "text": "C.0.1 Background\nThis letter has been archived by Nature, and can be currently found here. If the link stops working, please email me and I’ll fix it.\nAs mentioned in Chapter 10 , this letter was sent by psychologist Daniel Kahneman to psychologist John Bargh and others in the field of priming research. The letter was sent at a time where the entire field of not only priming, but psychology in general came under intense scrutiny in the form of a Replication Crisis, which since encouraged other fields to deal with their own crises of reproducibility, transparency, and replication. Today, the STEM fields as well are dealing with their own similar crises, particularly around fabrication or manipulation of images, which have become easier to detect with image recognition software and keen scientific integrity consultants like the formidable Dr Elisabeth Bik.\nThe letter sent by Kahneman is instructive and prescient. He correctly predicted the train wreck that would become the Replication Crisis, and while his daisy chain proposal may not have been realized among labs studying priming, it was reflected in similar large replication efforts such as Many Labs and Many Economists that continue to this day. Below, you will find the letter in full.\n\n\nC.0.2 The Letter in Full\nFrom: Daniel Kahneman\nSent: Wednesday, September 26, 2012 9:32 AM\nSubject: A proposal to deal with questions about priming effects\nDear colleagues,\nI write this letter to a collection of people who were described to me (mostly by John Bargh) as students of social priming. There were names on the list that I could not match to an email. Please pass it on to anyone else you think might be relevant.\nAs all of you know, of course, questions have been raised about the robustness of priming results. The storm of doubts is fed by several sources, including the recent exposure of fraudulent researchers, general concerns with replicability that affect many disciplines, multiple reported failures to replicate salient results in the priming literature, and the growing belief in the existence of a pervasive file drawer problem that undermines two methodological pillars of your field: the preference for conceptual over literal replication and the use of meta-analysis. Objective observers will point out that the problem could well be more severe in your field than in other branches of experimental psychology, because every priming study involves the invention of a new experimental situation.\nFor all these reasons, right or wrong, your field is now the poster child for doubts about the integrity of psychological research. Your problem is not with the few people who have actively challenged the validity of some priming results. It is with the much larger population of colleagues who in the past accepted your surprising results as facts when they were published. These people have now attached a question mark to the field, and it is your responsibility to remove it.\nI am not a member of your community, and all I have personally at stake is that I recently wrote a book that emphasizes priming research as a new approach to the study of associative memory – the core of what dual-system theorists call System 1. Count me as a general believer. I also believe in a point that John Bargh made in his response to Cleeremans, that priming effects are subtle and that their design requires high-level skills. I am skeptical about replications by investigators new to priming research, who may not be attuned to the subtlety of the conditions under which priming effects are observed, or to the ease with which these effects can be undermined.\nMy reason for writing this letter is that I see a train wreck looming. I expect the first victims to be young people on the job market. Being associated with a controversial and suspicious field will put them at a severe disadvantage in the competition for positions. Because of the high visibility of the issue, you may already expect the coming crop of graduates to encounter problems. Another reason for writing is that I am old enough to remember two fields that went into a prolonged eclipse after similar outsider attacks on the replicability of findings: subliminal perception and dissonance reduction.\nI believe that you should collectively do something about this mess. To deal effectively with the doubts you should acknowledge their existence and confront them straight on, because a posture of defiant denial is self-defeating. Specifically, I believe that you should have an association, with a board that might include prominent social psychologists from other field. The first mission of the board would be to organize an effort to examine the replicability of priming results, following a protocol that avoids the questions that have been raised and guarantees credibility among colleagues outside the field.\nThe following is just an example of such a protocol:\n\nAssemble a group of five labs, where the leading investigators have an established reputation (tenure should perhaps be a requirement). Substantial labs with several students are the most desirable participants.\nEach lab selects a recent demonstration of a priming effect, which they consider robust and most likely to replicate.\nThe board makes a public commitment to these five specific effects\nSet up a daisy chain of labs A-B-C-D-E-A, where each lab will replicate the study selected by its neighbor: B replicates A, C replicates B etc.\nHave the replicating lab send someone to see how subjects are run (hence the emphasis on recency – the experiments should be in the active repertoire of the original lab, so that additional subjects can be run with confidence that the same procedure is followed).\nHave the replicated lab send someone to vet the procedure of the replicating lab as it starts its work\nRun enough subjects to guarantee power (probably more than in the original study)\nUse technology (e.g. video) to ensure that every detail of the method is documented and can be copied by others.\nPre-commit to publish the results, letting the chips fall where they may, and make all data available for analysis by others.\n\nThis is something you could do quickly, and relatively cheaply. The main costs are 10 trips, and funds to cover these costs would be easy to get (I have checked). You would have to be careful in selecting laboratories and results to maximize credibility, and every step of the procedure should be open and documented. The unusually high openness to scrutiny may be annoying and even offensive, but it is a small price to pay for the big prize of restored credibility.\nSuccess (say, replication of four of the five positive priming results) would immediately rehabilitate the field. Importantly, success would also provide an effective challenge to the adequacy of outsiders’ replications. A publicly announced and open effort would be credible among colleagues at large, because it would show that you are sufficiently confident in your results to take a risk.\nMore ambiguous results would be painful, of course, but they would still protect the reputations of scholars who sincerely believe in their work – even if they are sometimes wrong.\nThe protocol I outlined is just an example of something you might do. The main point of my letter is that you should do something, and that you must do it collectively. No single individual will be able to overcome the doubts, but if you act as a group and avoid defensiveness you will be credible.\nAll best,"
  },
  {
    "objectID": "app3.html#background",
    "href": "app3.html#background",
    "title": "Appendix F — Kahneman’s Open Letter to Priming Researchers",
    "section": "F.1 Background",
    "text": "F.1 Background\nThis letter has been archived by Nature, and can be currently found here. If the link stops working, please email me and I’ll fix it.\nAs mentioned in Chapter 13 , this letter was sent by psychologist Daniel Kahneman to psychologist John Bargh and others in the field of priming research. The letter was sent at a time where the entire field of not only priming, but psychology in general came under intense scrutiny in the form of a Replication Crisis, which since encouraged other fields to deal with their own crises of reproducibility, transparency, and replication. Today, the STEM fields as well are dealing with their own similar crises, particularly around fabrication or manipulation of images, which have become easier to detect with image recognition software and keen scientific integrity consultants like the formidable Dr Elisabeth Bik.\nThe letter sent by Kahneman is instructive and prescient. He correctly predicted the train wreck that would become the Replication Crisis, and while his daisy chain proposal may not have been realized among labs studying priming, it was reflected in similar large replication efforts such as Many Labs and Many Economists that continue to this day. Below, you will find the letter in full."
  },
  {
    "objectID": "replication4.html#retractions",
    "href": "replication4.html#retractions",
    "title": "15  Crises of Replication",
    "section": "15.6 Retractions",
    "text": "15.6 Retractions\n\n15.6.1 What is it, and Why Does it Happen?\nA journal will retract a paper when it removes a paper from its records that it already published. Retractions do not occur over small issues like typos. It’s usually something quite seriously wrong about the paper or the peer review process. Usually, retractions occur due to major errors in the research, plagiarism, data falsification, or something else quite serious. Authors can self-retract if they later discover a serious error in their work, and that’s a very laudable thing. However, quite often retraction decisions are made by the editor or editorial board of the journal.\nLack of reproducibility is usually not grounds for retraction on its own. Consider the Bem ESP study. Since its publication, we now have a pretty decent understanding of Bem’s spurious results, including testimony from Bem himself in committing what we now call researcher degrees of freedom. However, when the editor of the Journal of Personality and Social Psychology was asked to retract the study in 2018, it took him two years to respond to the letter and inform the requester that the paper would not be retracted. Thus, it’s not a done deal even when you have a false hypothesis, evidence of bad research practice, and a lack of reproducibility.\nThat being said, as editors have become more aware of the many issues with research misconduct, and with the growth of plagiarism detection software, the number of retractions across all of science have risen many fold. In the year 2000, there were about 100 retractions annually across all of science; in 2014 that number grew to 1,000; and in 2022 it was up to about 3,600.16 17 The largest and most comprehensive database of retractions (and one of my favorite websites) is Retraction Watch. If you’re reading this, stop reading right now and click on the link and check out Retraction Watch (consider also making a tax-deductible donation if you like the vibe).\nSo, why are so many papers being retracted? Though the graphic in Figure 15.6 is a little older, it is still instructive.\n\n\n\nFigure 15.6: The majority of retractions between 1997 and 2015 have been due to fraud. These days, image fabrication is a particular problem in the STEM disciplines. Science.17\n\n\nPlagiarism from other papers or even from one’s own previously published papers is one of the main culprits of retraction. Importantly, fake peer review has also been a driving factor behind retractions. Fake peer review happens when an author gives a journal an email address ostensibly as the contact for a potential reviewer, but in reality they control the email address. This means that they can control their own peer review, which defeats the purpose of the whole exercise. Figure 15.7 is what it can look like like when a paper is retracted because the editor discovered that the peer review process had been manipulated.\n\n\n\nFigure 15.7: See the last sentence? This paper was retracted due to fake peer review.\n\n\n\n\n15.6.2 Life After Death: Continued Citations Despite Retraction\nBad papers are being retracted at an increasing pace. That’s good, right? Yes. But, there is another problem. Many papers continue to be cited even years after they’ve been retracted, and most of the citations don’t mention that the paper has been retracted. Let’s look at the top five of the most highly cited retracted papers from Retraction Watch’s database, shown in Table 15.1.18\n\n\nTable 15.1: I’ve put links to the original articles instead of citing them, for obvious reasons.\n\n\nArticle title, journal, and year\nYear of retraction\nCiting articles before retraction\nCiting articles after retraction\nTotal citations\n\n\n\n\n\nPrimary Prevention of Cardiovascular Disease with a Mediterranean Diet. New England Journal of Medicine; 2013.\n\n2018\n1,905\n950\n2,855\n\n\n\nIleal-lymphoid-nodular hyperplasia, non-specific colitis, and pervasive developmental disorder in children. Lancet; 1998.\n\n2010\n643\n940\n1,583\n\n\n\nVisfatin: A protein secreted by visceral fat that mimics the effects of insulin. Science; 2005.\n\n2007\n232\n1,232\n1,464\n\n\n\nAn enhanced transient expression system in plants based on suppression of gene silencing by the p19 protein of tomato bushy stunt virus. The Plant Journal; 2003.\n\n2015\n895\n421\n1,316\n\n\n\nLysyl oxidase is essential for hypoxia-induced metastasis. Nature; 2006.\n\n2020\n977\n105\n1,082\n\n\n\n\nDo you see how entries 2 and 3 have more citations after being retracted, than they did before retraction? That’s a problem! Additionally, entry 2 is fraudulent study claiming that vaccines cause autism. While this study has been thoroughly debunked, it has fueled the anti-vaccination movement, and its effects continue to be seen today. In this way, research misconduct does not just affect science, but can have massive downstream effects upon society, and the health and well-being of many.\nRetraction does appear to decrease citation frequency, but not by as much as we might like. One study examined compared the citation counts for 3,000 retracted papers to 3,000 non-retracted papers.19 They found that retraction decreased citation frequency by about 60%, and that many retracted papers continued to be cited.\n\n\n\n\n\n\nPerpetuation of Fraud: The Case of Scott Reuben\n\n\n\nOn February 21, 2010 American anesthesiologist Scott Reuben formally pled guilty to one count of health care fraud. He was sentenced to six months in prison, followed by three years of supervised release. He also had to pay a $5,000 fine, forfeit $50,000 to the government, and make restitution to pharmaceutical companies he had defrauded to the tune of $360,000.\nReuben was formerly a professor of anesthesiology and pain medicine at Tufts University. He admitted to having faking data underlying his research, and lied about conducting 21 clinical trials. The fake results from these trials were published in many journals. When he was outed as a fraud, his publications had been cited almost 1,200 times, and his work was quoted in clinical guidelines. An analysis of his published work in 2014 revealed that 45% of his retracted articles had been cited at least once, and of these, only a quarter correctly mentioned the work as being retracted. Thus, even five years after his articles were retracted, they were still being quoted and cited.20\nReuben was also able to slip past peer review and maintain fraudulent practices for 13 years, and wasted millions of dollars of funding. Consider also the impact his bogus findings had on the field of anesthesiology. At the time of the scandal, the editor-in-chief of the journal Anesthesia and Analgesia said of Reuben’s articles:\n\nWe are left with a large hole in our understanding of this field. There are substantial tendrils from this body of work that reach throughout the discipline of postoperative pain management. Those tendrils mean that almost every aspect will need to be carefully thought through. What do we still believe to be true? Do the conclusions hold up to scrutiny?21\n\n\n\nYou may be wondering, why do people cite retracted studies? It’s usually not to call them out as retracted. The reality is that most folks don’t care to check or don’t know that a study is retracted. Retraction notices on journals also vary widely, with some being more salient than others. Let’s have a look at some retraction notices.\nFirst up, we have the prestigious New England Journal of Medicine in Figure 15.8. This isn’t a great retraction notice because it’s just a narrow banner. These days, someone might mistake it for a pop-up about cookies, which has become ubiquitous. If someone downloads this in a hurry, I would worry that they might not see the notice.\n\n\n\nFigure 15.8: The retraction notice isn’t very salient. If someone is downloading this in a hurry, will they notice?\n\n\nNext, we’ve got Science in Figure 15.9. This one is bit more prominent, but could still be missed by someone in a hurry. We may also wonder, why not splash it against the title of the study?\n\n\n\nFigure 15.9: Better than NEJM’s retraction notice maybe? Still, could be better and more prominent.\n\n\nFinally, my favorite one so far is the retraction notice of the Lancet, shown in Figure 15.10. This is nice! It’s a huge notice splashed against the entire page. You cannot miss it. To me, this is the gold standard of retraction notices.\n\n\n\nFigure 15.10: Now THAT’S a retraction notice!\n\n\nDespite journal websites having retraction notices, some may still not be aware of the retractions because they might have downloaded the paper earlier pre-retraction. Thus, they might not need to ever visit the paper’s journal page, and may miss the retraction notice.\nThis is a big problem related to continued citation of retracted papers. One excellent solution to the problem exists IF you use Zotero for reference management. Zotero is a free reference management software, and now partners with Retraction Watch. This means that if a paper is in Retraction Watch’s database, it will show up as retracted in your Zotero. In Figure 15.11 you can see a screenshot of my Zotero, showing two papers that were retracted. The panel on the right explains why it was retracted, and a simple red cross gives me a quick indication that these papers have been retracted.\n\n\n\nFigure 15.11: A nice and easy notice of retraction. Thanks Zotero & Retraction Watch!\n\n\n\n\n\n\n1. Ioannidis JPA. Why Most Published Research Findings Are False. PLOS Medicine. 2005;2(8):e124. doi:10.1371/journal.pmed.0020124\n\n\n2. Engber D. Daryl Bem Proved ESP Is Real. Slate. Published online June 2017. Accessed July 13, 2023. https://slate.com/health-and-science/2017/06/daryl-bem-proved-esp-is-real-showed-science-is-broken.html\n\n\n3. Kekecs Z, Palfi B, Szaszi B, et al. Raising the value of research studies in psychological science by increasing the credibility of research reports: The transparent Psi project. Royal Society Open Science. 2023;10(2):191375. doi:10.1098/rsos.191375\n\n\n4. Bem D, Tressoldi P, Rabeyron T, Duggan M. Feeling the future: A meta-analysis of 90 experiments on the anomalous anticipation of random future events. F1000Research. 2016;4:1188. doi:10.12688/f1000research.7177.2\n\n\n5. Lakens D. The 20% Statistician: A pre-publication peer-review of the ’Feeling The Future’ meta-analysis. The 20% Statistician. Published online May 2014. Accessed July 14, 2023. http://daniellakens.blogspot.com/2014/05/a-pre-publication-peer-review-of-meta.html\n\n\n6. Simmons JP, Nelson LD, Simonsohn U. False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant. Psychological Science. 2011;22(11):1359-1366. doi:10.1177/0956797611417632\n\n\n7. Bargh JA, Chen M, Burrows L. Automaticity of social behavior: Direct effects of trait construct and stereotype activation on action. Journal of personality and social psychology. 1996;71(2):230.\n\n\n8. Doyen S, Klein O, Pichon CL, Cleeremans A. Behavioral Priming: It’s All in the Mind, but Whose Mind? PLOS ONE. 2012;7(1):e29081. doi:10.1371/journal.pone.0029081\n\n\n9. Schimmack U. Replicability Audit of John A. Bargh. Replicability-Index. Published online March 2019. Accessed July 14, 2023. https://replicationindex.com/2019/03/17/raudit-bargh/\n\n\n10. Yong E. A failed replication draws a scathing personal attack from a psychology professor. Science. Published online March 2012. Accessed July 14, 2023. https://www.nationalgeographic.com/science/article/failed-replication-bargh-psychology-study-doyen\n\n\n11. Schimmack U. A Meta-Scientific Perspective on “Thinking: Fast and Slow. Replicability-Index. Published online December 2020. Accessed July 18, 2023. https://replicationindex.com/2020/12/30/a-meta-scientific-perspective-on-thinking-fast-and-slow/\n\n\n12. Open Science Collaboration. Estimating the reproducibility of psychological science. Science. 2015;349(6251):aac4716. doi:10.1126/science.aac4716\n\n\n13. John LK, Loewenstein G, Prelec D. Measuring the Prevalence of Questionable Research Practices With Incentives for Truth Telling. Psychological Science. 2012;23(5):524-532. doi:10.1177/0956797611430953\n\n\n14. Klein RA, Vianello M, Hasselman F, et al. Many Labs 2: Investigating Variation in Replicability Across Samples and Settings. Advances in Methods and Practices in Psychological Science. 2018;1(4):443-490. doi:10.1177/2515245918810225\n\n\n15. Camerer CF, Dreber A, Holzmeister F, et al. Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015. Nature Human Behaviour. 2018;2(9):637-644. doi:10.1038/s41562-018-0399-z\n\n\n16. Oransky I. Retractions are increasing, but not enough. Nature. 2022;608(7921):9-9. doi:10.1038/d41586-022-02071-6\n\n\n17. Brainard J, You J. What a massive database of retracted papers reveals about science publishing’s “death penalty.” Science. 2018;25(1):1-5.\n\n\n18. Retraction Watch. Top 10 most highly cited retracted papers. Retraction Watch. Published online December 2015. Accessed July 19, 2023. https://retractionwatch.com/the-retraction-watch-leaderboard/top-10-most-highly-cited-retracted-papers/\n\n\n19. Kühberger A, Streit D, Scherndl T. Self-correction in science: The effect of retraction on the frequency of citations. PLOS ONE. 2022;17(12):e0277814. doi:10.1371/journal.pone.0277814\n\n\n20. Bornemann-Cimenti H, Szilagyi IS, Sandner-Kiesling A. Perpetuation of Retracted Publications Using the Example of the Scott S. Reuben Case: Incidences, Reasons and Possible Improvements. Science and Engineering Ethics. 2016;22(4):1063-1072. doi:10.1007/s11948-015-9680-y\n\n\n21. Gorski D. When fraud undermines science-based medicine. Science-Based Medicine. Published online March 2009. Accessed July 19, 2023. https://sciencebasedmedicine.org/when-fraud-undermines-science-based-medicine/"
  },
  {
    "objectID": "dataimport.html#importing-a-csv-or-excel-file-in-r",
    "href": "dataimport.html#importing-a-csv-or-excel-file-in-r",
    "title": "11  Importing and Cleaning Data",
    "section": "11.1 Importing a CSV or Excel file in R",
    "text": "11.1 Importing a CSV or Excel file in R\nPeople often store data in a spreadsheet. That makes a lot of sense since a spreadsheet has rows and columns, and that’s how most of our data in social science is going to be organized. It’s very easy to import spreadsheet data into R.\nTo begin, let’s be clear on the difference between Comma Separated Values (CSV) files and Microsoft’s proprietary Excel file formats (xls or xlsx). CSV files are a plain text format which separates a series of values with commas. Plain text means it cannot store formatting, macros, formulas, or other things that you might see in other file formats. You can open a CSV file in Microsoft Excel, but you cannot open an Excel file in a text editor. So, in general CSV files are probably the way to go in terms of storing data since they can be easily opened and manipulated, but Excel files are pretty great because they contain meta-data files, which can help detect instances of data tampering!\nLet’s go ahead and import the data using the read.csv() function, and assign the dataframe to an object called mydata1.\n\nmydata1 &lt;- read.csv(\"https://data.kingcounty.gov/api/views/yaai-7frk/rows.csv?accessType=DOWNLOAD\")\n\nLet’s check very quickly that the data look ok (i.e. there arent’s any weird anomalies that jump out) by using the View() function.\n\nView(mydata1)"
  },
  {
    "objectID": "dataimport.html#importing-a-csv-or-excel-file",
    "href": "dataimport.html#importing-a-csv-or-excel-file",
    "title": "6  Importing Data",
    "section": "6.1 Importing a CSV or Excel file",
    "text": "6.1 Importing a CSV or Excel file\nPeople often store data in a spreadsheet. That makes a lot of sense since a spreadsheet has rows and columns, and that’s how most of our data in social science is going to be organized. It’s very easy to import spreadsheet data into R.\nTo begin, let’s be clear on the difference between Comma Separated Values (CSV) files and Microsoft’s proprietary Excel file formats (xls or xlsx). CSV files are a plain text format which separates a series of values with commas. Plain text means it cannot store formatting, macros, formulas, or other things that you might see in other file formats. You can open a CSV file in Microsoft Excel, but you cannot open an Excel file in a text editor. So, in general CSV files are probably the way to go in terms of storing data since they can be easily opened and manipulated, but Excel files are pretty great because they contain meta-data files, which can help detect instances of data tampering!\nLet’s go ahead and import the data using the read.csv() function, and assign the dataframe to an object called mydata1.\n\n# This is how to import a CSV file from a URL\nmydata1 &lt;- read.csv(\"https://data.kingcounty.gov/api/views/yaai-7frk/rows.csv?accessType=DOWNLOAD\")\n\n# To import a CSV file that you have downloaded, just put the path in where the URL went above. \n# If you're on Windows like me, make sure the backslahes are doubled up like this \\\\. \n# You can also change them to single forward slashes /. \n\nmydata1 &lt;- read.csv(\"~/R Book/Datasets/Lost__found__adoptable_pets.csv\")\n\nLet’s start out by getting a sense of the scale of the dataframe using the dimensions dim() function. Then, let’s check very quickly that the data look ok (i.e. there arent’s any weird anomalies that jump out) by using the View() function.\n\ndim(mydata1)\n\n[1] 540  25\n\n\nUsing the dim() function, we can see that there are 528 rows and 25 columns in the dataframe. Each row corresponds to a unit of observation, and each column corresponds to a different variable. So, this dataframe has 528 units of observation, and 25 variables.\nCool, now let’s take a closer look at the data with the View() function.\n\nView(mydata1)\n\n\nOk, it looks like we have a variables corresponding to whether the dog is lost, found, or adoptable (Record_type), the specific ID of the animal (Animal_ID), it’s current location, and several others. It’s always a good idea to have a look at the data to see if anything looks amiss. We’ll get more into some specifics of data cleaning a bit later, but for now, it’s worthwhile just having a look.\nYou can import an Excel file easily into R using the readxl package. The corresponding read_xls() and read_xlsx() functions will help you import .xls (older Excel files) and .xlsx files, respectively. The main argument is the file path.\n\nlibrary(readxl)\n\n# Read a Microsoft Excel file.\n\ndf1 &lt;- read_xlsx(\"~/R Book/Datasets/Lost__found__adoptable_pets_excel.xlsx\")"
  },
  {
    "objectID": "dataimport.html#importing-a-text-file",
    "href": "dataimport.html#importing-a-text-file",
    "title": "6  Importing Data",
    "section": "6.2 Importing a text file",
    "text": "6.2 Importing a text file\nTo import a text file (a file with the extension .txt), we first need to know how data are separated. Usually, this is done by tabs (think of the ‘tab’ key on your keyboard). In such cases, the read.table() function is useful where the arguments should be the URL or file path, followed by the type of separator using the sep = argument. For tabs, it should be sep = '\\t'. For comma-separated data, it should be sep = ',' and for period-separated data, it should be sep = '.'. Since, this data frame is tab-separated (I know because I saved it as such), we will use the tab separator.\n\n# Read a tab-separated text file.\nmydata2 &lt;- read.table(\"~/R Book/Datasets/zoo.txt\", sep = '\\t')\n\ndim(mydata2)\n\n[1] 44 18\n\n\nUsing the dim() function, we can see that this dataframe has 44 units of observation and 18 variables. While we can always use the View() function to have a look at the data, let’s say that we’re in a real hurry and we just want to look at the first 10 rows of the data. Recall, that we can use head() function, and specify n = 10 to look at the first 10 rows.\n\nhead(mydata2, n = 10)\n\n            V1   V2       V3   V4   V5       V6      V7       V8      V9\n1  animal_name hair feathers eggs milk airborne aquatic predator toothed\n2       turtle    0        0    1    0        0       1        0       0\n3    chameleon    0        0    1    0        0       0        0       1\n4       iguana    0        0    1    0        0       0        1       1\n5       lizard    0        0    1    0        0       0        1       1\n6        gecko    0        0    1    0        0       0        0       1\n7       python    0        0    1    0        0       0        1       1\n8          boa    0        0    1    0        0       0        1       1\n9        adder    0        0    1    0        0       0        1       1\n10   crocodile    0        0    1    0        0       1        1       1\n        V10      V11      V12  V13  V14  V15      V16     V17        V18\n1  backbone breathes venomous fins legs tail domestic catsize class_type\n2         1        1        0    0    4    1        1       1          3\n3         1        1        0    0    4    1        1       0          3\n4         1        1        0    0    4    1        1       1          3\n5         1        1        0    0    4    1        0       0          3\n6         1        1        0    0    4    1        1       0          3\n7         1        1        1    0    0    1        0       1          3\n8         1        1        0    0    0    1        0       1          3\n9         1        1        1    0    0    1        0       1          3\n10        1        1        0    0    4    1        0       1          3\n\n\nUh oh, it looks like the first row of data is the name of the variables! That’s no good. The solution is simply to include the argument header = TRUE in the read.table() function. So let’s try that data import of a text file again with the new argument.\n\n# Read a tab-separated text file and keep the headers.\nmydata2 &lt;- read.table(\"~/R Book/Datasets/zoo.txt\", sep = '\\t', header = TRUE)\n\ndim(mydata2)\n\n[1] 43 18\n\n\nThe dim() function now shows me the dataframe has one less row than before (44 to 43). That’s good! Now, let’s use the head() function to check if the variable names are correctly excluded from our rows.\n\nhead(mydata2, n = 10)\n\n   animal_name hair feathers eggs milk airborne aquatic predator toothed\n1       turtle    0        0    1    0        0       1        0       0\n2    chameleon    0        0    1    0        0       0        0       1\n3       iguana    0        0    1    0        0       0        1       1\n4       lizard    0        0    1    0        0       0        1       1\n5        gecko    0        0    1    0        0       0        0       1\n6       python    0        0    1    0        0       0        1       1\n7          boa    0        0    1    0        0       0        1       1\n8        adder    0        0    1    0        0       0        1       1\n9    crocodile    0        0    1    0        0       1        1       1\n10   alligator    0        0    1    0        0       1        1       1\n   backbone breathes venomous fins legs tail domestic catsize class_type\n1         1        1        0    0    4    1        1       1          3\n2         1        1        0    0    4    1        1       0          3\n3         1        1        0    0    4    1        1       1          3\n4         1        1        0    0    4    1        0       0          3\n5         1        1        0    0    4    1        1       0          3\n6         1        1        1    0    0    1        0       1          3\n7         1        1        0    0    0    1        0       1          3\n8         1        1        1    0    0    1        0       1          3\n9         1        1        0    0    4    1        0       1          3\n10        1        1        0    0    4    1        0       1          3\n\n\nThat’s better! Always be mindful of the “header problem” wherein the headers get mistakenly included as observational units, rather than column labels."
  },
  {
    "objectID": "dataimport.html#importing-a-stata-sas-or-spss-file",
    "href": "dataimport.html#importing-a-stata-sas-or-spss-file",
    "title": "6  Importing Data",
    "section": "6.3 Importing a Stata, SAS, or SPSS file",
    "text": "6.3 Importing a Stata, SAS, or SPSS file\nRecall from Table 3.1 that Stata, SAS, and SPSS are different proprietary programs for statistical analysis. Stata is very commonly used by economists, and others in the social sciences. SAS is commonly used by healthcare and goverment agencies. SPSS is a commonly used by psychologists and other social scientists. Sometimes you might be working with someone who uses one of these software programs, or need to import a dataset from one of these programs.\nFirst, it’s important to know the file extension for each software program. Once you know the appropriate format, the haven package will allow us to import data from SAS, SPSS, or Stata into R. That’s pretty cool and easy! These can be found in Table 6.1\n\n\nTable 6.1: Data file extensions for Stata, SAS, or SPSS and the relevant haven package function.\n\n\nSoftware\nFile Extension\nRelevant haven package function\n\n\n\n\nSPSS\n.sav\nread_sav()\n\n\nSAS\n.sas7bdat or .xpt\nread_sas() or read_xpt()\n\n\nStata\n.dta\nread_dta()\n\n\n\n\nLet’s give each of these a shot with the relevant data format. Remember to install the haven package if you haven’t done that already with install.packages(\"haven\") and then load the package using library(haven).\n\nlibrary(haven)\n\n# Read a sas7bdat file.\nmydata3 &lt;- read_sas(\"~/R Book/Datasets/naws_all.sas7bdat\")\n\nWhen a SAS data file is exported, you may see the file extension .xpt. As seen in Table 6.1, this is not a problem for the formidable haven package. Let’s import an XPT file now.\n\nlibrary(haven)\n\n# Read a SAS xpt file.\nmydata4 &lt;- read_xpt(\"~/R Book/Datasets/P_DEMO.xpt\")\n\ndim(mydata4)\n\n[1] 15560    29\n\n\nWe can see that the data frame has 15,560 observational units and 29 columns or variables. Let’s have a look at the last 10 rows of data and first 6 columns.\n\ntail(mydata4[, c(1:6)], n = 10)\n\n# A tibble: 10 × 6\n     SEQN SDDSRVYR RIDSTATR RIAGENDR RIDAGEYR RIDAGEMN\n    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 124813       66        2        2       43       NA\n 2 124814       66        2        1       64       NA\n 3 124815       66        2        1       52       NA\n 4 124816       66        2        1        1       15\n 5 124817       66        2        2       67       NA\n 6 124818       66        2        1       40       NA\n 7 124819       66        2        1        2       NA\n 8 124820       66        2        2        7       NA\n 9 124821       66        2        1       63       NA\n10 124822       66        2        1       74       NA\n\n\nWe can see a number of variables with values, and things seem to have imported nicely. We can’t know this for sure until we’ve done some more careful examination and cleaning of the data, but the fact that nothing looks terribly out of place at first glance is a good sign.\nYou will also notice that when we examined the first and last 10 rows and first six columns, R gave us a tibble of dimensions 10 * 6. A tibble is basically a well-formatted and easy to understand summary of the data in R. Tibbles print data out nicely so they’re easy to glance at.\nNow you know how to import a number of common data types into R! So, why not…\n\n\n\nManx cat has public use dataset suggestions for you if you just click on him. Credit to the University of Missouri Libraries."
  },
  {
    "objectID": "cleaning.html",
    "href": "cleaning.html",
    "title": "12  Data Cleaning",
    "section": "",
    "text": "What does it mean to clean data? Raw data, or the data you obtain directly from measurement, and has yet to be processed and made digestible for human consumption. This is because raw data can missing data, contain errors, weird characters or symbols, personally identifiable information (information that can help identify an individual and break anonymity), or other things that you’d rather not have in there. Therefore, raw data needs to be cleaned (or cleansed or scrubbed) to proceed to data analysis, inference, visualization, and reporting.\nThere is no standard definition of data cleaning, but in general it means that you want to fix any issues in your data before you do anything else. This makes sense right? You don’t want to analyze data which has a bunch of errors in it, as that could bias your results, create a misleading interpretation, and ultimately introduce noise and distortion into the scientific literature. When it comes data analysis, remember the acronym GIGO, which is explained in Figure 12.1.\n\n\n\nFigure 12.1: Your analyses are only as good as the quality of your data."
  },
  {
    "objectID": "cleaning.html#data-cleaning-issues",
    "href": "cleaning.html#data-cleaning-issues",
    "title": "9  Data Cleaning",
    "section": "9.1 Data Cleaning Issues",
    "text": "9.1 Data Cleaning Issues\n\n9.1.1 Basic Folder Structure\nThere are several potential errors you can detect while looking at raw data. The first thing you should do is separate out raw data from the dataset you will be cleaning, and eventually produced a clean dataset. Leave the raw data in an encrypted folder or other secure location, and work with just the cleaned data (or data that will be cleaned). This will prevent you from overwriting the original dataset and making any errors in data cleaning permanent.\n\nOnce this separation of raw and clean (or to be cleaned) data is established, you can begin working on cleaning.\n\n\n9.1.2 Illogical Values & Typos\nOne of the first things I always do with a raw data set is to check if there are any illogical values. These are values that fall outside of the acceptable set of response options for a particular variable. For example, if a question is asking about a participant’s age in years, a value of 366 would be an illogical value since humans cannot yet live that long. However, a value of 366 for a variable about age in months makes perfect sense, as it translates to an age of about 30 years, which is a logical value. As another example, if a survey of undergraduate students conducted at Boston University asks them to state their class year (e.g. first year, freshman, second year, sophomore, etc), and you see some entries with the value “Boston University”, this is an illogical value. It should either be modified to the correct class year, or deleted and left as missing data.\nCommonly, typos can create illogical values. These should be fairly obvious if you a codebook, which is a document providing the name, description, and meaning of each variable, including how it is coded and what response options are valid. These could be incorrect or illogical entries, but could also be logically but with incorrect punctuation. For example, entries for a variable about one’s favorite color may contain entries red, RED, Red, or rED. These are all logical values, but they differ in which letters are capitalized, potentially leading to issues in the analysis. Thus, they should all be standardized. I would change them all to red. There is no special reason for this particular format, other than consistency, but I have found that lowercase variable names introduce less potential for errors due to capitalization.\n\n\n9.1.3 Duplicate Values & Outliers\nDifferent participants can have the same response to particular questions or variables. However, if different participants have EXACTLY the same values across ALL variables, then you more than likely have a case of a duplicate value, which should be removed. Duplicate values are key sources of bias, and can inflate or mask detectable effects, leading to misleading interpretations. Sometimes duplicate values can arise as errors when multiple data sets are joined together. A simple approach to avoiding this problem is to count the rows and columns in each dataset before they are joined together, and then ensure the total rows and columns after joining corresponds to the sum of rows and columns from each individual dataset.\nOutliers are values that are abnormally far away from other values in your dataset. Imagine you take the average net worth of five people sitting at a bar. Even though the average net worth of the first four people is $64,501, the average net worth of all five people at the bar comes to $ 58 billion! How is that possible? Because the fifth person at the bar is Bill Gates, whose 2023 net worth was $117.5 billion. Gates’ net worth value is an outlier because 117.5 billion is VERY VERY far distant from $64,501.\nFor right now, the important thing to note is that you should NOT remove outliers just because they are outliers. Outliers should only be removed if they represent true data entry/collection errors. For example, if an adult’s weight is entered as 12 pounds, this is likely a data entry error, but if their weight is listed as 432 pounds, this is a legitimately possible value, and if true, should not be removed. When outliers are legitimate extreme values, they represent a natural section of the population which you are studying. They are providing you information that should not be discarded simply to fit models easier, as this is a type of cherry-picking of the data which is . Rather, one should use statistical tests that are robust to outliers in such cases.\nSome people, such as professor of data science Pasquale Cirillo make a nice distinction between outliers (values that are not possible because they are too extreme in magnitude) and extremes (values that are possible and extreme in magnitude), show in Figure 9.2.\n\n\n\nFigure 9.2: The bounds for a legitimate value depend on the variable, and should guide your choice to delete or retain the value.\n\n\n\n\n9.1.4 Detecting Illogical or Extreme Values\nOne simple method of determining whether there exist illogical values in a numeric variable is to look at the maximum and minimum values (ensuring that they correspond to logical values for that variable) using the range() function. Similarly, boxplots boxplot() and histograms hist() are also useful. Let’s look at the built-in mtcars data frame.\n\n# Assign the mtcars dataframe to an object named df1\ndf1 &lt;- mtcars\n\n# I add a few outliers to the 'mpg' column\ndf1[c(2,5,7,9:11,22), 1] &lt;- 500\n\n# Let's examine the range of values in the 'mpg' column starting with the range.\nrange(df1$mpg)\n\n[1]  10.4 500.0\n\n\nImmediately, I see that the minimum value of 10 miles per gallon appears logical (think bigger SUVs which are less fuel-efficient than smaller sedan cars). However, the maximum value of 500 miles per gallon is not logical at all. How do I know? Well, for one I put in these entries. But if I didn’t know, a quick Google search tells me that the most fuel efficient gasoline car achieves 42 miles per gallon on the highway, and the most fuel efficient electric car achieves an 84 miles per gallon-equivalent. Thus, 500 miles per gallon is more of a distant dream rather than an accurate value.\nLet’s then visualize the variable with a histogram and a boxplot.\n\n# Next, let's look at a histogram of values\nhist(df1$mpg)\n\n\n\n# Next, let's look at a boxplot of the mpg variable.\nboxplot(df1$mpg)\n\n\n\n\nThe histogram shows us that the majority of values are less than 100, and a small subset of values lie around 500. This suggests that 500 might be an outlier value. The boxplot suggests that same, but provides us the Interquartile Range (IQR), which refers to the 25th percentile to 75th percentile of the data. The IQR is used to build box plots, which also display ‘whiskers’ or lines extending above and below the box, corresponding to 1.5 times the 75th percentile and 1.5 times the 25th percentile. If data points are outside the whiskers, these can be outliers. The boxplot above is super squished, and we can see a dot at 500. This indicates that a value of 500 might be an outlier.\nThe rstatix package also has some useful functions for detecting outliers. The identify_outliers() function in particular returns you a data frame with two new columns indicating whether a specific value is an outlier (defined in this package as 1.5 times the IQR) or an extreme value (defined in this package is 1.5 times the IQR).\n\nlibrary(rstatix)\nlibrary(dplyr)\n\ndf2 &lt;- df1 %&gt;%\n  identify_outliers(mpg)\n\n\nThe resulting data frame shows the seven unique values that are 1.5 and 3 times the IQR (see the two columns added at the end). For our purposes, this helps identify the specific outliers or extreme values. In this case, we have already determined that 500 is an illogical value for the fuel efficiency (miles per gallon) of cars. Thus, we can delete the values and replace them with NA to indicate missing values. To accomplish this, let’s use the which() function and %in% operator to identify the rows with a value of 500 for mpg from our original dataset. The which() function gives us the position or index which satisfies a given condition. Recall that the %in% operator checks whether the values in the first argument are present in the second argument, and returns a logical value. When we combine which() with the %in% operator, and add those into the first argument in square brackets after a dataframe, this will give the specific rows that satisfy a particular condition.\n\n# Identify rows with mpg == 500\ndf1[which(df1$mpg %in% 500),]\n\n                  mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4 Wag     500   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nHornet Sportabout 500   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nDuster 360        500   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 230          500   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280          500   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C         500   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nDodge Challenger  500   8 318.0 150 2.76 3.520 16.87  0  0    3    2\n\n# Replace 500 with NA\ndf1[which(df1$mpg %in% 500), 1] &lt;- NA\n\nIf we view the resulting dataframe, we can see that the mpg variable for the rows in question have all been replaced with NA to indicate missing data (indicated with red squares).\n\n\n\n9.1.5 Variable Subsetting, Naming, Transformation & Creation\nAnother key component of the data cleaning process is to prepare your set of variables for analysis. In R, since it is very easy to work with multiple dataframes at the same time (just assign them to different objects), it’s a good idea to create a dataframe with only the variables you need for the analysis. This is not strictly necessary, but keeps this organized, easy to view, and manageable. If you only need 20 variables for an analysis, why work with 175 variables in a data frame? It’s better to keep what you need and save those to a new dataframe.\nThis is easily accomplished by subsetting the dataframe based on one or more conditions. This could be a particular variable value, a particular set of rows, or some other condition. Let’s demonstrate this using data from the CrossFit Games from 2007-2023.\n\n# First let's import our CSV dataframe, and see the dimensions.\ncrossfit &lt;- read.csv(\"~/R Book/Datasets/crossfit.csv\", header = TRUE)\ndim(crossfit)\n\n[1] 1714   30\n\n# Next, let's have a quick peek at the first 10 rows and first six columns.\nhead(crossfit[, c(1:6)], n = 10)\n\n   competitorId    competitorName   firstName lastName gender genderId\n1          1616       Russ Greene        Russ   Greene      M        1\n2          1616       Russ Greene        Russ   Greene      M        1\n3          1685 Christopher Woods Christopher    Woods      M        1\n4          1690      Travis Mayer      Travis    Mayer      M        1\n5          1690      Travis Mayer      Travis    Mayer      M        1\n6          1690      Travis Mayer      Travis    Mayer      M        1\n7          1690      Travis Mayer      Travis    Mayer      M        1\n8          1690      Travis Mayer      Travis    Mayer      M        1\n9          1690      Travis Mayer      Travis    Mayer      M        1\n10         1690      Travis Mayer      Travis    Mayer      M        1\n\n\nLet’s say I’ve also had a more detailed look at the data, either beforehand using a codebook, or after importing the data into R and using the View() function. I’ve decided to conduct a descriptive analysis by examining the competitors with the most competition starts, the highest ranks, as well as some of their demographic characteristics. I decide to analyze the following variables shown in Table 9.1.\n\n\nTable 9.1: List of variables I will retain for analysis.\n\n\n\n\n\n\n\nVariable Name\nDescription\nVariable Type\n\n\n\n\ncompetitorId\nCompetitor’s unique identification number.\nFactor\n\n\ncompetitorName\nCompetitor’s full name.\nFactor\n\n\ngender\nCompetitor’s gender (male or female only in this dataset).\nFactor\n\n\nage\nCompetitor’s age at the time of competition.\nNumeric\n\n\nheight\nCompetitor’s height in centimeters.\nNumeric\n\n\nweight\nCompetitor’s weight in kilograms.\nNumeric\n\n\ncountryOfOriginName\nCompetitor’s country of origin\nFactor\n\n\nyear\nYear that competitor participated in competition.\nFactor\n\n\noverallScore\nCompetitor’s overall CrossFit score.\nNumeric\n\n\noverallRank\nCompetitor’s overall CrossFit rank.\nNumeric\n\n\n\n\nYou will notice that I put the type of variable in a column as well. This is not necessarily the variable type currently in the dataframe, but rather, the type of variable I want the variable to be before I proceed to any sort of analysis. If the variable is not in the correct class I want, I will transform it to the correct class.\nTo subset these variables, I can use the helpful subset() command from base R, where the first argument is the object to be subsetted, and the second argument can be used to select the variables I want to retain.\n\nlibrary(dplyr)\n\n# Keep only the variables I want using subset().\ncrossfit2 &lt;- subset(crossfit, select = c(\"competitorId\",\n                                         \"competitorName\",\n                                         \"gender\",\n                                         \"age\",\n                                         \"height\",\n                                         \"weight\",\n                                         \"countryOfOriginName\",\n                                         \"year\",\n                                         \"overallScore\",\n                                         \"overallRank\")\n                    )\n\nhead(crossfit2[, 1:7], n = 10)\n\n   competitorId    competitorName gender age height weight countryOfOriginName\n1          1616       Russ Greene      M  20    178     83                    \n2          1616       Russ Greene      M  21    178     83                    \n3          1685 Christopher Woods      M  29    163     82                    \n4          1690      Travis Mayer      M  23    181     93       United States\n5          1690      Travis Mayer      M  25    181     93       United States\n6          1690      Travis Mayer      M  26    181     93       United States\n7          1690      Travis Mayer      M  28    181     93       United States\n8          1690      Travis Mayer      M  29    181     93       United States\n9          1690      Travis Mayer      M  30    181     93       United States\n10         1690      Travis Mayer      M  31    181     93       United States\n\n\nAlright, that’s looking pretty good. I’ve got all the variables I want saved in a new dataframe called crossfit2. However, I’m not really digging some of the variable names. Some of them like countryOfOriginName got some weird capitalization going on, which can increase the likelihood of an error later on. Let’s make simpler names for all variables. Remember, any final variable names should be reflected in an updated codebook. This applies for any dataframe you work with, because reproducibility involves careful documentation of variable transformations and choices.\n\n# First, let's look at all variable names in our smaller dataframe.\nnames(crossfit2)\n\n [1] \"competitorId\"        \"competitorName\"      \"gender\"             \n [4] \"age\"                 \"height\"              \"weight\"             \n [7] \"countryOfOriginName\" \"year\"                \"overallScore\"       \n[10] \"overallRank\"        \n\n# Next, since I want to make changes to most of the names, let's create a new vector of names and replace the old ones.\n\nnewnames &lt;- c(\"id\",\n              \"name\",\n              \"gender\",\n              \"age\",\n              \"height\",\n              \"weight\",\n              \"country\",\n              \"year\",\n              \"score\",\n              \"rank\")\n\nnames(crossfit2) &lt;- newnames\n\n# Let's check that it worked correctly.\nnames(crossfit2)\n\n [1] \"id\"      \"name\"    \"gender\"  \"age\"     \"height\"  \"weight\"  \"country\"\n [8] \"year\"    \"score\"   \"rank\"   \n\n# If we wanted to change a few variable names instead of writing out a whole vector, we can use the rename() function from the dplyr package. In this function the first argument is the new name, then an equals sign, and finally the old name.\n\ncrossfit2 &lt;- crossfit2 %&gt;%\n  rename(score_overall = score) %&gt;%\n  rename(rank_overall = rank)\n\nnames(crossfit2)\n\n [1] \"id\"            \"name\"          \"gender\"        \"age\"          \n [5] \"height\"        \"weight\"        \"country\"       \"year\"         \n [9] \"score_overall\" \"rank_overall\""
  },
  {
    "objectID": "Operations.html#exercises",
    "href": "Operations.html#exercises",
    "title": "7  Strings, Booleans & Operators",
    "section": "7.1 Exercises",
    "text": "7.1 Exercises\nAs always, it’s a good idea to attempt these while the material is still fresh. You can find the answers in Appendix D.\n\nCreate a variable called MarySue1 with the value \"Dr Mary Sue Coleman, former president of the University of Michigan once said\". Then, create another variable called MarySue2 with the value \"For today, goodbye. For tomorrow, good luck. And Forever, Go Blue!\". Then find the number of characters in each variable using the nchar() function. Then, check if the letter ‘r’ is present in each variable by using the grepl() function, in which the first argument is thing you want to find (i.e. 'r'), and the second argument is the variable name (i.e. MarySue1 and MarySue2).\nCreate a variable called MarySue3 whose value is a concatenation (combination) of MarySue1 and MarySue2 using the paste() function, in which the arguments are the variables separated by a comma. Then, print the value for MarySue3."
  },
  {
    "objectID": "app4.html",
    "href": "app4.html",
    "title": "Appendix D — Answers for Section 5.3",
    "section": "",
    "text": "Create a variable called MarySue1 with the value \"Dr Mary Sue Coleman, former president of the University of Michigan once said\". Then, create another variable called MarySue2 with the value \"For today, goodbye. For tomorrow, good luck. And Forever, Go Blue!\". Then find the number of characters in each variable using the nchar() function. Then, check if the letter ‘r’ is present in each variable, and report the results.\n\n\nMarySue1 &lt;- \"Dr Mary Sue Coleman, former president of the University of Michigan once said\"\n\nMarySue2 &lt;- \"For today, goodbye. For tomorrow, good luck. And Forever, Go Blue!\"\n\nnchar(MarySue1)\n\n[1] 77\n\nnchar(MarySue2)\n\n[1] 66\n\n# There are 77 characters in MarySue1 and 66 characters in MarySue2.\n\n\ngrepl('r', MarySue1)\n\n[1] TRUE\n\ngrepl('r', MarySue2)\n\n[1] TRUE\n\n# The letter r is present in both variables MarySue1 and MarySue2.\n\n\nCreate a variable called MarySue3 whose value is a concatenation (combination) of MarySue1 and MarySue2 using the paste() function, in which the arguments are the variables separated by a comma. Then, print the value for MarySue3.\n\n\nMarySue3 &lt;- paste(MarySue1, MarySue2)\nprint(MarySue3)\n\n[1] \"Dr Mary Sue Coleman, former president of the University of Michigan once said For today, goodbye. For tomorrow, good luck. And Forever, Go Blue!\"\n\n\n\nCreate a string vector called basho and assign it the value \"An old silent pond. A frog jumps into the pond—Splash! Silence again.\" Then create and print another variable called basho2 in which the word ‘frog’ has been replaced by ‘buffalo’, and the word ‘Splash!’ has been replaced by ‘Yikes!’.\n\n\nbasho &lt;- \"An old silent pond. A frog jumps into the pond—Splash! Silence again.\"\n\nbasho2 &lt;- basho\n\ngsub('frog', 'buffalo', basho2)\n\n[1] \"An old silent pond. A buffalo jumps into the pond—Splash! Silence again.\"\n\ngsub('Splash!', 'Yikes!', basho2)\n\n[1] \"An old silent pond. A frog jumps into the pond—Yikes! Silence again.\"\n\nprint(basho2)\n\n[1] \"An old silent pond. A frog jumps into the pond—Splash! Silence again.\"\n\n\n\nLet’s do a variation of Mad Libs I will call Mad Sentences. Install and load the keyToEnglish package (be mindful of the capitalization in this package’s name). Then create three variables named after your three favorite cuisines. For each variable, assign the value generate_random_sentences(n = 2, punctuate = TRUE) to generate two random sentences per variable. This will produce a total of six sentences (two per variable). Finally, create a variable called madsentences whose value combines (pastes) all three variables. Print madsentences. If it sounds nonsensical, then it worked!\n\n\nlibrary(keyToEnglish)\n\nindian &lt;- generate_random_sentences(n = 2, punctuate = TRUE)\nchinese &lt;- generate_random_sentences(n = 2, punctuate = TRUE)\nkorean &lt;- generate_random_sentences(n = 2, punctuate = TRUE)\n\nmadsentence &lt;- paste(indian, chinese, korean)\nprint(madsentence)\n\n[1] \"Disinherited reflective quasar sues artificial maniacs. Illustrative turquoise grape evaporates sleek shades. Confused foamy sequoia accuses rusty ticks.\"\n[2] \"Aeronautic long salmon vexes salty shawls. Misunderstood velvet laser engineers kaleidoscopic tachyons. Quirky brass hen eats spongy candles.\"            \n\n\n\nLet’s compare the returns from simple vs compound interest after five years. First, define p as 1000, r as .07 and t as 5. Then Create a variable called simple with the value p * r * t. Next, create a variable compound with the value p * (1 + r)^t - p. Then, perform a logical test to see if simple is equal to compound, and write out the results of the test in one sentence.\n\n\np &lt;- 1000\nr &lt;- 7 / 100\nt &lt;- 5\n\nsimple &lt;- p * r * t\n\ncompound &lt;- p * (1 + r)^t - p\n\nsimple == compound\n\n[1] FALSE\n\n# Simple interest is not equal to compound interest in five years at a principal of 1000 (dollars) and a rate of seven percent.\n\n\nRetain the variables you created above and write a series of conditional (If else/Else If) statements according to the following rules: 1) If simple is less than compound, print the statement “Simple interest is less than compound interest.”; 2) If simple is greater compound, print the statement “Simple interest is greater than compound interest.”; 3) If simple is equal to compound, print the statement “Simple interest is equal to compound interest.”\n\n\nif (simple &lt; compound) {\n  print(\"Simple interest is less than compound interest\")\n} else if (simple &gt; compound) {\n  print(\"Simple interest is greater than compound interest\")\n} else {                             # could also be else if (simple == compound)\n  print(\"Simple interest is equal to compound interest\")\n}\n\n[1] \"Simple interest is less than compound interest\""
  },
  {
    "objectID": "intro.html#sec-introex",
    "href": "intro.html#sec-introex",
    "title": "3  Introduction to R",
    "section": "3.8 Exercises",
    "text": "3.8 Exercises\nIt’s a good idea to attempt these right away after reading this section while the content is fresh. You can find the answers in Appendix A.\n\nCalculate \\(89+9/(4*5)^2\\) and assign the value to the object solution1. Then print solution1.\nWhat is the difference between R and R Studio?\nHow do you add a comment to a Script file?\nWhat are packages in R, and how do you install them?\nInstall and load the package rstudioapi.\nHow do you modify the appearance of R Studio?\nAssign the value of 365 to an object called year. Then, create another object called months and assign it the value year * 0.032854884083862.\nCreate three variables named Cowboys, Giants, and Commanders and assign them all the value \"Inferior Team\" using multiple assignment operators."
  },
  {
    "objectID": "Operations.html#sec-sboex",
    "href": "Operations.html#sec-sboex",
    "title": "5  Strings & Comparisons",
    "section": "5.3 Exercises",
    "text": "5.3 Exercises\nAs always, it’s a good idea to attempt these while the material is still fresh. You can find the answers in Appendix D.\n\nCreate a variable called MarySue1 with the value \"Dr Mary Sue Coleman, former president of the University of Michigan once said\". Then, create another variable called MarySue2 with the value \"For today, goodbye. For tomorrow, good luck. And Forever, Go Blue!\". Then find the number of characters in each variable using the nchar() function. Then, check if the letter ‘r’ is present in each variable, and report the results.\nCreate a variable called MarySue3 whose value is a concatenation (combination) of MarySue1 and MarySue2 using the paste() function, in which the arguments are the variables separated by a comma. Then, print the value for MarySue3.\nCreate a string vector called basho and assign it the value \"An old silent pond. A frog jumps into the pond—Splash! Silence again.\" Then create and print another variable called basho2 in which the word ‘frog’ has been replaced by ‘buffalo’, and the word ‘Splash!’ has been replaced by ‘Yikes!’.\nLet’s do a variation of Mad Libs I will call Mad Sentences. Install and load the keyToEnglish package (be mindful of the capitalization in this package’s name). Then create three variables named after your three favorite cuisines. For each variable, assign the value generate_random_sentences(n = 2, punctuate = TRUE) to generate two random sentences per variable. This will produce a total of six sentences (two per variable). Finally, create a variable called madsentences whose value combines (pastes) all three variables. Print madsentences. If it sounds nonsensical, then it worked!\nLet’s compare the returns from simple vs compound interest after five years. First, define p as 1000, r as .07 and t as 5. Then Create a variable called simple with the value p * r * t. Next, create a variable compound with the value p * (1 + r)^t - p. Then, perform a logical test to see if simple is equal to compound, and write out the results of the test in one sentence.\nRetain the variables you created above and write a series of conditional (If else/Else If) statements according to the following rules: 1) If simple is less than compound, print the statement “Simple interest is less than compound interest”; 2) If simple is greater compound, print the statement “Simple interest is greater than compound interest”; 3) If simple is equal to compound, print the statement “Simple interest is equal to compound interest”."
  },
  {
    "objectID": "datatypes.html#sec-dtex",
    "href": "datatypes.html#sec-dtex",
    "title": "4  Data Types",
    "section": "4.7 Exercises",
    "text": "4.7 Exercises\nAs always, it’s a good idea to attempt these while the material is still fresh. You can find the answers in Appendix B.\n\nCreate a numeric vector called vec1 comprising the elements 3, -12, 532, 0, -100, 55, and -42. Then, find the median and lowest value in the vector.\nCreate a new vector vec2 which contains all the elements of vec1 that are positive integers. Then create a new vector vec3 that contains all the elements of vec1 that are negative integers. Then create a new vector vec4 which is the sum of vec2 and vec3.\nAssign the built-in dataframe mtcars to an object named after your favorite animal. Then calculate and print the median of the variable mpg.\nCreate a new variable and add to the object you created above (named after your favorite animal). This variable will take the variable Miles per Gallon mpg and convert it to Kilometers per Liter, which you will name kpl. This can be accomplished by taking mpg and dividing it by 2.352. Once you’ve created this new variable and added it to the object, calculate the median of kpl.\nAssign the built-in dataframe OrchardSprays to an object name of your choice. Then, convert the variable treatment to an ordered factor variable, and change the existing names of factor levels from A:H to a list of sulphur levels from Sulpher_8 to Sulpher_1. Then print the dataframe."
  },
  {
    "objectID": "cleaning.html#basic-folder-structure",
    "href": "cleaning.html#basic-folder-structure",
    "title": "8  Data Cleaning",
    "section": "8.1 Basic Folder Structure",
    "text": "8.1 Basic Folder Structure\nThere are several potential errors you can detect while looking at raw data. The first thing you should do is separate out raw data from the dataset you will be cleaning, and eventually produced a clean dataset. Leave the raw data in an encrypted folder or other secure location, and work with just the cleaned data (or data that will be cleaned). This will prevent you from overwriting the original dataset and making any errors in data cleaning permanent.\n\nOnce this separation of raw and clean (or to be cleaned) data is established, you can begin working on cleaning."
  },
  {
    "objectID": "cleaning.html#detecting-illogical-or-extreme-values",
    "href": "cleaning.html#detecting-illogical-or-extreme-values",
    "title": "8  Data Cleaning",
    "section": "8.5 Detecting Illogical or Extreme Values",
    "text": "8.5 Detecting Illogical or Extreme Values\nOne simple method of determining whether there exist illogical values in a numeric variable is to look at the maximum and minimum values (ensuring that they correspond to logical values for that variable) using the range() function. Similarly, boxplots boxplot() and histograms hist() are also useful. Let’s look at the built-in mtcars data frame.\n\n# Assign the mtcars dataframe to an object named df1\ndf1 &lt;- mtcars\n\n# I add a few outliers to the 'mpg' column\ndf1[c(2,5,7,9:11,22), 1] &lt;- 500\n\n# Let's examine the range of values in the 'mpg' column starting with the range.\nrange(df1$mpg)\n\n[1]  10.4 500.0\n\n\nImmediately, I see that the minimum value of 10 miles per gallon appears logical (think bigger SUVs which are less fuel-efficient than smaller sedan cars). However, the maximum value of 500 miles per gallon is not logical at all. How do I know? Well, for one I put in these entries. But if I didn’t know, a quick Google search tells me that the most fuel efficient gasoline car achieves 42 miles per gallon on the highway, and the most fuel efficient electric car achieves an 84 miles per gallon-equivalent. Thus, 500 miles per gallon is more of a distant dream rather than an accurate value.\nLet’s then visualize the variable with a histogram and a boxplot.\n\n# Next, let's look at a histogram of values\nhist(df1$mpg)\n\n\n\n# Next, let's look at a boxplot of the mpg variable.\nboxplot(df1$mpg)\n\n\n\n\nThe histogram shows us that the majority of values are less than 100, and a small subset of values lie around 500. This suggests that 500 might be an outlier value. The boxplot suggests that same, but provides us the Interquartile Range (IQR), which refers to the 25th percentile to 75th percentile of the data. The IQR is used to build box plots, which also display ‘whiskers’ or lines extending above and below the box, corresponding to 1.5 times the 75th percentile and 1.5 times the 25th percentile. If data points are outside the whiskers, these can be outliers. The boxplot above is super squished, and we can see a dot at 500. This indicates that a value of 500 might be an outlier.\nThe rstatix package also has some useful functions for detecting outliers. The identify_outliers() function in particular returns you a data frame with two new columns indicating whether a specific value is an outlier (defined in this package as 1.5 times the IQR) or an extreme value (defined in this package is 1.5 times the IQR).\n\nlibrary(rstatix)\nlibrary(dplyr)\n\ndf2 &lt;- df1 %&gt;%\n  identify_outliers(mpg)\n\n\nThe resulting data frame shows the seven unique values that are 1.5 and 3 times the IQR (see the two columns added at the end). For our purposes, this helps identify the specific outliers or extreme values. In this case, we have already determined that 500 is an illogical value for the fuel efficiency (miles per gallon) of cars. Thus, we can delete the values and replace them with NA to indicate missing values. To accomplish this, let’s use the which() function and %in% operator to identify the rows with a value of 500 for mpg from our original dataset. The which() function gives us the position or index which satisfies a given condition. Recall that the %in% operator checks whether the values in the first argument are present in the second argument, and returns a logical value. When we combine which() with the %in% operator, and add those into the first argument in square brackets after a dataframe, this will give the specific rows that satisfy a particular condition.\n\n# Identify rows with mpg == 500\ndf1[which(df1$mpg %in% 500),]\n\n                  mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4 Wag     500   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nHornet Sportabout 500   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nDuster 360        500   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 230          500   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280          500   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C         500   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nDodge Challenger  500   8 318.0 150 2.76 3.520 16.87  0  0    3    2\n\n# Replace 500 with NA\ndf1[which(df1$mpg %in% 500), 1] &lt;- NA\n\nIf we view the resulting dataframe, we can see that the mpg variable for the rows in question have all been replaced with NA to indicate missing data (indicated with red squares)."
  },
  {
    "objectID": "cleaning.html#variable-subsetting-naming-transformation-creation",
    "href": "cleaning.html#variable-subsetting-naming-transformation-creation",
    "title": "8  Data Cleaning",
    "section": "8.6 Variable Subsetting, Naming, Transformation & Creation",
    "text": "8.6 Variable Subsetting, Naming, Transformation & Creation\n\n8.6.1 Subsetting\nAnother key component of the data cleaning process is to prepare your set of variables for analysis. In R, since it is very easy to work with multiple dataframes at the same time (just assign them to different objects), it’s a good idea to create a dataframe with only the variables you need for the analysis. This is not strictly necessary, but keeps this organized, easy to view, and manageable. If you only need 20 variables for an analysis, why work with 175 variables in a data frame? It’s better to keep what you need and save those to a new dataframe.\nThis is easily accomplished by subsetting the dataframe based on one or more conditions. This could be a particular variable value, a particular set of rows, or some other condition. Let’s demonstrate this using data from the CrossFit Games from 2007-2023.\n\n# First let's import our CSV dataframe, and see the dimensions.\ncrossfit &lt;- read.csv(\"~/R Book/Datasets/crossfit.csv\", header = TRUE)\ndim(crossfit)\n\n[1] 1714   30\n\n# Next, let's have a quick peek at the first 10 rows and first six columns.\nhead(crossfit[, c(1:6)], n = 10)\n\n   competitorId    competitorName   firstName lastName gender genderId\n1          1616       Russ Greene        Russ   Greene      M        1\n2          1616       Russ Greene        Russ   Greene      M        1\n3          1685 Christopher Woods Christopher    Woods      M        1\n4          1690      Travis Mayer      Travis    Mayer      M        1\n5          1690      Travis Mayer      Travis    Mayer      M        1\n6          1690      Travis Mayer      Travis    Mayer      M        1\n7          1690      Travis Mayer      Travis    Mayer      M        1\n8          1690      Travis Mayer      Travis    Mayer      M        1\n9          1690      Travis Mayer      Travis    Mayer      M        1\n10         1690      Travis Mayer      Travis    Mayer      M        1\n\n\nLet’s say I’ve also had a more detailed look at the data, either beforehand using a codebook, or after importing the data into R and using the View() function. I’ve decided to conduct a descriptive analysis by examining the competitors with the most competition starts, the highest ranks, as well as some of their demographic characteristics. I decide to analyze the following variables shown in Table 8.1.\n\n\nTable 8.1: List of variables I will retain for analysis.\n\n\n\n\n\n\n\nVariable Name\nDescription\nVariable Type\n\n\n\n\ncompetitorId\nCompetitor’s unique identification number.\nFactor\n\n\ncompetitorName\nCompetitor’s full name.\nFactor\n\n\ngender\nCompetitor’s gender (male or female only in this dataset).\nFactor\n\n\nage\nCompetitor’s age at the time of competition.\nNumeric\n\n\nheight\nCompetitor’s height in centimeters.\nNumeric\n\n\nweight\nCompetitor’s weight in kilograms.\nNumeric\n\n\ncountryOfOriginName\nCompetitor’s country of origin\nFactor\n\n\nyear\nYear that competitor participated in competition.\nFactor\n\n\noverallScore\nCompetitor’s overall CrossFit score.\nNumeric\n\n\noverallRank\nCompetitor’s overall CrossFit rank.\nNumeric\n\n\n\n\nYou will notice that I put the type of variable in a column as well. This is not necessarily the variable type currently in the dataframe, but rather, the type of variable I want the variable to be before I proceed to any sort of analysis. If the variable is not in the correct class I want, I will transform it to the correct class.\nTo subset these variables, I can use the helpful subset() command from base R, where the first argument is the object to be subsetted, and the second argument can be used to select the variables I want to retain.\n\nlibrary(dplyr)\n\n# Keep only the variables I want using subset().\ncrossfit2 &lt;- subset(crossfit, select = c(\"competitorId\",\n                                         \"competitorName\",\n                                         \"gender\",\n                                         \"age\",\n                                         \"height\",\n                                         \"weight\",\n                                         \"countryOfOriginName\",\n                                         \"year\",\n                                         \"overallScore\",\n                                         \"overallRank\")\n                    )\n\nhead(crossfit2[, 1:7], n = 10)\n\n   competitorId    competitorName gender age height weight countryOfOriginName\n1          1616       Russ Greene      M  20    178     83                    \n2          1616       Russ Greene      M  21    178     83                    \n3          1685 Christopher Woods      M  29    163     82                    \n4          1690      Travis Mayer      M  23    181     93       United States\n5          1690      Travis Mayer      M  25    181     93       United States\n6          1690      Travis Mayer      M  26    181     93       United States\n7          1690      Travis Mayer      M  28    181     93       United States\n8          1690      Travis Mayer      M  29    181     93       United States\n9          1690      Travis Mayer      M  30    181     93       United States\n10         1690      Travis Mayer      M  31    181     93       United States\n\n\n\n\n8.6.2 (Re)Naming\nAlright, that’s looking pretty good. I’ve got all the variables I want saved in a new dataframe called crossfit2. However, I’m not really digging some of the variable names. Some of them like countryOfOriginName got some weird capitalization going on, which can increase the likelihood of an error later on. Let’s make simpler names for all variables. Remember, any final variable names should be reflected in an updated codebook. This applies for any dataframe you work with, because reproducibility involves careful documentation of variable transformations and choices.\n\n# First, let's look at all variable names in our smaller dataframe.\nnames(crossfit2)\n\n [1] \"competitorId\"        \"competitorName\"      \"gender\"             \n [4] \"age\"                 \"height\"              \"weight\"             \n [7] \"countryOfOriginName\" \"year\"                \"overallScore\"       \n[10] \"overallRank\"        \n\n# Next, since I want to make changes to most of the names, let's create a new vector of names and replace the old ones.\n\nnewnames &lt;- c(\"id\",\n              \"name\",\n              \"gender\",\n              \"age\",\n              \"height\",\n              \"weight\",\n              \"country\",\n              \"year\",\n              \"score\",\n              \"rank\")\n\nnames(crossfit2) &lt;- newnames\n\n# Let's check that it worked correctly.\nnames(crossfit2)\n\n [1] \"id\"      \"name\"    \"gender\"  \"age\"     \"height\"  \"weight\"  \"country\"\n [8] \"year\"    \"score\"   \"rank\"   \n\n# If we wanted to change a few variable names instead of writing out a whole vector, we can use the rename() function from the dplyr package. In this function the first argument is the new name, then an equals sign, and finally the old name.\n\ncrossfit2 &lt;- crossfit2 %&gt;%\n  rename(score_overall = score) %&gt;%\n  rename(rank_overall = rank)\n\nnames(crossfit2)\n\n [1] \"id\"            \"name\"          \"gender\"        \"age\"          \n [5] \"height\"        \"weight\"        \"country\"       \"year\"         \n [9] \"score_overall\" \"rank_overall\" \n\n\nAlright, so we now how to modify variable names. But what about variable labels? These are brief descriptions of each variable that are helpful to have in a dataframe to facilitate reproducibility. Even if you have a codebook, having variable labels can be useful for quick reference, and to know which variables are relevant to one’s analyses. This can be accomplished easily using the expss package’s apply_labels() function. The function takes the dataframe as the first argument, followed by a list of variables equal to their variable labels in quotes. Note: if the variable name contains spaces, then you should put it in quotes, or R won’t recognize it as a variable name in this function.\nLet’s go ahead and add some variable labels to these variables. We can then either View() the dataframe to see the variable labels, or use the str() function to have a quick look.\n\nlibrary(expss)\n\n# Let's add variable labels to all the variables.\n\ncrossfit2 &lt;- apply_labels(crossfit2,\n                          id = \"Participant's unique identification number\",\n                          name = \"Participant's full name\",\n                          gender = \"Participant's gender\",\n                          age = \"Participant's age at time of competition\",\n                          height = \"Participant's height (in centimeters)\",\n                          weight = \"Participant's weight (in kilograms)\",\n                          country = \"Participant's country of origin\",\n                          year = \"Participant's year of competition\",\n                          score_overall = \"Participant's total CrossFit Games Score\",\n                          rank_overall = \"Participant's overall CrossFit rank\"\n                          )\n\nstr(crossfit2)\n\n'data.frame':   1714 obs. of  10 variables:\n $ id           :Class 'labelled' int  1616 1616 1685 1690 1690 1690 1690 1690 1690 1690 ...\n   .. .. LABEL: Participant's unique identification number \n $ name         :Class 'labelled' chr  \"Russ Greene\" \"Russ Greene\" \"Christopher Woods\" \"Travis Mayer\" ...\n   .. .. LABEL: Participant's full name \n $ gender       :Class 'labelled' chr  \"M\" \"M\" \"M\" \"M\" ...\n   .. .. LABEL: Participant's gender \n $ age          :Class 'labelled' int  20 21 29 23 25 26 28 29 30 31 ...\n   .. .. LABEL: Participant's age at time of competition \n $ height       :Class 'labelled' num  178 178 163 181 181 181 181 181 181 181 ...\n   .. .. LABEL: Participant's height (in centimeters) \n $ weight       :Class 'labelled' num  83 83 82 93 93 93 93 93 93 93 ...\n   .. .. LABEL: Participant's weight (in kilograms) \n $ country      :Class 'labelled' chr  \"\" \"\" \"\" \"United States\" ...\n   .. .. LABEL: Participant's country of origin \n $ year         :Class 'labelled' int  2007 2008 2008 2014 2016 2017 2019 2020 2021 2022 ...\n   .. .. LABEL: Participant's year of competition \n $ score_overall:Class 'labelled' int  232 21 19 483 702 674 368 0 822 685 ...\n   .. .. LABEL: Participant's total CrossFit Games Score \n $ rank_overall :Class 'labelled' int  11 53 32 29 10 12 12 19 12 18 ...\n   .. .. LABEL: Participant's overall CrossFit rank \n\n# Let's say we only want to add variable labels for one variable. Then we can use the var_lab() function. \n\nvar_lab(crossfit2$score_overall) &lt;- \"Total CrossFit Games score\"\nvar_lab(crossfit2$score_overall)\n\n[1] \"Total CrossFit Games score\"\n\n\n\n\n8.6.3 Transformation & Creation\nNow that we know how to rename and subset the variables we want, what about transforming an old variable into a new one? Say that we wanted to derive a new height variable (in feet) from our existing height variable (in centimeters). From a quick Google search, I learn that to convert from centimeters to feet involves dividing the centimeters by 30.48.\nSimilarly, let’s also derive a new weight variable (in pounds) from our existing weight variable (in kilograms). Once again, a quick Google search tells me that to convert from kilograms to pounds involves dividing the kilograms by 0.45359237.\nWe can easily derive one variable from another in this fashion using the mutate() function from the dplyr package.\n\n# Convert centimeters to feet in a new variable called height_feet.\ncrossfit2 &lt;- crossfit2 %&gt;%\n  mutate(height_feet = height / 30.48) \n  \n# Let's look at these two variables side-by-side.\nhead(crossfit2[, c(\"height\", \"height_feet\")], n = 10)\n\n   height height_feet\n1     178    5.839895\n2     178    5.839895\n3     163    5.347769\n4     181    5.938320\n5     181    5.938320\n6     181    5.938320\n7     181    5.938320\n8     181    5.938320\n9     181    5.938320\n10    181    5.938320\n\n# Ok, it looks good generally. But, I'm not liking the many decimal places in the height_feet variable. In everyday parlance, we usually refer to someone as, for example, \"five foot eight\" and not \"five foot eight point 39895\". So let's use the round() function to shorten this variable to one decimal place.\ncrossfit2$height_feet &lt;- round(crossfit2$height_feet, digits = 1)\n\n# Let's see if it worked.\nhead(crossfit2[, c(\"height\", \"height_feet\")], n = 10)\n\n   height height_feet\n1     178         5.8\n2     178         5.8\n3     163         5.3\n4     181         5.9\n5     181         5.9\n6     181         5.9\n7     181         5.9\n8     181         5.9\n9     181         5.9\n10    181         5.9\n\n# Great, now let's do the same with the new variable weight_lbs.\ncrossfit2 &lt;- crossfit2 %&gt;%\n  mutate(weight_lbs = weight / 0.45359237)\n\n# Let's check out the variables side-by-side.\nhead(crossfit2[, c(\"weight\", \"weight_lbs\")], n = 10)\n\n   weight weight_lbs\n1      83   182.9837\n2      83   182.9837\n3      82   180.7791\n4      93   205.0299\n5      93   205.0299\n6      93   205.0299\n7      93   205.0299\n8      93   205.0299\n9      93   205.0299\n10     93   205.0299\n\n# Let's round this to the nearest unit, and see if it works.\ncrossfit2$weight_lbs &lt;- round(crossfit2$weight_lbs, digits = 0)\n\nhead(crossfit2[, c(\"weight\", \"weight_lbs\")], n = 10)\n\n   weight weight_lbs\n1      83        183\n2      83        183\n3      82        181\n4      93        205\n5      93        205\n6      93        205\n7      93        205\n8      93        205\n9      93        205\n10     93        205\n\n\nBefore we can summarize the data with some descriptive analyses, we need to make sure the the variable classes match those in Table 8.1. To quickly look at all the variable types in the dataframe, we can use the str() function.\n\nstr(crossfit2)\n\n'data.frame':   1714 obs. of  12 variables:\n $ id           :Class 'labelled' int  1616 1616 1685 1690 1690 1690 1690 1690 1690 1690 ...\n   .. .. LABEL: Participant's unique identification number \n $ name         :Class 'labelled' chr  \"Russ Greene\" \"Russ Greene\" \"Christopher Woods\" \"Travis Mayer\" ...\n   .. .. LABEL: Participant's full name \n $ gender       :Class 'labelled' chr  \"M\" \"M\" \"M\" \"M\" ...\n   .. .. LABEL: Participant's gender \n $ age          :Class 'labelled' int  20 21 29 23 25 26 28 29 30 31 ...\n   .. .. LABEL: Participant's age at time of competition \n $ height       :Class 'labelled' num  178 178 163 181 181 181 181 181 181 181 ...\n   .. .. LABEL: Participant's height (in centimeters) \n $ weight       :Class 'labelled' num  83 83 82 93 93 93 93 93 93 93 ...\n   .. .. LABEL: Participant's weight (in kilograms) \n $ country      :Class 'labelled' chr  \"\" \"\" \"\" \"United States\" ...\n   .. .. LABEL: Participant's country of origin \n $ year         :Class 'labelled' int  2007 2008 2008 2014 2016 2017 2019 2020 2021 2022 ...\n   .. .. LABEL: Participant's year of competition \n $ score_overall:Class 'labelled' int  232 21 19 483 702 674 368 0 822 685 ...\n   .. .. LABEL: Total CrossFit Games score \n $ rank_overall :Class 'labelled' int  11 53 32 29 10 12 12 19 12 18 ...\n   .. .. LABEL: Participant's overall CrossFit rank \n $ height_feet  :Class 'labelled' num  5.8 5.8 5.3 5.9 5.9 5.9 5.9 5.9 5.9 5.9 ...\n   .. .. LABEL: Participant's height (in centimeters) \n $ weight_lbs   :Class 'labelled' num  183 183 181 205 205 205 205 205 205 205 ...\n   .. .. LABEL: Participant's weight (in kilograms) \n\n\nWe can see that though some variables are in the correct class (e.g. year, score_overall, etc.), others are not (e.g. id, name, gender, and country). These variables are currently stored as strings (or characters), whereas we want them to be factors. This is why people often add the argument stringsAsFactors = TRUE to the read.csv() function, which automatically converts character variables to factor, which is the appropriate class for categorical variables. However, even if you invoke this argument, there will still be some variables that are misclassed. Most commonly, ID variables are often classes as integers, whereas they should really be factors, since they are discrete identifiers.\nFor now, let’s convert all the misclassed variables in the dataframe to factor using two methods: 1) a simple but inefficient approach, and 2) a slightly more complex but more efficient approach.\n\n\n# First, let's use the simple approach.\ncrossfit2$id &lt;- as.factor(crossfit2$id)\n\n# You can always use this approach with each variable one-by-one. But let's use a more efficient approach.\n\ncatvars &lt;- c(\"id\", \"name\", \"gender\", \"country\")\n\ncrossfit2 &lt;- crossfit2 %&gt;%\n  mutate(across(catvars, as.factor)\n         )\n\n# Let's check if it worked\nstr(crossfit2)\n\n'data.frame':   1714 obs. of  12 variables:\n $ id           : Factor w/ 953 levels \"1616\",\"1685\",..: 1 1 2 3 3 3 3 3 3 3 ...\n $ name         : Factor w/ 975 levels \"Aaron Finley\",..: 820 820 200 931 931 931 931 931 931 931 ...\n $ gender       : Factor w/ 2 levels \"F\",\"M\": 2 2 2 2 2 2 2 2 2 2 ...\n $ age          :Class 'labelled' int  20 21 29 23 25 26 28 29 30 31 ...\n   .. .. LABEL: Participant's age at time of competition \n $ height       :Class 'labelled' num  178 178 163 181 181 181 181 181 181 181 ...\n   .. .. LABEL: Participant's height (in centimeters) \n $ weight       :Class 'labelled' num  83 83 82 93 93 93 93 93 93 93 ...\n   .. .. LABEL: Participant's weight (in kilograms) \n $ country      : Factor w/ 116 levels \"\",\"Afghanistan\",..: 1 1 1 112 112 112 112 112 112 112 ...\n $ year         :Class 'labelled' int  2007 2008 2008 2014 2016 2017 2019 2020 2021 2022 ...\n   .. .. LABEL: Participant's year of competition \n $ score_overall:Class 'labelled' int  232 21 19 483 702 674 368 0 822 685 ...\n   .. .. LABEL: Total CrossFit Games score \n $ rank_overall :Class 'labelled' int  11 53 32 29 10 12 12 19 12 18 ...\n   .. .. LABEL: Participant's overall CrossFit rank \n $ height_feet  :Class 'labelled' num  5.8 5.8 5.3 5.9 5.9 5.9 5.9 5.9 5.9 5.9 ...\n   .. .. LABEL: Participant's height (in centimeters) \n $ weight_lbs   :Class 'labelled' num  183 183 181 205 205 205 205 205 205 205 ...\n   .. .. LABEL: Participant's weight (in kilograms) \n\n\nWe can see that the four variables are now correctly classed as factor. Notice that the more efficient approach involves the mutate(), across(), and as.factor() functions. We’ve already seen mutate()function, but note that the across() function allows us to carry out an operation across multiple columns. We created a vector called catvars of column names we want to manipulate, and passed that into the across() and mutate() functions. Finally, the as.factor() function is used without parentheses in order to re-class the variables to factor. Similarly, you can convert variables to numeric using the as.numeric() function, and convert to character using the as.character() function."
  },
  {
    "objectID": "cleaning.html#illogical-values-typos",
    "href": "cleaning.html#illogical-values-typos",
    "title": "8  Data Cleaning",
    "section": "8.3 Illogical Values & Typos",
    "text": "8.3 Illogical Values & Typos\nOne of the first things I always do with a raw data set is to check if there are any illogical values. These are values that fall outside of the acceptable set of response options for a particular variable. For example, if a question is asking about a participant’s age in years, a value of 366 would be an illogical value since humans cannot yet live that long. However, a value of 366 for a variable about age in months makes perfect sense, as it translates to an age of about 30 years, which is a logical value. As another example, if a survey of undergraduate students conducted at Boston University asks them to state their class year (e.g. first year, freshman, second year, sophomore, etc), and you see some entries with the value “Boston University”, this is an illogical value. It should either be modified to the correct class year, or deleted and left as missing data.\nCommonly, typos can create illogical values. These should be fairly obvious if you a codebook, which is a document providing the name, description, and meaning of each variable, including how it is coded and what response options are valid. These could be incorrect or illogical entries, but could also be logically but with incorrect punctuation. For example, entries for a variable about one’s favorite color may contain entries red, RED, Red, or rED. These are all logical values, but they differ in which letters are capitalized, potentially leading to issues in the analysis. Thus, they should all be standardized. I would change them all to red. There is no special reason for this particular format, other than consistency, but I have found that lowercase variable names introduce less potential for errors due to capitalization."
  },
  {
    "objectID": "cleaning.html#duplicate-values-outliers",
    "href": "cleaning.html#duplicate-values-outliers",
    "title": "8  Data Cleaning",
    "section": "8.4 Duplicate Values & Outliers",
    "text": "8.4 Duplicate Values & Outliers\nDifferent participants can have the same response to particular questions or variables. However, if different participants have EXACTLY the same values across ALL variables, then you more than likely have a case of a duplicate value, which should be removed. Duplicate values are key sources of bias, and can inflate or mask detectable effects, leading to misleading interpretations. Sometimes duplicate values can arise as errors when multiple data sets are joined together. A simple approach to avoiding this problem is to count the rows and columns in each dataset before they are joined together, and then ensure the total rows and columns after joining corresponds to the sum of rows and columns from each individual dataset.\nOutliers are values that are abnormally far away from other values in your dataset. Imagine you take the average net worth of five people sitting at a bar. Even though the average net worth of the first four people is $64,501, the average net worth of all five people at the bar comes to $ 58 billion! How is that possible? Because the fifth person at the bar is Bill Gates, whose 2023 net worth was $117.5 billion. Gates’ net worth value is an outlier because 117.5 billion is VERY VERY far distant from $64,501.\nFor right now, the important thing to note is that you should NOT remove outliers just because they are outliers. Outliers should only be removed if they represent true data entry/collection errors. For example, if an adult’s weight is entered as 12 pounds, this is likely a data entry error, but if their weight is listed as 432 pounds, this is a legitimately possible value, and if true, should not be removed. When outliers are legitimate extreme values, they represent a natural section of the population which you are studying. They are providing you information that should not be discarded simply to fit models easier, as this is a type of cherry-picking of the data which is . Rather, one should use statistical tests that are robust to outliers in such cases.\nSome people, such as professor of data science Pasquale Cirillo make a nice distinction between outliers (values that are not possible because they are too extreme in magnitude) and extremes (values that are possible and extreme in magnitude), show in Figure 8.2.\n\n\n\nFigure 8.2: The bounds for a legitimate value depend on the variable, and should guide your choice to delete or retain the value."
  },
  {
    "objectID": "cleaning.html#saving-your-cleaned-dataframe",
    "href": "cleaning.html#saving-your-cleaned-dataframe",
    "title": "8  Data Cleaning",
    "section": "8.7 Saving Your Cleaned Dataframe",
    "text": "8.7 Saving Your Cleaned Dataframe\n\n8.7.1 As an R Data File\nOnce you’re satisfied that the data are clean and ready for analysis, you can save the cleaned dataframe (or any object) as as an R Data file (.Rdata) or an RDS file (.RDS). R Data files can store multiple objects, while RDS files can only store a single R object. I tend to prefer R Data files as I can use them to store and load multiple objects at once. The save() function takes the R object names you want to save as the first n arguments, followed by the file =argument in which you specify a file name (and path) for saved object(s) in quotes and with the .Rdata file extension. To retrieve the object, simply use the load() function with the object name in quotes.\n\nsave(crossfit2, file = \"crossfitdf.Rdata\")\nload(\"crossfitdf.Rdata\")\n\n\n\n8.7.2 As an Excel or CSV File\nR data files are native to R, and you might want to save your dataframe in other formats. Perhaps, you want to save your data frame in Excel format (.xlsx). This can be done using the writexl package, and the write_xlsx() function where the first argument is the R object you want to save, and the second argument is the name (and path) of the Excel file with the .xlsx file extension.\nTo export to CSV format, you can use the built-in write.csv() function, where the first argument is the R object name, and the second is the file name (and path). I also recommend a third argument row.names = FALSE which gets rid of the row numbering which you already have when you open a spreadsheet.\n\n# To export to Excel format, use the writexl package.\nlibrary(writexl)\n\nwrite_xlsx(crossfit2, \"crossfitdf.xlsx\")\n\n# To export to csv, use the write.csv() function.\nwrite.csv(crossfit2, \"crossfitdf.csv\", row.names = FALSE)\n\n\n\n8.7.3 As a Stata, SPSS, or SAS File\nIf you want to export to SAS, SPSS, or SAS formats, we can use the haven package. For SAS, you can use the write_xpt() function. For Stata, you can use the write_dta() function. For SPSS, you can use the write_sav() function. The first argument of each is the R object to be saved, and the second argument is the name of the file (and path) with the appropriate file extension.\n\nlibrary(haven)\n\n# Export to SAS format\nwrite_xpt(crossfit2, \"crossfitdf.xpt\")\n\n# Export to Stata format\nwrite_dta(crossfit2, \"crossfitdf.dta\")\n\n# Export to SPSS format\nwrite_dta(crossfit2, \"crossfitdf.sav\")\n\n\n\n8.7.4 As a Text File\nThe write.table() function can be used to export your dataframe to a text file. As with the other functions, the first argument is the R object to be exported, and the second is the name (and path) with the appropriate file extension. I recommend using a separator for readability using the sep =argument. Common choices for separator are commas sep = \",\" or tabs sep = \"\\t\".\n\n# Export to text file\nwrite.table(crossfit2, \"crossfitdf.txt\", sep = \"\\t\")"
  },
  {
    "objectID": "cleaning.html#exercises",
    "href": "cleaning.html#exercises",
    "title": "7  Data Cleaning",
    "section": "7.7 Exercises",
    "text": "7.7 Exercises"
  },
  {
    "objectID": "Operations.html#strings",
    "href": "Operations.html#strings",
    "title": "5  Strings & Comparisons",
    "section": "5.1 Strings",
    "text": "5.1 Strings\n\nAs we discussed in Chapter 4, character values are stored in objects known as strings in R. Let’s go over a few key things with strings.\nFirst, let’s remember that string values are surrounded by quotes, such as x &lt;- \"Hello, World\". These can also be single quotes, such as y &lt;- 'Hello, World'. However, you CANNOT combine double quotes on one end of the value and single quotes on the other, such as z &lt;- 'Hello, World\". So, make sure you are consistent. I recommend using double quotes consistently, as R will always print and store the value with double quotes, even if you store the variable using single quotes.\nWhat if you want to store quote within a quote? In this case, you can use the standard grammar rules of American English. According to Brittney Ross from grammarly, in American English, we use double quotation marks for quotes, and single quotation marks for quotes within quotes. Here’s an example.\n\na &lt;- \"Invoking the Bard, she replied 'To thine own self be true.'\"\na\n\n[1] \"Invoking the Bard, she replied 'To thine own self be true.'\"\n\n\nNotice that simply running the object name returns its value, in the same way that print()does. We can check the length of a string with the nchar() function.\n\nnchar(a)\n\n[1] 59\n\n\nWe can also check if a character or sequence of characters exist in a given string using the grepl() function, in which the first argument is the character/sequence of interest, and the second argument is the string. Evaluating this expression returns a logical value.\n\ngrepl(\"Bard\", a)\n\n[1] TRUE\n\ngrepl(\"Z\", a)\n\n[1] FALSE\n\n\nTo combine strings, we can use the paste() function, where the arguments are the string objects to be combined. This is also called concatenating or merging multiple strings.\n\n# Let's create two new string objects b and c.\n\nb &lt;- \"Demurely and without hesitation, I invoked Jonson 'There is no greater hell than to be a prisoner of fear.'\"\n\nc &lt;- \"That ended the conversation rather quickly.\"\n\n# Now we combine the three\n\nd &lt;- paste(a,b,c)\n\nd\n\n[1] \"Invoking the Bard, she replied 'To thine own self be true.' Demurely and without hesitation, I invoked Jonson 'There is no greater hell than to be a prisoner of fear.' That ended the conversation rather quickly.\"\n\n\nNote that the paste() function concatenates strings with a space by default. If we don’t want spaces by default, we can use the paste0() function which does not separate strings by spaces, by default.\n\npaste0(\"Remove\", \"All\", \"Spaces\", \"Now\", \"!\", \"!\", \"!\")\n\n[1] \"RemoveAllSpacesNow!!!\"\n\n\nThe stringr package also has a number of useful functions for manipulating strings. One thing I find especially helpful in this package is the ability to convert the characters in a string to lowercase, uppercase, or title case. This is especially helpful if you have values with inconsistent punctuation. Here’s an example.\n\n# A string with inconsistent punctuation\nlibrary(stringr)\n\nWarning: package 'stringr' was built under R version 4.2.2\n\ne &lt;- \"WOW, tHiS Is qUiTE a mESsY oNe.\"\n\n# Let's str_to_lower() function to make all characters lowercase.\n\nf &lt;- str_to_lower(e)\nf\n\n[1] \"wow, this is quite a messy one.\"\n\n# Now we use the str_to_upper() function to make all characters uppercase. \n\ng &lt;- str_to_upper(f)\ng\n\n[1] \"WOW, THIS IS QUITE A MESSY ONE.\"\n\n# Finally, we use the str_to_title() function to make all characters title case.\n\nh &lt;- str_to_title(g)\nh\n\n[1] \"Wow, This Is Quite A Messy One.\"\n\n\nSometimes you have a string or vector of strings for which you want to apply a broad-sweeping change. This commonly happens with variable names or values, and sometimes, you want to write code to make changes to a number of values without changing the values one-by-one. In this case, the sub() and gsub() functions are very useful. These functions take the first argument as the pattern to match, the second as the thing you want to replace the pattern with, and the third is the dataframe or vector you are modifying. The main difference between sub() and gsub() is that sub() only modifies the first match in an individual string or vector, whereas gsub() modifies all matches in a particular string or vector. I tend to use gsub() more often than sub(), but use what works for you. Let’s see how it works.\n\n# First let's create a vector of strings.\n\nintro &lt;- c(\"Hello, my name is Jamal.\")\nprint(intro)\n\n[1] \"Hello, my name is Jamal.\"\n\n# Let's say I want to substitute another name for Jamal here. The gsub() function helps me do that. \n\ngsub('Jamal', \"Monica\", intro)\n\n[1] \"Hello, my name is Monica.\"\n\n# If you want to add a prefix to a vector of strings based on a particular pattern, you can use the weird symbol combination .*^ (period asterisk caret) as the first argument.\n\ngsub(\".*^\", \"Welcome and \", intro)\n\n[1] \"Welcome and Hello, my name is Jamal.\"\n\n# Finally, let's work with a dataframe.\n\ndf &lt;- data.frame(id = 1:4,\n                     gender = c(\"male\", \"female\", \"transgender\", \"non-binary\"),\n                     state = c(\"California\", \"Pennsylvania\", \"New York\", \"Georgia\")\n                     )\nhead(df)\n\n  id      gender        state\n1  1        male   California\n2  2      female Pennsylvania\n3  3 transgender     New York\n4  4  non-binary      Georgia\n\n# Let's change all the gender values to abbreviations.\n\ndf$gender &lt;- gsub(\"female\", \"F\", df$gender)\ndf$gender &lt;- gsub(\"male\", \"M\", df$gender)\ndf$gender &lt;- gsub(\"transgender\", \"T\", df$gender)\ndf$gender &lt;- gsub(\"non-binary\", \"NB\", df$gender)\n\nhead(df)\n\n  id gender        state\n1  1      M   California\n2  2      F Pennsylvania\n3  3      T     New York\n4  4     NB      Georgia\n\n# Looks good. Now let's change all the state names to their abbreviations. \n\ndf$state &lt;- gsub(\"California\", \"CA\", df$state)\ndf$state &lt;- gsub(\"Pennsylvania\", \"PA\", df$state)\ndf$state &lt;- gsub(\"New York\", \"NY\", df$state)\ndf$state &lt;- gsub(\"Georgia\", \"GA\", df$state)\n\nhead(df)\n\n  id gender state\n1  1      M    CA\n2  2      F    PA\n3  3      T    NY\n4  4     NB    GA\n\n# You can also substitute a blank space for any value with gsub()\n\ndf$state &lt;- gsub(\"CA\", \" \", df$state)\ndf$state\n\n[1] \" \"  \"PA\" \"NY\" \"GA\"\n\n\nWhen it comes to substituting strings, it’s better to test replacement code on a smaller set of values before expanding to all the values you want to modify. This will avoid errors."
  },
  {
    "objectID": "Operations.html#comparisons",
    "href": "Operations.html#comparisons",
    "title": "5  Strings & Comparisons",
    "section": "5.2 Comparisons",
    "text": "5.2 Comparisons\n\nVery often we are interested in comparing two quantities in R. For instance, we may compare a variable to a value, or two variables to one another. We can also use Boolean operators in constructing our comparisons. Boolean operators are words like AND, NOT, and OR that are used as conjunctions commonly to create advanced search terms. However, we can also use them in creating comparisons.\n\n5.2.1 Comparison Operators\nTo start, let us remind ourselves of common operators used in comparisons, as well as the three common Boolean operators, seen in Table 5.1.\n\n\nTable 5.1: Operators used for comparisons in R.\n\n\nOperator (R code)\nDescription\n\n\n\n\n==\nEquals to\n\n\n!=\nNot equal to\n\n\n&lt;\nLess than\n\n\n&gt;\nGreater than\n\n\n&lt;=\nLess than or equal to\n\n\n&gt;=\nGreater than or equal to\n\n\n!\nNOT\n\n\n|\nOR\n\n\n&\nAND\n\n\n\n\nYou can use these to compare values or objects. Comparisons result in a logical value being returned.\n\n# Comparing values\n\n12 &gt; 4\n\n[1] TRUE\n\n4.32 == (16.03/4)\n\n[1] FALSE\n\npi^pi &lt;= (4 * pi) / (pi + 4)\n\n[1] FALSE\n\n# Comparing objects\n\na &lt;- 453\n\nb &lt;- 4 * 23\n\na &gt;= b\n\n[1] TRUE\n\n## The objects being can be vectors of the same length. This will return a vector of logical values.\n\nc &lt;- mtcars$mpg &gt; mtcars$wt\nprint(c)\n\n [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[16] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[31] TRUE TRUE\n\n# Booleans can be used to create multiple conditions for the comparison.\n\nd &lt;- 500\ne &lt;- 50\n\n# This evaluates to FALSE becase d &gt; 100.\nd &lt; 100 & e &lt; 100\n\n[1] FALSE\n\n# This evaluates to TRUE because at least one of {d,e} &lt; 100. \nd &lt; 100 | e &lt; 100\n\n[1] TRUE\n\n# This evaluates to TRUE because d &gt; e.  \n\nd != e\n\n[1] TRUE\n\n\n\n\n5.2.2 Comparisons in Conditional Statements\nComparisons are commonly used within if and if else statements. These are conditional statements that specify a condition that must be satisfied, and then list rules for what happens if the condition is satisfied and what happens when it is not satisfied. The logic of the if else statement in show in Figure 5.1.\n\n\n\nFigure 5.1: The simple and elegant logic of the If Else statement.\n\n\nLet’s have a look at some simple If Else statements involving comparisons. Note that the If and Else blocks are surrounded by curly braces { }.\n\n# First, we create some variables.\n\nf &lt;- 25\ng &lt;- 50\nh &lt;- 10\n\n# Now let's try a simple If Else statement comparing the variables.\n\nif (g &lt; f) {                     # Here comes the If block\n  print(\"g is less than f\")\n} else {                         # Here comes the Else block\n  print(\"g is greater than f\")\n}\n\n[1] \"g is greater than f\"\n\nif (g &lt; h) {                     # Here comes the If block\n  print(\"g is less than h\")\n} else {                         # Here comes the Else block\n  print(\"g is not less than h\")\n}\n\n[1] \"g is not less than h\"\n\n\nWe can also add more conditions with the else if statement, which basically tells R “If the previous condition is not true, here’s another condition with a rule.” Let’s add an Else If statement to the example above.\n\n# First, we create some variables.\n\nf &lt;- 25\ng &lt;- 50\ni &lt;- 50\n\n# Now we add some Else If statements to our IF statement.\n\nif (g &lt; i) {\n  print(\"g is less than i\")\n} else if (g &gt; i) {\n  print(\"g is greater than i\")\n} else if (g == i) {\n  print(\"g is equal to i\")\n}\n\n[1] \"g is equal to i\"\n\n# You might say that the third Else If condition above can just be an Else statement, since if g !&lt; i & g !&gt; i, it must be that g == i. This is true! Let's write it that way.\n\nif (g &lt; i) {\n  print(\"g is less than i\")\n} else if (g &gt; i) {\n  print(\"g is greater than i\")\n} else {\n  print(\"g is equal to i\")\n}\n\n[1] \"g is equal to i\"\n\n\nWe can also add the AND operator & and OR operator | to the conditional statements.\n\n# Let's create some variables.\n\nj &lt;- 100\nk &lt;- 200\nl &lt;- 150\n\n# Now we use & (AND) in our if else statement.\n\nif (j &lt; l & j &lt; k) {\n  print(\"j is the lowest\")\n} else if (j &gt; l & j &gt; k) {\n  print(\"j is the highest\")\n} else {\n  print(\"j is the middle value\")\n}\n\n[1] \"j is the lowest\"\n\n# Now we use | (OR) in our if else statement.\n\nif (l &lt; k | l &lt; j) {\n  print(\"l is less than at least one other value\")\n} else {\n  print(\"l is the highest\")\n}\n\n[1] \"l is less than at least one other value\"\n\n\nThat about wraps it up for Strings and Comparisons. As always, don’t forget to…"
  },
  {
    "objectID": "cleaning.html#sec-cleanex",
    "href": "cleaning.html#sec-cleanex",
    "title": "8  Data Cleaning",
    "section": "8.8 Exercises",
    "text": "8.8 Exercises\nAs always, it’s a good idea to attempt these while the material is still fresh. You can find the answers in Appendix D.\nLet’s create a dataframe with the following code:\n\nindianfood &lt;- data.frame(name = c(\"Boondi\",\n                                  \"Gajar ka halwa\",\n                                  \"Ghevar\",\n                                  \"Kalakand\",\n                                  \"Misti Doi\",\n                                  \"Aloo tikki\",\n                                  \"Chicken tikka masala\"),\n                         ingredients = c(\"Maida flour, yogurt, oil, sugar\",\n                                         \"Carots, milk, sugar, ghee, cashews, raisins\",\n                                         \"Flour, ghee, kewra, milk, clarified butter, sugar, almonds, pistachio, saffron, green cardimom\",\n                                         \"Milk, cottage cheese, sugar\",\n                                         \"Milk, jaggery\",\n                                         \"Rice flour, potatoe, bread crumbs, garam masalaaaa, salt\",\n                                         \"Naan bread, tomato sauce, skinless chicken breasts, heavy cream, garam masala\"),\n                         prep_time = c(45,\n                                       15,\n                                       15,\n                                       20,\n                                       480,\n                                       5,\n                                       15020240),\n                         state = c(\"West Bengal\",\n                                   \"Punjab\",\n                                   \"Rajasthan\",\n                                   \"West Bengal\",\n                                   \"West Bengal\",\n                                   \"Punjab\",\n                                   \"Punjab\")\n                         )\n\nNext, let’s fix some errors and make some changes!\n\nFirst, let’s create some more descriptive variable names. Change the existing variables names to Name, Ingredients, Preparation Time (mins), and Origin (state). Then, print the names of the dataframe.\nNext, having read through the ingredients, we can see various typos that should be fixed. Go ahead and fix the spelling mistakes (typos) you see in the Ingredients column. Hint: there are four spelling mistakes in the Ingredients column, and we are using standard American English for spelling.\nI see that the ingredients list has the terms “cottage cheese”, “clarified butter”, and “naan bread.” Substitute paneer for “cottage cheese” and remove “clarified butter” since there is already ‘ghee’ in the same row, which is pretty much the same thing. Also, remove ‘bread’ from “Naan bread”, since that is unnecessary. Then print the Ingredients column.\nLet’s check the class of the Origin (state) variable. If it is not already in factor class, go ahead and convert it to factor. Make sure to save the new factor-classed variable back in the dataset. Then, check the class of the variable again to make sure it is factor.\nNext, let’s add some variable labels to each variable. The variable labels can say whatever you think might be helpful. In general, think of some description that will provide context if you are looking at these data for the first time. Then, do something to check if it worked. Hint: you’ve seen two ways to do this.\nFinally, let’s have a look at the Preparation Time (mins) variable to detect any outliers. Print the range of values and produce a histogram of this variable. Are any values seemingly outliers from the histogram?\nIf you detected an outlier in the previous question (and I hope that you did!), explain your decision as to remove it or retain it in the dataframe. If you choose to remove the outlier value, replace it with a more plausible value derived from a quick Google search. Then, print the variable.\nFinally, save your cleaned indianfood dataframe as an R data file as well as a CSV file using the file name of your choice."
  },
  {
    "objectID": "app5.html",
    "href": "app5.html",
    "title": "Appendix E — Answers for Section 7.7",
    "section": "",
    "text": "The following are answers to the exercises in Section 7.7.\nLet’s create a dataframe with the following code:\n\nindianfood &lt;- data.frame(name = c(\"Boondi\",\n                                  \"Gajar ka halwa\",\n                                  \"Ghevar\",\n                                  \"Kalakand\",\n                                  \"Misti Doi\",\n                                  \"Aloo tikki\",\n                                  \"Chicken tikka masala\"),\n                         ingredients = c(\"Maida flour, yogurt, oil, sugar\",\n                                         \"Carots, milk, sugar, ghee, cashews, raisins\",\n                                         \"Flour, ghee, kewra, milk, clarified butter, sugar, almonds, pistachio, saffron, green cardimom\",\n                                         \"Milk, cottage cheese, sugar\",\n                                         \"Milk, jaggery\",\n                                         \"Rice flour, potatoe, bread crumbs, garam masalaaaa, salt\",\n                                         \"Naan bread, tomato sauce, skinless chicken breasts, heavy cream, garam masala\"),\n                         prep_time = c(45,\n                                       15,\n                                       15,\n                                       20,\n                                       480,\n                                       5,\n                                       15020240),\n                         state = c(\"West Bengal\",\n                                   \"Punjab\",\n                                   \"Rajasthan\",\n                                   \"West Bengal\",\n                                   \"West Bengal\",\n                                   \"Punjab\",\n                                   \"Punjab\")\n                         )\n\nNext, let’s fix some errors and make some changes!\n\nFirst, let’s create some more descriptive variable names. Change the existing variables names to Name, Ingredients, Preparation Time (mins), and Origin (state). Then, print the names of the dataframe.\n\n\nnewnames &lt;- c(\"Name\", \"Ingredients\", \"Preparation Time (mins)\", \"Origin (state)\")\n\nnames(indianfood) &lt;- newnames\nnames(indianfood)\n\n[1] \"Name\"                    \"Ingredients\"            \n[3] \"Preparation Time (mins)\" \"Origin (state)\"         \n\n\n\nNext, having read through the ingredients, we can see various typos that should be fixed. Go ahead and fix the spelling mistakes (typos) you see in the Ingredients column. Hint: there are four spelling mistakes in the Ingredients column, and we are using standard American English for spelling.\n\n\n# I see typos in Rows 2, 3, and 6. In row 2, carrot is spelled incorrectly. In row 3, cardamom is spelled incorrectly. In row 6, potato and masala are spelled incorrectly. \n\nindianfood[2,2] &lt;- \"Carrots, milk, sugar, ghee, cashews, raisins\"\nindianfood[3,2] &lt;- \"Flour, ghee, kewra, milk, clarified butter, sugar, almonds, pistachio, saffron, green cardamom\"\nindianfood[6,2] &lt;- \"Rice flour, potato, bread crumbs, garam masala, salt\"\n\n\nI see that the ingredients list has the terms “cottage cheese”, “clarified butter”, and “naan bread.” Substitute paneer for “cottage cheese” and remove “clarified butter” since there is already ‘ghee’ in the same row, which is pretty much the same thing. Also, remove ‘bread’ from “Naan bread”, since that is unnecessary. Then print the Ingredients column.\n\n\nindianfood$Ingredients &lt;- gsub(\"cottage cheese\", \"paneer\", indianfood$Ingredients)\nindianfood$Ingredients &lt;- gsub(\" clarified butter,\", \" \", indianfood$Ingredients)\nindianfood$Ingredients &lt;- gsub(\"Naan bread\", \"Naan\", indianfood$Ingredients)\n\n\nprint(indianfood$Ingredients)\n\n[1] \"Maida flour, yogurt, oil, sugar\"                                              \n[2] \"Carrots, milk, sugar, ghee, cashews, raisins\"                                 \n[3] \"Flour, ghee, kewra, milk,  sugar, almonds, pistachio, saffron, green cardamom\"\n[4] \"Milk, paneer, sugar\"                                                          \n[5] \"Milk, jaggery\"                                                                \n[6] \"Rice flour, potato, bread crumbs, garam masala, salt\"                         \n[7] \"Naan, tomato sauce, skinless chicken breasts, heavy cream, garam masala\"      \n\n\n\nLet’s check the class of the Origin (state) variable. If it is not already in factor class, go ahead and convert it to factor. Make sure to save the new factor-classed variable back in the dataset. Then, check the class of the variable again to make sure it is factor.\n\n\n# Check the class of the Origin (state) variable.\nclass(indianfood$`Origin (state)`)\n\n[1] \"character\"\n\n# It's currently in character class, so let's make it factor.\nindianfood$`Origin (state)` &lt;- as.factor(indianfood$`Origin (state)`)\n\n# Let's check the new class.\nclass(indianfood$`Origin (state)`)\n\n[1] \"factor\"\n\n\n\nNext, let’s add some variable labels to each variable. The variable labels can say whatever you think might be helpful. In general, think of some description that will provide context if you are looking at these data for the first time. Then, do something to check if it worked. Hint: you’ve seen two ways to do this.\n\n\nlibrary(expss)\n\nindianfood &lt;- apply_labels(indianfood,\n                           Name = \"Name of the Indian dish\",\n                           Ingredients = \"Required components to make the Indian dish\",\n                           \"Preparation Time (mins)\" = \"The time in minutes it takes to prepare to cook the Indian dish\",\n                           \"Origin (state)\" = \"The provenance of the Indian dish by state.\")\n\nstr(indianfood) # you can also View(indianfood) to see the variable labels under the variable names.\n\n'data.frame':   7 obs. of  4 variables:\n $ Name                   :Class 'labelled' chr  \"Boondi\" \"Gajar ka halwa\" \"Ghevar\" \"Kalakand\" ...\n   .. .. LABEL: Name of the Indian dish \n $ Ingredients            :Class 'labelled' chr  \"Maida flour, yogurt, oil, sugar\" \"Carrots, milk, sugar, ghee, cashews, raisins\" \"Flour, ghee, kewra, milk,  sugar, almonds, pistachio, saffron, green cardamom\" \"Milk, paneer, sugar\" ...\n   .. .. LABEL: Required components to make the Indian dish \n $ Preparation Time (mins):Class 'labelled' num  45 15 15 20 480 ...\n   .. .. LABEL: The time in minutes it takes to prepare to cook the Indian dish \n $ Origin (state)         :Class 'labelled' Factor w/ 3 levels \"Punjab\",\"Rajasthan\",..: 3 1 2 3 3 1 1\n   .. .. LABEL: The provenance of the Indian dish by state. \n\n\n\nFinally, let’s have a look at the Preparation Time (mins) variable to detect any outliers. Print the range of values and produce a histogram of this variable. Are any values seemingly outliers from the histogram?\n\n\n# Look at the range of the values.\nrange(indianfood$`Preparation Time (mins)`)\n\n[1]        5 15020240\n\n# Alright, the highest value is definitely suspect. Let's look at the histogram.\nhist(indianfood$`Preparation Time (mins)`)\n\n\n\n# The value 15020240 definitely seems like an outlier because it is SO far away from the other values.\n\n\nIf you detected an outlier in the previous question (and I hope that you did!), explain your decision as to remove it or retain it in the dataframe. If you choose to remove the outlier value, replace it with a more plausible value derived from a quick Google search. Then, print the variable.\n\n\n# Though you don't need to do this since we already see that 15020240 is way beyond the IQR, you can look at a boxplot or using the rstatix's package's identify_outliers function to check this as well. \n\nlibrary(rstatix)\nlibrary(dplyr)\n\nindianfoodoutliers &lt;- indianfood %&gt;%\n  identify_outliers(`Preparation Time (mins)`)\n\n# If you want to see the actual row with the outlier, you can run View(indianfoodoutliers).\n\n# Does a value of 15020240 minutes make sense as the preparation time for chicken tikka masala. First, how long is that value in hours?\n15020240 / 60\n\n[1] 250337.3\n\n## Ok, so it equates to 250,337.3 hours. This seems obviously false. Just to be sure, I Googled chicken tikka masala recipes and see that the preparation time is 20 minutes. Thus, this value is an outlier in that it is illogical and likely a data entry error. I will replace it with 20 minutes.\n\n# Find the row number corresponding to the outlying value.\nwhich(indianfood$`Preparation Time (mins)` == 15020240)\n\n[1] 7\n\n# Now, change the value for Preparation Time (mins) in row 7 to 20, and check that it worked.\nindianfood[7, 3] &lt;- 20\n\nprint(indianfood$`Preparation Time (mins)`)\n\nLABEL: The time in minutes it takes to prepare to cook the Indian dish \nVALUES:\n45, 15, 15, 20, 480, 5, 20\n\n\n\nFinally, save your cleaned indianfood dataframe as an R data file as well as a CSV file using the file name of your choice.\n\n\n# Save as an R Data file\nsave(indianfood, file = \"indianfood.Rdata\")\n\n# Save as a CSV file\nwrite.csv(indianfood, \"indianfood.csv\", row.names = FALSE)"
  },
  {
    "objectID": "dataexp.html",
    "href": "dataexp.html",
    "title": "8  Data Exploration with the Tidyverse",
    "section": "",
    "text": "Source: www.tidyverse.org\n\n\nAccording to its website:\n\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.\n\nIt currently has about 30 packages which can all be installed simultaneously with install.packages(\"tidyverse\"). Once you load the tidyverse, you can see a full list of its packages with the function tidyverse_packages().\n\nlibrary(tidyverse)\n\ntidyverse_packages()\n\n [1] \"broom\"         \"conflicted\"    \"cli\"           \"dbplyr\"       \n [5] \"dplyr\"         \"dtplyr\"        \"forcats\"       \"ggplot2\"      \n [9] \"googledrive\"   \"googlesheets4\" \"haven\"         \"hms\"          \n[13] \"httr\"          \"jsonlite\"      \"lubridate\"     \"magrittr\"     \n[17] \"modelr\"        \"pillar\"        \"purrr\"         \"ragg\"         \n[21] \"readr\"         \"readxl\"        \"reprex\"        \"rlang\"        \n[25] \"rstudioapi\"    \"rvest\"         \"stringr\"       \"tibble\"       \n[29] \"tidyr\"         \"xml2\"          \"tidyverse\"    \n\n\nThe tidyverse is a collection of useful tools that are commonly used by R users. In particular, the packages dplyr, and ggplot2 are the stars of the show, and very useful for data wrangling and visualization."
  },
  {
    "objectID": "stringscomps.html#strings",
    "href": "stringscomps.html#strings",
    "title": "5  Strings & Comparisons",
    "section": "5.1 Strings",
    "text": "5.1 Strings\n\nAs we discussed in Chapter 4, character values are stored in objects known as strings in R. Let’s go over a few key things with strings.\nFirst, let’s remember that string values are surrounded by quotes, such as x &lt;- \"Hello, World\". These can also be single quotes, such as y &lt;- 'Hello, World'. However, you CANNOT combine double quotes on one end of the value and single quotes on the other, such as z &lt;- 'Hello, World\". So, make sure you are consistent. I recommend using double quotes consistently, as R will always print and store the value with double quotes, even if you store the variable using single quotes.\nWhat if you want to store quote within a quote? In this case, you can use the standard grammar rules of American English. According to Brittney Ross from grammarly, in American English, we use double quotation marks for quotes, and single quotation marks for quotes within quotes. Here’s an example.\n\na &lt;- \"Invoking the Bard, she replied 'To thine own self be true.'\"\na\n\n[1] \"Invoking the Bard, she replied 'To thine own self be true.'\"\n\n\nNotice that simply running the object name returns its value, in the same way that print()does. We can check the length of a string with the nchar() function.\n\nnchar(a)\n\n[1] 59\n\n\nWe can also check if a character or sequence of characters exist in a given string using the grepl() function, in which the first argument is the character/sequence of interest, and the second argument is the string. Evaluating this expression returns a logical value.\n\ngrepl(\"Bard\", a)\n\n[1] TRUE\n\ngrepl(\"Z\", a)\n\n[1] FALSE\n\n\nTo combine strings, we can use the paste() function, where the arguments are the string objects to be combined. This is also called concatenating or merging multiple strings.\n\n# Let's create two new string objects b and c.\n\nb &lt;- \"Demurely and without hesitation, I invoked Jonson 'There is no greater hell than to be a prisoner of fear.'\"\n\nc &lt;- \"That ended the conversation rather quickly.\"\n\n# Now we combine the three\n\nd &lt;- paste(a,b,c)\n\nd\n\n[1] \"Invoking the Bard, she replied 'To thine own self be true.' Demurely and without hesitation, I invoked Jonson 'There is no greater hell than to be a prisoner of fear.' That ended the conversation rather quickly.\"\n\n\nNote that the paste() function concatenates strings with a space by default. If we don’t want spaces by default, we can use the paste0() function which does not separate strings by spaces, by default.\n\npaste0(\"Remove\", \"All\", \"Spaces\", \"Now\", \"!\", \"!\", \"!\")\n\n[1] \"RemoveAllSpacesNow!!!\"\n\n\nThe stringr package also has a number of useful functions for manipulating strings. One thing I find especially helpful in this package is the ability to convert the characters in a string to lowercase, uppercase, or title case. This is especially helpful if you have values with inconsistent punctuation. Here’s an example.\n\n# A string with inconsistent punctuation\nlibrary(stringr)\n\nWarning: package 'stringr' was built under R version 4.2.2\n\ne &lt;- \"WOW, tHiS Is qUiTE a mESsY oNe.\"\n\n# Let's str_to_lower() function to make all characters lowercase.\n\nf &lt;- str_to_lower(e)\nf\n\n[1] \"wow, this is quite a messy one.\"\n\n# Now we use the str_to_upper() function to make all characters uppercase. \n\ng &lt;- str_to_upper(f)\ng\n\n[1] \"WOW, THIS IS QUITE A MESSY ONE.\"\n\n# Finally, we use the str_to_title() function to make all characters title case.\n\nh &lt;- str_to_title(g)\nh\n\n[1] \"Wow, This Is Quite A Messy One.\"\n\n\nSometimes you have a string or vector of strings for which you want to apply a broad-sweeping change. This commonly happens with variable names or values, and sometimes, you want to write code to make changes to a number of values without changing the values one-by-one. In this case, the sub() and gsub() functions are very useful. These functions take the first argument as the pattern to match, the second as the thing you want to replace the pattern with, and the third is the dataframe or vector you are modifying. The main difference between sub() and gsub() is that sub() only modifies the first match in an individual string or vector, whereas gsub() modifies all matches in a particular string or vector. I tend to use gsub() more often than sub(), but use what works for you. Let’s see how it works.\n\n# First let's create a vector of strings.\n\nintro &lt;- c(\"Hello, my name is Jamal.\")\nprint(intro)\n\n[1] \"Hello, my name is Jamal.\"\n\n# Let's say I want to substitute another name for Jamal here. The gsub() function helps me do that. \n\ngsub('Jamal', \"Monica\", intro)\n\n[1] \"Hello, my name is Monica.\"\n\n# If you want to add a prefix to a vector of strings based on a particular pattern, you can use the weird symbol combination .*^ (period asterisk caret) as the first argument.\n\ngsub(\".*^\", \"Welcome and \", intro)\n\n[1] \"Welcome and Hello, my name is Jamal.\"\n\n# Finally, let's work with a dataframe.\n\ndf &lt;- data.frame(id = 1:4,\n                     gender = c(\"male\", \"female\", \"transgender\", \"non-binary\"),\n                     state = c(\"California\", \"Pennsylvania\", \"New York\", \"Georgia\")\n                     )\nhead(df)\n\n  id      gender        state\n1  1        male   California\n2  2      female Pennsylvania\n3  3 transgender     New York\n4  4  non-binary      Georgia\n\n# Let's change all the gender values to abbreviations.\n\ndf$gender &lt;- gsub(\"female\", \"F\", df$gender)\ndf$gender &lt;- gsub(\"male\", \"M\", df$gender)\ndf$gender &lt;- gsub(\"transgender\", \"T\", df$gender)\ndf$gender &lt;- gsub(\"non-binary\", \"NB\", df$gender)\n\nhead(df)\n\n  id gender        state\n1  1      M   California\n2  2      F Pennsylvania\n3  3      T     New York\n4  4     NB      Georgia\n\n# Looks good. Now let's change all the state names to their abbreviations. \n\ndf$state &lt;- gsub(\"California\", \"CA\", df$state)\ndf$state &lt;- gsub(\"Pennsylvania\", \"PA\", df$state)\ndf$state &lt;- gsub(\"New York\", \"NY\", df$state)\ndf$state &lt;- gsub(\"Georgia\", \"GA\", df$state)\n\nhead(df)\n\n  id gender state\n1  1      M    CA\n2  2      F    PA\n3  3      T    NY\n4  4     NB    GA\n\n# You can also substitute a blank space for any value with gsub()\n\ndf$state &lt;- gsub(\"CA\", \" \", df$state)\ndf$state\n\n[1] \" \"  \"PA\" \"NY\" \"GA\"\n\n\nWhen it comes to substituting strings, it’s better to test replacement code on a smaller set of values before expanding to all the values you want to modify. This will avoid errors."
  },
  {
    "objectID": "stringscomps.html#comparisons",
    "href": "stringscomps.html#comparisons",
    "title": "5  Strings & Comparisons",
    "section": "5.2 Comparisons",
    "text": "5.2 Comparisons\n\nVery often we are interested in comparing two quantities in R. For instance, we may compare a variable to a value, or two variables to one another. We can also use Boolean operators in constructing our comparisons. Boolean operators are words like AND, NOT, and OR that are used as conjunctions commonly to create advanced search terms. However, we can also use them in creating comparisons.\n\n5.2.1 Comparison Operators\nTo start, let us remind ourselves of common operators used in comparisons, as well as the three common Boolean operators, seen in Table 5.1.\n\n\nTable 5.1: Operators used for comparisons in R.\n\n\nOperator (R code)\nDescription\n\n\n\n\n==\nEquals to\n\n\n!=\nNot equal to\n\n\n&lt;\nLess than\n\n\n&gt;\nGreater than\n\n\n&lt;=\nLess than or equal to\n\n\n&gt;=\nGreater than or equal to\n\n\n!\nNOT\n\n\n|\nOR\n\n\n&\nAND\n\n\n\n\nYou can use these to compare values or objects. Comparisons result in a logical value being returned.\n\n# Comparing values\n\n12 &gt; 4\n\n[1] TRUE\n\n4.32 == (16.03/4)\n\n[1] FALSE\n\npi^pi &lt;= (4 * pi) / (pi + 4)\n\n[1] FALSE\n\n# Comparing objects\n\na &lt;- 453\n\nb &lt;- 4 * 23\n\na &gt;= b\n\n[1] TRUE\n\n## The objects being can be vectors of the same length. This will return a vector of logical values.\n\nc &lt;- mtcars$mpg &gt; mtcars$wt\nprint(c)\n\n [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[16] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[31] TRUE TRUE\n\n# Booleans can be used to create multiple conditions for the comparison.\n\nd &lt;- 500\ne &lt;- 50\n\n# This evaluates to FALSE becase d &gt; 100.\nd &lt; 100 & e &lt; 100\n\n[1] FALSE\n\n# This evaluates to TRUE because at least one of {d,e} &lt; 100. \nd &lt; 100 | e &lt; 100\n\n[1] TRUE\n\n# This evaluates to TRUE because d &gt; e.  \n\nd != e\n\n[1] TRUE\n\n\n\n\n5.2.2 Comparisons in Conditional Statements\nComparisons are commonly used within if and if else statements. These are conditional statements that specify a condition that must be satisfied, and then list rules for what happens if the condition is satisfied and what happens when it is not satisfied. The logic of the if else statement in show in Figure 5.1.\n\n\n\nFigure 5.1: The simple and elegant logic of the If Else statement.\n\n\nLet’s have a look at some simple If Else statements involving comparisons. Note that the If and Else blocks are surrounded by curly braces { }.\n\n# First, we create some variables.\n\nf &lt;- 25\ng &lt;- 50\nh &lt;- 10\n\n# Now let's try a simple If Else statement comparing the variables.\n\nif (g &lt; f) {                     # Here comes the If block\n  print(\"g is less than f\")\n} else {                         # Here comes the Else block\n  print(\"g is greater than f\")\n}\n\n[1] \"g is greater than f\"\n\nif (g &lt; h) {                     # Here comes the If block\n  print(\"g is less than h\")\n} else {                         # Here comes the Else block\n  print(\"g is not less than h\")\n}\n\n[1] \"g is not less than h\"\n\n\nWe can also add more conditions with the else if statement, which basically tells R “If the previous condition is not true, here’s another condition with a rule.” Let’s add an Else If statement to the example above.\n\n# First, we create some variables.\n\nf &lt;- 25\ng &lt;- 50\ni &lt;- 50\n\n# Now we add some Else If statements to our IF statement.\n\nif (g &lt; i) {\n  print(\"g is less than i\")\n} else if (g &gt; i) {\n  print(\"g is greater than i\")\n} else if (g == i) {\n  print(\"g is equal to i\")\n}\n\n[1] \"g is equal to i\"\n\n# You might say that the third Else If condition above can just be an Else statement, since if g !&lt; i & g !&gt; i, it must be that g == i. This is true! Let's write it that way.\n\nif (g &lt; i) {\n  print(\"g is less than i\")\n} else if (g &gt; i) {\n  print(\"g is greater than i\")\n} else {\n  print(\"g is equal to i\")\n}\n\n[1] \"g is equal to i\"\n\n\nWe can also add the AND operator & and OR operator | to the conditional statements.\n\n# Let's create some variables.\n\nj &lt;- 100\nk &lt;- 200\nl &lt;- 150\n\n# Now we use & (AND) in our if else statement.\n\nif (j &lt; l & j &lt; k) {\n  print(\"j is the lowest\")\n} else if (j &gt; l & j &gt; k) {\n  print(\"j is the highest\")\n} else {\n  print(\"j is the middle value\")\n}\n\n[1] \"j is the lowest\"\n\n# Now we use | (OR) in our if else statement.\n\nif (l &lt; k | l &lt; j) {\n  print(\"l is less than at least one other value\")\n} else {\n  print(\"l is the highest\")\n}\n\n[1] \"l is less than at least one other value\"\n\n\nThat about wraps it up for Strings and Comparisons. As always, don’t forget to…"
  },
  {
    "objectID": "stringscomps.html#sec-sboex",
    "href": "stringscomps.html#sec-sboex",
    "title": "5  Strings & Comparisons",
    "section": "5.3 Exercises",
    "text": "5.3 Exercises\nAs always, it’s a good idea to attempt these while the material is still fresh. You can find the answers in Appendix C.\n\nCreate a variable called MarySue1 with the value \"Dr Mary Sue Coleman, former president of the University of Michigan once said\". Then, create another variable called MarySue2 with the value \"For today, goodbye. For tomorrow, good luck. And Forever, Go Blue!\". Then find the number of characters in each variable using the nchar() function. Then, check if the letter ‘r’ is present in each variable, and report the results.\nCreate a variable called MarySue3 whose value is a concatenation (combination) of MarySue1 and MarySue2. Then, print the value for MarySue3.\nCreate a string vector called basho and assign it the value \"An old silent pond. A frog jumps into the pond—Splash! Silence again.\" Then create and print another variable called basho2 in which the word ‘frog’ has been replaced by ‘buffalo’, and the word ‘Splash!’ has been replaced by ‘Yikes!’.\nLet’s do a variation of Mad Libs I will call Mad Sentences. Install and load the keyToEnglish package (be mindful of the capitalization in this package’s name). Then create three variables named after your three favorite cuisines. For each variable, assign the value generate_random_sentences(n = 2, punctuate = TRUE) to generate two random sentences per variable. This will produce a total of six sentences (two per variable). Finally, create a variable called madsentences whose value combines (pastes) all three variables. Print madsentences. If it sounds nonsensical, then it worked!\nLet’s compare the returns from simple vs compound interest after five years. First, define p as 1000, r as .07 and t as 5. Then Create a variable called simple with the value p * r * t. Next, create a variable compound with the value p * (1 + r)^t - p. Then, perform a logical test to see if simple is equal to compound, and write out the results of the test in one sentence.\nRetain the variables you created above and write a series of conditional (If else/Else If) statements according to the following rules: 1) If simple is less than compound, print the statement “Simple interest is less than compound interest”; 2) If simple is greater compound, print the statement “Simple interest is greater than compound interest”; 3) If simple is equal to compound, print the statement “Simple interest is equal to compound interest”."
  },
  {
    "objectID": "dataexp.html#the-pipe-operator",
    "href": "dataexp.html#the-pipe-operator",
    "title": "9  Data Exploration",
    "section": "9.1 The Pipe Operator (%>%)",
    "text": "9.1 The Pipe Operator (%&gt;%)\nThe pipe operator %&gt;% is used to combine functions in a way that sets up a chain of operations. It was originally part of the magrittr package, and now comes standard with the dplyr package as well. The pipe is essential telling R “Take the output from the left side and pass it into the right side as the first argument.” Put more simply, the pipe is saying “Take this [stuff on left] and put it through that [stuff on right]”. This is important because it can make our code more efficient when we want to multiple things. Let’s look at a simple example.\n\nlibrary(dplyr)\n\n# First, let's randomly sample 10 numbers from a normal distribution.\n\nx &lt;- rnorm(10)\nprint(x)\n\n [1] -0.8730578 -0.2581946  0.8216233  0.1592645 -0.5122149 -2.1989321\n [7] -1.2747658 -1.3612863  0.2225896  1.6848890\n\n# Here's the slow way to 1) Add five to every number in the vector; 2) round the numbers to one decimal place; 3) take the log of the numbers; and 4) take the mean of the resulting numbers. \n\n## First, we add five to every number in the vector\nx &lt;- x + 5\nprint(x)\n\n [1] 4.126942 4.741805 5.821623 5.159264 4.487785 2.801068 3.725234 3.638714\n [9] 5.222590 6.684889\n\n## Second, we round the vector to one decimal place.\nx &lt;- round(x, digits = 1)\nprint(x)\n\n [1] 4.1 4.7 5.8 5.2 4.5 2.8 3.7 3.6 5.2 6.7\n\n## Third, we take the log of each number.\nx &lt;- log(x)\nprint(x)\n\n [1] 1.410987 1.547563 1.757858 1.648659 1.504077 1.029619 1.308333 1.280934\n [9] 1.648659 1.902108\n\n## Take the mean of the resulting vector. \nprint(mean(x))\n\n[1] 1.50388\n\n# Let's now accomplish this the more efficient way with pipes!\n\ny &lt;- rnorm(10)\nprint(y)\n\n [1]  0.182280753  0.110722836  0.979960424  0.259777714  0.752809419\n [6] -0.498925101 -0.997183130  1.237221375 -0.563920628 -0.006867612\n\ny %&gt;%\n  `+`(5) %&gt;%              # Notice that arithmetic operators are surrounded by backticks `` and scalar values are surrounded by parentheses.\n  round(digits = 2) %&gt;%   # Only the second argument is needed, since the first argument is the vector being passed by the pipes.\n  log() %&gt;%\n  mean() %&gt;%\n  print()\n\n[1] 1.62932\n\n# The summary() function can also provide us with useful information very easily. This information is the interquartile range, including the median and mean.\ny %&gt;%\n  summary()\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-0.9972 -0.3759  0.1465  0.1456  0.6296  1.2372 \n\n\nWe can also use the pipe operator to manipulate dataframes, including operations with string vectors or columns. Importantly, note that the pipe operator puts the output from the left side as the first argument on the right side. However, this becomes a problem when we use a function whose first argument is not a dataframe or vector, such as the gsub() function. In such cases, you can put a period . in the place where the vector or dataframe should go. Let’s look at an example.\n\n# Let's create a small dataframe.\n\nbball &lt;- data.frame(team = c(\"San Antonio\",\n                             \"San Antonio\",\n                             \"New Orleans\",\n                             \"Washington\",\n                             \"Utah\",\n                             \"Milwaukee\",\n                             \"Cleveland\"),\n                    player_name = c(\"Romeo Langford\",\n                                    \"Jakov Poeltl\",\n                                    \"Dyson Daniels\",\n                                    \"Will Barton\",\n                                    \"Mike Conley\",\n                                    \"Joe Ingles\",\n                                    \"Raul Neto\"),\n                    points = c(2,\n                               23,\n                               35,\n                               23,\n                               8,\n                               33,\n                               28)\n                    )\n\n# Let's use the pipe operator to chain some functions together! The functions below change the team names and finally print the unique entries in the vector using the unique() function.\n\nbball$team %&gt;%\n  gsub(\"San Antonio\", \"San Antonio Spurs\", .) %&gt;%\n  gsub(\"New Orleans\", \"New Orleans Pelicans\", .) %&gt;%\n  gsub(\"Washington\", \"Washington Commanders\", .) %&gt;%\n  gsub(\"Utah\", \"Utah Jazz\", .) %&gt;%\n  gsub(\"Milwaukee\", \"Milwaukee Bucks\", .) %&gt;%\n  gsub(\"Cleveland\", \"Cleveland Cavaliers\", .) %&gt;%\n  unique()\n\n[1] \"San Antonio Spurs\"     \"New Orleans Pelicans\"  \"Washington Commanders\"\n[4] \"Utah Jazz\"             \"Milwaukee Bucks\"       \"Cleveland Cavaliers\""
  },
  {
    "objectID": "dataexp.html#the-dplyr-package",
    "href": "dataexp.html#the-dplyr-package",
    "title": "8  Data Exploration with the Tidyverse",
    "section": "8.2 The dplyr Package",
    "text": "8.2 The dplyr Package\nThe dplyr package is a great tool for organizing and manipulating our dataframe. Note, I use the term manipulate not to convey anything nefarious or unethical, but in the sense of data management. The functions of this package are named after useful verbs, making them relatively easy to remember. Some common dplyr functions are listed in Table 8.1.\n\n\nTable 8.1: dplyr’s useful functions with their description.\n\n\n\n\n\n\ndplyr Function\nDescription\n\n\n\n\narrange()\nChange the order of rows.\n\n\nfilter()\nSelect rows based on column values.\n\n\nmutate()\nChange the values in certain columns, and create new columns.\n\n\nrelocate()\nChange the order of columns.\n\n\nrename()\nChange the name of columns.\n\n\nselect()\nInclude or exclude a column.\n\n\nslice()\nSelect rows based on location.\n\n\nsummarise()\nCollapse a group into a single row.\n\n\ngroup_by()\nSelect a grouping variable to perform an operation by group.\n\n\n\n\nLet’s look at some examples with the built-in mtcars dataframe.\n\n# Load mtcars dataframe. Assign it to an object.\ndf &lt;- mtcars\n\n# Let's look at cars with only eight cylinders.\ndf %&gt;%\n  filter(cyl == 8)\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\n\n# Let's add to that, and order the dataframe by mpg in descending order (highest to lowest). To do this, we embed the desc() function within arrange(). For ascending order, no need for the desc() function.\n\ndf %&gt;%\n  filter(cyl == 8) %&gt;%\n  arrange(desc(mpg))\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\n\n# Now let's add a new column called kpl (kilometers per litre) using mutate(). A quick Google search tells me that going from mpg to kpl involves dividing mpg by 2.352.  \n\ndf %&gt;%\n  filter(cyl == 8) %&gt;%\n  arrange(desc(mpg)) %&gt;%\n  mutate(kpl = mpg / 2.352) %&gt;%\n  head()\n\n                   mpg cyl  disp  hp drat    wt  qsec vs am gear carb      kpl\nPontiac Firebird  19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2 8.163265\nHornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2 7.950680\nMerc 450SL        17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3 7.355442\nMerc 450SE        16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3 6.972789\nFord Pantera L    15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4 6.717687\nDodge Challenger  15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2 6.590136\n\n# Let's add a new name for the wt variable called weight.\ndf %&gt;%\n  filter(cyl == 8) %&gt;%\n  arrange(desc(mpg)) %&gt;%\n  mutate(kpl = mpg / 2.352) %&gt;%\n  rename(weight = wt) %&gt;%\n  head()\n\n                   mpg cyl  disp  hp drat weight  qsec vs am gear carb      kpl\nPontiac Firebird  19.2   8 400.0 175 3.08  3.845 17.05  0  0    3    2 8.163265\nHornet Sportabout 18.7   8 360.0 175 3.15  3.440 17.02  0  0    3    2 7.950680\nMerc 450SL        17.3   8 275.8 180 3.07  3.730 17.60  0  0    3    3 7.355442\nMerc 450SE        16.4   8 275.8 180 3.07  4.070 17.40  0  0    3    3 6.972789\nFord Pantera L    15.8   8 351.0 264 4.22  3.170 14.50  0  1    5    4 6.717687\nDodge Challenger  15.5   8 318.0 150 2.76  3.520 16.87  0  0    3    2 6.590136\n\n# Let's now only select a few variables using select()\n\ndf %&gt;%\n  select(mpg, cyl, wt) %&gt;%\n  head()\n\n                   mpg cyl    wt\nMazda RX4         21.0   6 2.620\nMazda RX4 Wag     21.0   6 2.875\nDatsun 710        22.8   4 2.320\nHornet 4 Drive    21.4   6 3.215\nHornet Sportabout 18.7   8 3.440\nValiant           18.1   6 3.460\n\n## If we want to select all but a few variables, we can still use select. Let's say I want all variables except mpg, cyl, and wt. I just need to add a minus before each variable name.\n\ndf %&gt;%\n  select(-mpg, -cyl, -wt)\n\n                     disp  hp drat  qsec vs am gear carb\nMazda RX4           160.0 110 3.90 16.46  0  1    4    4\nMazda RX4 Wag       160.0 110 3.90 17.02  0  1    4    4\nDatsun 710          108.0  93 3.85 18.61  1  1    4    1\nHornet 4 Drive      258.0 110 3.08 19.44  1  0    3    1\nHornet Sportabout   360.0 175 3.15 17.02  0  0    3    2\nValiant             225.0 105 2.76 20.22  1  0    3    1\nDuster 360          360.0 245 3.21 15.84  0  0    3    4\nMerc 240D           146.7  62 3.69 20.00  1  0    4    2\nMerc 230            140.8  95 3.92 22.90  1  0    4    2\nMerc 280            167.6 123 3.92 18.30  1  0    4    4\nMerc 280C           167.6 123 3.92 18.90  1  0    4    4\nMerc 450SE          275.8 180 3.07 17.40  0  0    3    3\nMerc 450SL          275.8 180 3.07 17.60  0  0    3    3\nMerc 450SLC         275.8 180 3.07 18.00  0  0    3    3\nCadillac Fleetwood  472.0 205 2.93 17.98  0  0    3    4\nLincoln Continental 460.0 215 3.00 17.82  0  0    3    4\nChrysler Imperial   440.0 230 3.23 17.42  0  0    3    4\nFiat 128             78.7  66 4.08 19.47  1  1    4    1\nHonda Civic          75.7  52 4.93 18.52  1  1    4    2\nToyota Corolla       71.1  65 4.22 19.90  1  1    4    1\nToyota Corona       120.1  97 3.70 20.01  1  0    3    1\nDodge Challenger    318.0 150 2.76 16.87  0  0    3    2\nAMC Javelin         304.0 150 3.15 17.30  0  0    3    2\nCamaro Z28          350.0 245 3.73 15.41  0  0    3    4\nPontiac Firebird    400.0 175 3.08 17.05  0  0    3    2\nFiat X1-9            79.0  66 4.08 18.90  1  1    4    1\nPorsche 914-2       120.3  91 4.43 16.70  0  1    5    2\nLotus Europa         95.1 113 3.77 16.90  1  1    5    2\nFord Pantera L      351.0 264 4.22 14.50  0  1    5    4\nFerrari Dino        145.0 175 3.62 15.50  0  1    5    6\nMaserati Bora       301.0 335 3.54 14.60  0  1    5    8\nVolvo 142E          121.0 109 4.11 18.60  1  1    4    2\n\n# If we want to select certain rows of a dataframe, we can do this with slice() by mentioning the index number of the columns. If we want to know the row number based on a column value (e.g. mpg &gt; 20), we can use the which() function where you can write the column, relational operator, and value.\n\nwhich(df$mpg &gt; 20)\n\n [1]  1  2  3  4  8  9 18 19 20 21 26 27 28 32\n\ndf %&gt;%\n  slice(1:4,\n        8,\n        9,\n        18:21,\n        26:28,\n        32) %&gt;%\n  print()\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nMerc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n## Often we want to know the highest and lowest values of a variable. We can use slice_min() for the lowest values and slice_max() for the highest values. The first argument is the column and the second is how many rows you want (e.g. n = 5).\n\n## The five cars with the lowest mpg.\n\ndf %&gt;%\n  slice_min(mpg, n = 5)\n\n                     mpg cyl disp  hp drat    wt  qsec vs am gear carb\nCadillac Fleetwood  10.4   8  472 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8  460 215 3.00 5.424 17.82  0  0    3    4\nCamaro Z28          13.3   8  350 245 3.73 3.840 15.41  0  0    3    4\nDuster 360          14.3   8  360 245 3.21 3.570 15.84  0  0    3    4\nChrysler Imperial   14.7   8  440 230 3.23 5.345 17.42  0  0    3    4\n\n## The five cars with the highest mpg.\n\ndf %&gt;%\n  slice_max(mpg, n = 5)\n\n                mpg cyl disp  hp drat    wt  qsec vs am gear carb\nToyota Corolla 33.9   4 71.1  65 4.22 1.835 19.90  1  1    4    1\nFiat 128       32.4   4 78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4 75.7  52 4.93 1.615 18.52  1  1    4    2\nLotus Europa   30.4   4 95.1 113 3.77 1.513 16.90  1  1    5    2\nFiat X1-9      27.3   4 79.0  66 4.08 1.935 18.90  1  1    4    1\n\n# If I want to change the order of how some columns appear in the dataframe, I can do so with relocate() where the first argument is the column(s) to move, and the second argument specifies the intended new location with either .before or .after to indicate where you want to place the columns. \n\n## Let's move wt to the front.\n\ndf %&gt;%\n  relocate(wt, .before = everything()\n           )\n\n                       wt  mpg cyl  disp  hp drat  qsec vs am gear carb\nMazda RX4           2.620 21.0   6 160.0 110 3.90 16.46  0  1    4    4\nMazda RX4 Wag       2.875 21.0   6 160.0 110 3.90 17.02  0  1    4    4\nDatsun 710          2.320 22.8   4 108.0  93 3.85 18.61  1  1    4    1\nHornet 4 Drive      3.215 21.4   6 258.0 110 3.08 19.44  1  0    3    1\nHornet Sportabout   3.440 18.7   8 360.0 175 3.15 17.02  0  0    3    2\nValiant             3.460 18.1   6 225.0 105 2.76 20.22  1  0    3    1\nDuster 360          3.570 14.3   8 360.0 245 3.21 15.84  0  0    3    4\nMerc 240D           3.190 24.4   4 146.7  62 3.69 20.00  1  0    4    2\nMerc 230            3.150 22.8   4 140.8  95 3.92 22.90  1  0    4    2\nMerc 280            3.440 19.2   6 167.6 123 3.92 18.30  1  0    4    4\nMerc 280C           3.440 17.8   6 167.6 123 3.92 18.90  1  0    4    4\nMerc 450SE          4.070 16.4   8 275.8 180 3.07 17.40  0  0    3    3\nMerc 450SL          3.730 17.3   8 275.8 180 3.07 17.60  0  0    3    3\nMerc 450SLC         3.780 15.2   8 275.8 180 3.07 18.00  0  0    3    3\nCadillac Fleetwood  5.250 10.4   8 472.0 205 2.93 17.98  0  0    3    4\nLincoln Continental 5.424 10.4   8 460.0 215 3.00 17.82  0  0    3    4\nChrysler Imperial   5.345 14.7   8 440.0 230 3.23 17.42  0  0    3    4\nFiat 128            2.200 32.4   4  78.7  66 4.08 19.47  1  1    4    1\nHonda Civic         1.615 30.4   4  75.7  52 4.93 18.52  1  1    4    2\nToyota Corolla      1.835 33.9   4  71.1  65 4.22 19.90  1  1    4    1\nToyota Corona       2.465 21.5   4 120.1  97 3.70 20.01  1  0    3    1\nDodge Challenger    3.520 15.5   8 318.0 150 2.76 16.87  0  0    3    2\nAMC Javelin         3.435 15.2   8 304.0 150 3.15 17.30  0  0    3    2\nCamaro Z28          3.840 13.3   8 350.0 245 3.73 15.41  0  0    3    4\nPontiac Firebird    3.845 19.2   8 400.0 175 3.08 17.05  0  0    3    2\nFiat X1-9           1.935 27.3   4  79.0  66 4.08 18.90  1  1    4    1\nPorsche 914-2       2.140 26.0   4 120.3  91 4.43 16.70  0  1    5    2\nLotus Europa        1.513 30.4   4  95.1 113 3.77 16.90  1  1    5    2\nFord Pantera L      3.170 15.8   8 351.0 264 4.22 14.50  0  1    5    4\nFerrari Dino        2.770 19.7   6 145.0 175 3.62 15.50  0  1    5    6\nMaserati Bora       3.570 15.0   8 301.0 335 3.54 14.60  0  1    5    8\nVolvo 142E          2.780 21.4   4 121.0 109 4.11 18.60  1  1    4    2\n\n## Let's move wt to before disp.\n\ndf %&gt;%\n  relocate(wt, .before = disp)\n\n                     mpg cyl    wt  disp  hp drat  qsec vs am gear carb\nMazda RX4           21.0   6 2.620 160.0 110 3.90 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 2.875 160.0 110 3.90 17.02  0  1    4    4\nDatsun 710          22.8   4 2.320 108.0  93 3.85 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 3.215 258.0 110 3.08 19.44  1  0    3    1\nHornet Sportabout   18.7   8 3.440 360.0 175 3.15 17.02  0  0    3    2\nValiant             18.1   6 3.460 225.0 105 2.76 20.22  1  0    3    1\nDuster 360          14.3   8 3.570 360.0 245 3.21 15.84  0  0    3    4\nMerc 240D           24.4   4 3.190 146.7  62 3.69 20.00  1  0    4    2\nMerc 230            22.8   4 3.150 140.8  95 3.92 22.90  1  0    4    2\nMerc 280            19.2   6 3.440 167.6 123 3.92 18.30  1  0    4    4\nMerc 280C           17.8   6 3.440 167.6 123 3.92 18.90  1  0    4    4\nMerc 450SE          16.4   8 4.070 275.8 180 3.07 17.40  0  0    3    3\nMerc 450SL          17.3   8 3.730 275.8 180 3.07 17.60  0  0    3    3\nMerc 450SLC         15.2   8 3.780 275.8 180 3.07 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 5.250 472.0 205 2.93 17.98  0  0    3    4\nLincoln Continental 10.4   8 5.424 460.0 215 3.00 17.82  0  0    3    4\nChrysler Imperial   14.7   8 5.345 440.0 230 3.23 17.42  0  0    3    4\nFiat 128            32.4   4 2.200  78.7  66 4.08 19.47  1  1    4    1\nHonda Civic         30.4   4 1.615  75.7  52 4.93 18.52  1  1    4    2\nToyota Corolla      33.9   4 1.835  71.1  65 4.22 19.90  1  1    4    1\nToyota Corona       21.5   4 2.465 120.1  97 3.70 20.01  1  0    3    1\nDodge Challenger    15.5   8 3.520 318.0 150 2.76 16.87  0  0    3    2\nAMC Javelin         15.2   8 3.435 304.0 150 3.15 17.30  0  0    3    2\nCamaro Z28          13.3   8 3.840 350.0 245 3.73 15.41  0  0    3    4\nPontiac Firebird    19.2   8 3.845 400.0 175 3.08 17.05  0  0    3    2\nFiat X1-9           27.3   4 1.935  79.0  66 4.08 18.90  1  1    4    1\nPorsche 914-2       26.0   4 2.140 120.3  91 4.43 16.70  0  1    5    2\nLotus Europa        30.4   4 1.513  95.1 113 3.77 16.90  1  1    5    2\nFord Pantera L      15.8   8 3.170 351.0 264 4.22 14.50  0  1    5    4\nFerrari Dino        19.7   6 2.770 145.0 175 3.62 15.50  0  1    5    6\nMaserati Bora       15.0   8 3.570 301.0 335 3.54 14.60  0  1    5    8\nVolvo 142E          21.4   4 2.780 121.0 109 4.11 18.60  1  1    4    2\n\n## Let's move wt to after qsec\n\ndf %&gt;%\n  relocate(wt, .after = qsec)\n\n                     mpg cyl  disp  hp drat  qsec    wt vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 16.46 2.620  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 17.02 2.875  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 18.61 2.320  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 19.44 3.215  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 17.02 3.440  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 20.22 3.460  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 15.84 3.570  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 20.00 3.190  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 22.90 3.150  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 18.30 3.440  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 18.90 3.440  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 17.40 4.070  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 17.60 3.730  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 18.00 3.780  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 17.98 5.250  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 17.82 5.424  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 17.42 5.345  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 19.47 2.200  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 18.52 1.615  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 19.90 1.835  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 20.01 2.465  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 16.87 3.520  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 17.30 3.435  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 15.41 3.840  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 17.05 3.845  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 18.90 1.935  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 16.70 2.140  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 16.90 1.513  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 14.50 3.170  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 15.50 2.770  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 14.60 3.570  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 18.60 2.780  1  1    4    2\n\n# Let's use the summarise() and group_by() functions to get a summary of the weight of each car (wt) grouped by cylinders (cyl), and rounded to two decimal places.\n\ndf %&gt;%\n  group_by(cyl) %&gt;%\n  summarise(mean_weight = mean(wt)) %&gt;%\n  round(digits = 2)\n\n# A tibble: 3 × 2\n    cyl mean_weight\n  &lt;dbl&gt;       &lt;dbl&gt;\n1     4        2.29\n2     6        3.12\n3     8        4   \n\n\nThus, we can see that the dplyr package offers several useful functions to manage our data. Remember, that if you want any changes to be reflected in your dataframe, such as renaming a variable, remember to assign your dplyr code to your dataframe. For example, if I want the name change for wt to weight to stick, I would assign that to the dataframe like this:\n\ndf &lt;- df %&gt;%\n  rename(weight = wt)"
  },
  {
    "objectID": "dataexp.html#sec-tidyex",
    "href": "dataexp.html#sec-tidyex",
    "title": "9  Data Exploration with dplyr",
    "section": "9.5 Exercises",
    "text": "9.5 Exercises\nAs always, it’s a good idea to attempt these while the material is still fresh. You can find the answers in Appendix E.\n\nLoad the tidyverse package. Then assign the built-in dataframe starwars to an object named whatever you want. Then subset the dataframe by human species only. Save the subsetted dataframe as an object called swhuman. Then calculate and report the mean and median height in swhuman. Also report an NAs (missing data) in the height variable. Note: the units for the height variable are centimeters.\nHopefully you noticed that there are indeed some NAs in the swhuman dataframe! Detect which rows have NAs for the height variable, and write the names of the characters that have this. Next, let’s fix these errors. Perform an internet search and populate those NAs with plausible values. If you need to convert from feet to centimeters, multiply the value in feet by 30.48. If you absolutely cannot find the height of any character substitute the median height from Question 1 for their height.\nOnce you have filled in this missing data, calculate the new mean and median for the height variable. Comment on how much of a difference the additional values made on the mean and median compared with the values you calculated in Question 1. Then determine the three shortest characters, and three tallest characters.\nReturn to the larger starwars dataframe or whatever object to which you assigned it. Determine which characters have NA for height. If there are any characters with NA for height (hint: there are), enter plausible values for their heights using the approach taken in Question 2. Then report the mean and median height across everyone in this dataframe.\nStill working with the starwars dataframe, convert the species variable to factor. Then, group and summarise the mean height by species, and print this in descending order. Report which species is the tallest, on average. Then, rearrange and report the species which is the shortest, on average."
  },
  {
    "objectID": "app6.html",
    "href": "app6.html",
    "title": "Appendix E — Answers for Section 9.3",
    "section": "",
    "text": "The following are answers to the exercises in Section 9.3.\n\nLoad the tidyverse package. Then assign the built-in dataframe starwars to an object named whatever you want. Then subset the dataframe by human species only. Save the subsetted dataframe as an object called swhuman. Then calculate and report the mean and median height in swhuman. Also report an NAs (missing data) in the height variable. Note: the units for the height variable are centimeters.\n\n\nlibrary(tidyverse)\n\nstardf &lt;- starwars\n\nswhuman &lt;- stardf %&gt;%\n  filter(species == \"Human\")\n\nswhuman %&gt;%\n  select(height) %&gt;%\n  summary()\n\n     height     \n Min.   :150.0  \n 1st Qu.:168.5  \n Median :180.0  \n Mean   :176.6  \n 3rd Qu.:184.0  \n Max.   :202.0  \n NA's   :4      \n\n# The mean height is 176.6 centimeters, and the median height is 180 centimeters.There are 4 NAs.\n\n\nHopefully you noticed that there are indeed some NAs in the swhuman dataframe! Detect which rows have NAs for the height variable, and write the names of the characters that have this. Next, let’s fix these errors. Perform an internet search and populate those NAs with plausible values. If you need to convert from feet to centimeters, multiply the value in feet by 30.48. If you absolutely cannot find the height of any character substitute the median height from Question 1 for their height.\n\n\n# Which rows have NA for the height variable?\n\nwhich(is.na(swhuman$height))\n\n[1] 18 32 33 34\n\nswhuman %&gt;%\n  slice(18,\n        32,\n        33,\n        34) %&gt;%\n  print()\n\n# A tibble: 4 × 14\n  name      height  mass hair_color skin_color eye_color birth_year sex   gender\n  &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 Arvel Cr…     NA    NA brown      fair       brown             NA male  mascu…\n2 Finn          NA    NA black      dark       dark              NA male  mascu…\n3 Rey           NA    NA brown      light      hazel             NA fema… femin…\n4 Poe Dame…     NA    NA brown      light      brown             NA male  mascu…\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n## It looks like Arvel Crynyd, Finn, Rey, and Poe Dameron all have NA values for height. From a quick Google search, I found heights of Finn, Rey, and Poe Dameron but not for Arvel Crynyd. Thus, I will enter a value of 180 (median from Question 1) for Arvel Crynyd. For Finn, Rey, and Poe Dameron, I will enter 176.8, 170.7, and 172, respectively. \n\nswhuman$height[18] &lt;- 180\n\nswhuman$height[32:34] &lt;- c(176.8, 170.7, 172)\n\n\nOnce you have filled in this missing data, calculate the new mean and median for the height variable. Comment on how much of a difference the additional values made on the mean and median compared with the values you calculated in Question 1. Then determine the three shortest characters, and three tallest characters.\n\n\nsummary(swhuman$height)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  150.0   170.0   178.0   176.4   183.0   202.0 \n\n# The new mean is 176.4. This is 0.2 centimeters less than the mean from Question 1.\n# The new median is 178. This is 2 centimeters less than the median from Question 1.\n\n# Next, let's print the three shortest and three tallest characters.\n\nswhuman %&gt;%\n  slice_min(height, n = 3)\n\n# A tibble: 3 × 14\n  name      height  mass hair_color skin_color eye_color birth_year sex   gender\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 Leia Org…    150    49 brown      light      brown             19 fema… femin…\n2 Mon Moth…    150    NA auburn     fair       blue              48 fema… femin…\n3 Cordé        157    NA brown      light      brown             NA fema… femin…\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\nswhuman %&gt;%\n  slice_max(height, n = 3)\n\n# A tibble: 3 × 14\n  name      height  mass hair_color skin_color eye_color birth_year sex   gender\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 Darth Va…    202   136 none       white      yellow          41.9 male  mascu…\n2 Qui-Gon …    193    89 brown      fair       blue            92   male  mascu…\n3 Dooku        193    80 white      fair       brown          102   male  mascu…\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n## The three shortest characters are Leia Organa, Mon Mothma, and Cordé.\n## The three tallest characters are Darth Vader, Qui-Gon Jinn, and Dooku.\n\n\nReturn to the larger starwars dataframe or whatever object to which you assigned it. Determine which characters have NA for height. If there are any characters with NA for height (hint: there are), enter plausible values for their heights using the approach taken in Question 2. Then report the mean and median height across everyone in this dataframe.\n\n\n# Both approaches tell you the rows with NA in height. Use the approach you like.\n\nwhich(is.na(stardf$height))\n\n[1] 28 82 83 84 85 86\n\nstardf$height %&gt;%\n  is.na() %&gt;%\n  which()\n\n[1] 28 82 83 84 85 86\n\n# Determine which characters have NA for height.\n\nstardf %&gt;%\n  slice(28,\n        82:86) %&gt;%\n  print()\n\n# A tibble: 6 × 14\n  name      height  mass hair_color skin_color eye_color birth_year sex   gender\n  &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 Arvel Cr…     NA    NA brown      fair       brown             NA male  mascu…\n2 Finn          NA    NA black      dark       dark              NA male  mascu…\n3 Rey           NA    NA brown      light      hazel             NA fema… femin…\n4 Poe Dame…     NA    NA brown      light      brown             NA male  mascu…\n5 BB8           NA    NA none       none       black             NA none  mascu…\n6 Captain …     NA    NA unknown    unknown    unknown           NA &lt;NA&gt;  &lt;NA&gt;  \n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n# I already have heights for the first four. BB8's height is 67.1, and Captain Phasma's height is 200.1.\n\nstardf$height[28] &lt;- 180\n\nstardf$height[82:86] &lt;- c(176.8, 170.7, 172, 67.1, 200.1)\n\n# Now calculate mean and median height.\n\nsummary(stardf$height)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   66.0   167.0   180.0   173.4   191.0   264.0 \n\n## The mean height is 173.4 cm and the median height is 180 cm.\n\n\nStill working with the starwars dataframe, convert the species variable to factor. Then, group and summarise the mean height by species, and print this in descending order. Report which species is the tallest, on average. Then, rearrange and report the species which is the shortest, on average.\n\n\n# Convert species to factor\n\nstardf$species &lt;- as.factor(stardf$species)\n\n# Which is the tallest species, on average?\n\nstardf %&gt;%\n  group_by(species) %&gt;%\n  summarise(mean_height = mean(height)) %&gt;%\n  arrange(desc(mean_height))\n\n# A tibble: 38 × 2\n   species  mean_height\n   &lt;fct&gt;          &lt;dbl&gt;\n 1 Quermian        264 \n 2 Wookiee         231 \n 3 Kaminoan        221 \n 4 Kaleesh         216 \n 5 Gungan          209.\n 6 Pau'an          206 \n 7 Besalisk        198 \n 8 Cerean          198 \n 9 Chagrian        196 \n10 Nautolan        196 \n# ℹ 28 more rows\n\n## The Quermian species is the tallest on average.\n\n# Which is the shortest species on average?\n\nstardf %&gt;%\n  group_by(species) %&gt;%\n  summarise(mean_height = mean(height)) %&gt;%\n  arrange(mean_height)\n\n# A tibble: 38 × 2\n   species        mean_height\n   &lt;fct&gt;                &lt;dbl&gt;\n 1 Yoda's species         66 \n 2 Aleena                 79 \n 3 Ewok                   88 \n 4 Vulptereen             94 \n 5 Dug                   112 \n 6 Droid                 121.\n 7 Xexto                 122 \n 8 Toydarian             137 \n 9 Sullustan             160 \n10 Toong                 163 \n# ℹ 28 more rows\n\n## Yoda's species is the shortest on average."
  },
  {
    "objectID": "syllabus.html#sec-outline",
    "href": "syllabus.html#sec-outline",
    "title": "Syllabus",
    "section": "Outline of classes",
    "text": "Outline of classes\nBelow is an outline of our course material over the semester. Please note that the Assignments are things you should complete BEFORE class. The Chapters listed under Assignments refer to sections of this e-textbook. While we have a defined curriculum and textbook for this course, there is some room to tailor some of the class content to your interests. So, if you have something you want to learn in R, please let me know! If we cannot get to it in class, I will provide resources for you to learn more, and you can incorporate any special topics into presentations.\n\n\n\n\n\n\n\n\nDate\nTopics and Events\nAssignments\n\n\n\n\nAugust 31\n\nResearch Transparency and Reproducibility (RT2) Story: Bolivian National Election.\nRT2 Story: Hydroxychloroquine.\nIntroduction to Base R and RStudio (arithmetic operations, objects, functions, packages, R Studio).\nIntroduction to RT2 - what it is and a brief history of replication.\n\n\nSuccessful installation of R and R Studio.\nChapters 1 - 3 and 21.\nChapter 3 exercises (optional).\n\n\n\nSeptember 7\n\nRT2 Story: A nutrition researcher’s arrogance becomes his downfall.\nDiscussion board post discussion.\nData types (vectors, matrices, dataframes, lists, arrays).\nMertonian norms and counter-norms of science.\n\n\nChapters 4 and 22.\nChapter 3 and 4 exercises.\nDiscussion board post.\n\n\n\nSeptember 14\n\nRT2 Story: Worm Wars.\nDiscussion board post discussion.\nWorking with strings and comparisons (including conditional statements to evaluate comparisons).\nStatistical preliminaries (hypothesis testing, p-values, power, confidence intervals, sample statistics and population parameters).\n\n\nChapters 5 and 11.\nChapter 5 exercises.\nDiscussion board post.\n\n\n\nSeptember 21\nQUIZ #1 in class (covers material from Aug 31, Sept 7, and Sep 14).\n\nRT2 Story: A dishonesty researcher is allegedly dishonest.\nDiscussion board post discussion.\nImporting data.\nIntroduction to data cleaning (folder structure, detecting illogical and extreme values, subsetting, variable transformation, data export).\nCrises of replication and the domino effect.\n\n\nChapters 6, 7, and 23.\nDiscussion board post.\n\n\n\nSeptember 28\n\nRT2 Story: Another dishonesty researcher is allegedly dishonest!\nDiscussion board post discussion.\nData cleaning continued.\nIntroduction to the tidyverse (esp. dplyr for data management and exploration)\nMitigating researcher degrees of freedom.\n\n\nChapters 7 and 8.\nChapter 7 exercises.\nChapter 8 exercises (optional).\nDiscussion board post.\n\n\n\nOctober 5\n\nRT2 Story: Fertility research + data integrity = terrorism?\nDiscussion board post discussion.\nPossible guest lecture: Daniel Putman, PhD on Many Economists project.\nData visualization (base R, ggplot2 and some related packages).\nMitigating researcher degrees of freedom continued.\n\n\nChapter 9.\nChapter 8 exercises.\nChapter 9 exercises (optional).\nHadley, W. (2014). Tidy data. Journal of Statistical Software, 59(10), 1-23.\nDiscussion board post.\n\n\n\nOctober 12\nFall Break (No Class)\n\nEnjoy Fall Break and youth, in general.\n\n\n\nOctober 19\nQUIZ #2 in class (covers material from Sep 21, Sep 28, and Oct 5).\n\nRT2 Story: A student newspaper brings down a president.\nDiscussion board post discussion.\nUnivariate and bivariate analyses (descriptive statistics, correlations, related plots).\n\n\nChapters 12 and 13.\nChapter 9 exercises.\nChapter 12 and 13 exercises (optional).\nDiscussion board post.\n\n\n\nOctober 26\n\nRT2 Story: What the heck is going on with Anesthesia research?!\nDiscussion board post discussion.\nIntroduction to probability (basics of probability theory, common distributions, Bayesian vs Frequentist paradigms).\n\n\nChapter 14.\nChapter 12 and 13 exercises.\nDiscussion board post.\n\n\n\nNovember 2\n\nRT2 Story: Paper mills and predatory journals.\nDiscussion board post discussion.\nCategorical data analysis, comparing \\(n\\) means (t-tests, ANOVA), non-parametric tests.\nRandom assignment.\n\n\nChapters 15 and 16.\nChapter 15 and 16 exercises (optional).\nDiscussion board post.\n\n\n\nNovember 9\nStudent Presentations - Round 1\n\nNo additional work beyond preparing for your presentation.\n\n\n\nNovember 16\nQUIZ #3 in class (covers material from Oct 19, Oct 26, and Nov 2).\n\nDiscussion board post discussion.\nLinear regression and ANCOVA.\nPossible guest lecture: Pei-Hsun Hsieh, PhD on simple scraping and text analysis.\nTabulating and preparing results with R Markdown.\n\n\nChapters 10 and 17.\nChapter 10 and 17 exercises (optional).\nChapter 15 and 16 exercises.\nDiscussion board post.\n\n\n\nNovember 21 (Tuesday)\nThur-Fri class schedule on Tue-Wed.\n\nDiscussion board post discussion.\nLogistic regression and interactions.\nMore on R Markdown (maybe even a little Quarto).\n\n\nChapters 18 and 19.\nChapter 10 and 17 exercises.\nChapter 18 and 19 exercises (optional).\nDiscussion board post.\n\n\n\nNovember 30\nQUIZ #4 in class (covers material from Nov 16, Nov 21, and Nov 30).\n\nDiscussion board post discussion.\nIntroduction to Bayesian inference (analysis of contingency tables, t-tests, ANOVAs, and regressions).\n\nStudents who wish to improve their grade can do another RT2 presentation on this day.\n\nChapter 20.\nChapter 18 and 19 exercises.\nDiscussion board post.\n\n\n\nDecember 7\nLast Day of Class.\nStudent Presentations - Round 2.\n\nNo additional work beyond preparing for your presentation."
  },
  {
    "objectID": "dataexp.html#exploring-data-with-dplyr",
    "href": "dataexp.html#exploring-data-with-dplyr",
    "title": "9  Data Exploration",
    "section": "9.2 Exploring Data with dplyr",
    "text": "9.2 Exploring Data with dplyr\nThe dplyr package is a great tool for organizing and manipulating our dataframe. Note, I use the term manipulate not to convey anything nefarious or unethical, but in the sense of data management. The functions of this package are named after useful verbs, making them relatively easy to remember. Some common dplyr functions are listed in Table 9.1.\n\n\nTable 9.1: dplyr’s useful functions with their description.\n\n\n\n\n\n\ndplyr Function\nDescription\n\n\n\n\narrange()\nChange the order of rows.\n\n\nfilter()\nSelect rows based on column values.\n\n\nmutate()\nChange the values in certain columns, and create new columns.\n\n\nrelocate()\nChange the order of columns.\n\n\nrename()\nChange the name of columns.\n\n\nselect()\nInclude or exclude a column.\n\n\nslice()\nSelect rows based on location.\n\n\nsummarise()\nCollapse a group into a single row.\n\n\ngroup_by()\nSelect a grouping variable to perform an operation by group.\n\n\n\n\nLet’s look at some examples with the built-in mtcars dataframe.\n\n9.2.1 Filter() & Arrange()\n\n# Load mtcars dataframe. Assign it to an object.\ndf &lt;- mtcars\n\n# Let's look at cars with only eight cylinders.\ndf %&gt;%\n  filter(cyl == 8)\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\n\n# Let's add to that, and order the dataframe by mpg in descending order (highest to lowest). To do this, we embed the desc() function within arrange(). For ascending order, no need for the desc() function.\n\ndf %&gt;%\n  filter(cyl == 8) %&gt;%\n  arrange(desc(mpg))\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\n\n\n\n\n9.2.2 Mutate() & Rename()\n\n# Now let's add a new column called kpl (kilometers per litre) using mutate(). A quick Google search tells me that going from mpg to kpl involves dividing mpg by 2.352.  \n\ndf %&gt;%\n  filter(cyl == 8) %&gt;%\n  arrange(desc(mpg)) %&gt;%\n  mutate(kpl = mpg / 2.352) %&gt;%\n  head()\n\n                   mpg cyl  disp  hp drat    wt  qsec vs am gear carb      kpl\nPontiac Firebird  19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2 8.163265\nHornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2 7.950680\nMerc 450SL        17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3 7.355442\nMerc 450SE        16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3 6.972789\nFord Pantera L    15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4 6.717687\nDodge Challenger  15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2 6.590136\n\n# Let's add a new name for the wt variable called weight.\ndf %&gt;%\n  filter(cyl == 8) %&gt;%\n  arrange(desc(mpg)) %&gt;%\n  mutate(kpl = mpg / 2.352) %&gt;%\n  rename(weight = wt) %&gt;%\n  head()\n\n                   mpg cyl  disp  hp drat weight  qsec vs am gear carb      kpl\nPontiac Firebird  19.2   8 400.0 175 3.08  3.845 17.05  0  0    3    2 8.163265\nHornet Sportabout 18.7   8 360.0 175 3.15  3.440 17.02  0  0    3    2 7.950680\nMerc 450SL        17.3   8 275.8 180 3.07  3.730 17.60  0  0    3    3 7.355442\nMerc 450SE        16.4   8 275.8 180 3.07  4.070 17.40  0  0    3    3 6.972789\nFord Pantera L    15.8   8 351.0 264 4.22  3.170 14.50  0  1    5    4 6.717687\nDodge Challenger  15.5   8 318.0 150 2.76  3.520 16.87  0  0    3    2 6.590136\n\n\n\n\n9.2.3 Select() & Slice()\n\n# Let's now only select a few variables using select()\n\ndf %&gt;%\n  select(mpg, cyl, wt) %&gt;%\n  head()\n\n                   mpg cyl    wt\nMazda RX4         21.0   6 2.620\nMazda RX4 Wag     21.0   6 2.875\nDatsun 710        22.8   4 2.320\nHornet 4 Drive    21.4   6 3.215\nHornet Sportabout 18.7   8 3.440\nValiant           18.1   6 3.460\n\n## If we want to select all but a few variables, we can still use select. Let's say I want all variables except mpg, cyl, and wt. I just need to add a minus before each variable name.\n\ndf %&gt;%\n  select(-mpg, -cyl, -wt)\n\n                     disp  hp drat  qsec vs am gear carb\nMazda RX4           160.0 110 3.90 16.46  0  1    4    4\nMazda RX4 Wag       160.0 110 3.90 17.02  0  1    4    4\nDatsun 710          108.0  93 3.85 18.61  1  1    4    1\nHornet 4 Drive      258.0 110 3.08 19.44  1  0    3    1\nHornet Sportabout   360.0 175 3.15 17.02  0  0    3    2\nValiant             225.0 105 2.76 20.22  1  0    3    1\nDuster 360          360.0 245 3.21 15.84  0  0    3    4\nMerc 240D           146.7  62 3.69 20.00  1  0    4    2\nMerc 230            140.8  95 3.92 22.90  1  0    4    2\nMerc 280            167.6 123 3.92 18.30  1  0    4    4\nMerc 280C           167.6 123 3.92 18.90  1  0    4    4\nMerc 450SE          275.8 180 3.07 17.40  0  0    3    3\nMerc 450SL          275.8 180 3.07 17.60  0  0    3    3\nMerc 450SLC         275.8 180 3.07 18.00  0  0    3    3\nCadillac Fleetwood  472.0 205 2.93 17.98  0  0    3    4\nLincoln Continental 460.0 215 3.00 17.82  0  0    3    4\nChrysler Imperial   440.0 230 3.23 17.42  0  0    3    4\nFiat 128             78.7  66 4.08 19.47  1  1    4    1\nHonda Civic          75.7  52 4.93 18.52  1  1    4    2\nToyota Corolla       71.1  65 4.22 19.90  1  1    4    1\nToyota Corona       120.1  97 3.70 20.01  1  0    3    1\nDodge Challenger    318.0 150 2.76 16.87  0  0    3    2\nAMC Javelin         304.0 150 3.15 17.30  0  0    3    2\nCamaro Z28          350.0 245 3.73 15.41  0  0    3    4\nPontiac Firebird    400.0 175 3.08 17.05  0  0    3    2\nFiat X1-9            79.0  66 4.08 18.90  1  1    4    1\nPorsche 914-2       120.3  91 4.43 16.70  0  1    5    2\nLotus Europa         95.1 113 3.77 16.90  1  1    5    2\nFord Pantera L      351.0 264 4.22 14.50  0  1    5    4\nFerrari Dino        145.0 175 3.62 15.50  0  1    5    6\nMaserati Bora       301.0 335 3.54 14.60  0  1    5    8\nVolvo 142E          121.0 109 4.11 18.60  1  1    4    2\n\n# If we want to select certain rows of a dataframe, we can do this with slice() by mentioning the index number of the columns. If we want to know the row number based on a column value (e.g. mpg &gt; 20), we can use the which() function where you can write the column, relational operator, and value.\n\nwhich(df$mpg &gt; 20)\n\n [1]  1  2  3  4  8  9 18 19 20 21 26 27 28 32\n\ndf %&gt;%\n  slice(1:4,\n        8,\n        9,\n        18:21,\n        26:28,\n        32) %&gt;%\n  print()\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nMerc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n# Similarly, if you want to know which rows have the value 'NA' which indicates missing data for a particular column, we can wrap the is.na() function within the which() function like this.\n\n## Let's add some NAs to the disp variable first.\n\ndf[c(5,8,21:22), 3] &lt;- NA\n\n## Now we will see which rows in disp have NA. \n\nwhich(is.na(df$disp))\n\n[1]  5  8 21 22\n\n## Often we want to know the highest and lowest values of a variable. We can use slice_min() for the lowest values and slice_max() for the highest values. The first argument is the column and the second is how many rows you want (e.g. n = 5).\n\n## The five cars with the lowest mpg.\n\ndf %&gt;%\n  slice_min(mpg, n = 5)\n\n                     mpg cyl disp  hp drat    wt  qsec vs am gear carb\nCadillac Fleetwood  10.4   8  472 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8  460 215 3.00 5.424 17.82  0  0    3    4\nCamaro Z28          13.3   8  350 245 3.73 3.840 15.41  0  0    3    4\nDuster 360          14.3   8  360 245 3.21 3.570 15.84  0  0    3    4\nChrysler Imperial   14.7   8  440 230 3.23 5.345 17.42  0  0    3    4\n\n## The five cars with the highest mpg.\n\ndf %&gt;%\n  slice_max(mpg, n = 5)\n\n                mpg cyl disp  hp drat    wt  qsec vs am gear carb\nToyota Corolla 33.9   4 71.1  65 4.22 1.835 19.90  1  1    4    1\nFiat 128       32.4   4 78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4 75.7  52 4.93 1.615 18.52  1  1    4    2\nLotus Europa   30.4   4 95.1 113 3.77 1.513 16.90  1  1    5    2\nFiat X1-9      27.3   4 79.0  66 4.08 1.935 18.90  1  1    4    1\n\n\n\n\n9.2.4 Relocate() and Summarise()\n\n# If I want to change the order of how some columns appear in the dataframe, I can do so with relocate() where the first argument is the column(s) to move, and the second argument specifies the intended new location with either .before or .after to indicate where you want to place the columns. \n\n## Let's move wt to the front.\n\ndf %&gt;%\n  relocate(wt, .before = everything()\n           )\n\n                       wt  mpg cyl  disp  hp drat  qsec vs am gear carb\nMazda RX4           2.620 21.0   6 160.0 110 3.90 16.46  0  1    4    4\nMazda RX4 Wag       2.875 21.0   6 160.0 110 3.90 17.02  0  1    4    4\nDatsun 710          2.320 22.8   4 108.0  93 3.85 18.61  1  1    4    1\nHornet 4 Drive      3.215 21.4   6 258.0 110 3.08 19.44  1  0    3    1\nHornet Sportabout   3.440 18.7   8    NA 175 3.15 17.02  0  0    3    2\nValiant             3.460 18.1   6 225.0 105 2.76 20.22  1  0    3    1\nDuster 360          3.570 14.3   8 360.0 245 3.21 15.84  0  0    3    4\nMerc 240D           3.190 24.4   4    NA  62 3.69 20.00  1  0    4    2\nMerc 230            3.150 22.8   4 140.8  95 3.92 22.90  1  0    4    2\nMerc 280            3.440 19.2   6 167.6 123 3.92 18.30  1  0    4    4\nMerc 280C           3.440 17.8   6 167.6 123 3.92 18.90  1  0    4    4\nMerc 450SE          4.070 16.4   8 275.8 180 3.07 17.40  0  0    3    3\nMerc 450SL          3.730 17.3   8 275.8 180 3.07 17.60  0  0    3    3\nMerc 450SLC         3.780 15.2   8 275.8 180 3.07 18.00  0  0    3    3\nCadillac Fleetwood  5.250 10.4   8 472.0 205 2.93 17.98  0  0    3    4\nLincoln Continental 5.424 10.4   8 460.0 215 3.00 17.82  0  0    3    4\nChrysler Imperial   5.345 14.7   8 440.0 230 3.23 17.42  0  0    3    4\nFiat 128            2.200 32.4   4  78.7  66 4.08 19.47  1  1    4    1\nHonda Civic         1.615 30.4   4  75.7  52 4.93 18.52  1  1    4    2\nToyota Corolla      1.835 33.9   4  71.1  65 4.22 19.90  1  1    4    1\nToyota Corona       2.465 21.5   4    NA  97 3.70 20.01  1  0    3    1\nDodge Challenger    3.520 15.5   8    NA 150 2.76 16.87  0  0    3    2\nAMC Javelin         3.435 15.2   8 304.0 150 3.15 17.30  0  0    3    2\nCamaro Z28          3.840 13.3   8 350.0 245 3.73 15.41  0  0    3    4\nPontiac Firebird    3.845 19.2   8 400.0 175 3.08 17.05  0  0    3    2\nFiat X1-9           1.935 27.3   4  79.0  66 4.08 18.90  1  1    4    1\nPorsche 914-2       2.140 26.0   4 120.3  91 4.43 16.70  0  1    5    2\nLotus Europa        1.513 30.4   4  95.1 113 3.77 16.90  1  1    5    2\nFord Pantera L      3.170 15.8   8 351.0 264 4.22 14.50  0  1    5    4\nFerrari Dino        2.770 19.7   6 145.0 175 3.62 15.50  0  1    5    6\nMaserati Bora       3.570 15.0   8 301.0 335 3.54 14.60  0  1    5    8\nVolvo 142E          2.780 21.4   4 121.0 109 4.11 18.60  1  1    4    2\n\n## Let's move wt to before disp.\n\ndf %&gt;%\n  relocate(wt, .before = disp)\n\n                     mpg cyl    wt  disp  hp drat  qsec vs am gear carb\nMazda RX4           21.0   6 2.620 160.0 110 3.90 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 2.875 160.0 110 3.90 17.02  0  1    4    4\nDatsun 710          22.8   4 2.320 108.0  93 3.85 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 3.215 258.0 110 3.08 19.44  1  0    3    1\nHornet Sportabout   18.7   8 3.440    NA 175 3.15 17.02  0  0    3    2\nValiant             18.1   6 3.460 225.0 105 2.76 20.22  1  0    3    1\nDuster 360          14.3   8 3.570 360.0 245 3.21 15.84  0  0    3    4\nMerc 240D           24.4   4 3.190    NA  62 3.69 20.00  1  0    4    2\nMerc 230            22.8   4 3.150 140.8  95 3.92 22.90  1  0    4    2\nMerc 280            19.2   6 3.440 167.6 123 3.92 18.30  1  0    4    4\nMerc 280C           17.8   6 3.440 167.6 123 3.92 18.90  1  0    4    4\nMerc 450SE          16.4   8 4.070 275.8 180 3.07 17.40  0  0    3    3\nMerc 450SL          17.3   8 3.730 275.8 180 3.07 17.60  0  0    3    3\nMerc 450SLC         15.2   8 3.780 275.8 180 3.07 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 5.250 472.0 205 2.93 17.98  0  0    3    4\nLincoln Continental 10.4   8 5.424 460.0 215 3.00 17.82  0  0    3    4\nChrysler Imperial   14.7   8 5.345 440.0 230 3.23 17.42  0  0    3    4\nFiat 128            32.4   4 2.200  78.7  66 4.08 19.47  1  1    4    1\nHonda Civic         30.4   4 1.615  75.7  52 4.93 18.52  1  1    4    2\nToyota Corolla      33.9   4 1.835  71.1  65 4.22 19.90  1  1    4    1\nToyota Corona       21.5   4 2.465    NA  97 3.70 20.01  1  0    3    1\nDodge Challenger    15.5   8 3.520    NA 150 2.76 16.87  0  0    3    2\nAMC Javelin         15.2   8 3.435 304.0 150 3.15 17.30  0  0    3    2\nCamaro Z28          13.3   8 3.840 350.0 245 3.73 15.41  0  0    3    4\nPontiac Firebird    19.2   8 3.845 400.0 175 3.08 17.05  0  0    3    2\nFiat X1-9           27.3   4 1.935  79.0  66 4.08 18.90  1  1    4    1\nPorsche 914-2       26.0   4 2.140 120.3  91 4.43 16.70  0  1    5    2\nLotus Europa        30.4   4 1.513  95.1 113 3.77 16.90  1  1    5    2\nFord Pantera L      15.8   8 3.170 351.0 264 4.22 14.50  0  1    5    4\nFerrari Dino        19.7   6 2.770 145.0 175 3.62 15.50  0  1    5    6\nMaserati Bora       15.0   8 3.570 301.0 335 3.54 14.60  0  1    5    8\nVolvo 142E          21.4   4 2.780 121.0 109 4.11 18.60  1  1    4    2\n\n## Let's move wt to after qsec\n\ndf %&gt;%\n  relocate(wt, .after = qsec)\n\n                     mpg cyl  disp  hp drat  qsec    wt vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 16.46 2.620  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 17.02 2.875  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 18.61 2.320  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 19.44 3.215  1  0    3    1\nHornet Sportabout   18.7   8    NA 175 3.15 17.02 3.440  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 20.22 3.460  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 15.84 3.570  0  0    3    4\nMerc 240D           24.4   4    NA  62 3.69 20.00 3.190  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 22.90 3.150  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 18.30 3.440  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 18.90 3.440  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 17.40 4.070  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 17.60 3.730  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 18.00 3.780  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 17.98 5.250  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 17.82 5.424  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 17.42 5.345  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 19.47 2.200  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 18.52 1.615  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 19.90 1.835  1  1    4    1\nToyota Corona       21.5   4    NA  97 3.70 20.01 2.465  1  0    3    1\nDodge Challenger    15.5   8    NA 150 2.76 16.87 3.520  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 17.30 3.435  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 15.41 3.840  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 17.05 3.845  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 18.90 1.935  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 16.70 2.140  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 16.90 1.513  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 14.50 3.170  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 15.50 2.770  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 14.60 3.570  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 18.60 2.780  1  1    4    2\n\n# Let's use the summarise() and group_by() functions to get a summary of the weight of each car (wt) grouped by cylinders (cyl), and rounded to two decimal places.\n\ndf %&gt;%\n  group_by(cyl) %&gt;%\n  summarise(mean_weight = mean(wt)) %&gt;%\n  round(digits = 2)\n\n# A tibble: 3 × 2\n    cyl mean_weight\n  &lt;dbl&gt;       &lt;dbl&gt;\n1     4        2.29\n2     6        3.12\n3     8        4   \n\n\nThus, we can see that the dplyr package offers several useful functions to manage our data. Remember, that if you want any changes to be reflected in your dataframe, such as renaming a variable, remember to assign your dplyr code to your dataframe. For example, if I want the name change for wt to weight to stick, I would assign that to the dataframe like this:\n\ndf &lt;- df %&gt;%\n  rename(weight = wt)"
  },
  {
    "objectID": "syllabus.html#are-there-any-prerequisites-for-this-course",
    "href": "syllabus.html#are-there-any-prerequisites-for-this-course",
    "title": "Syllabus",
    "section": "Are there any prerequisites for this course?",
    "text": "Are there any prerequisites for this course?\nIf you’ve never used R in your life, nor have even heard of RT2, that’s fine! I assume no prior knowledge of these concepts. It would be helpful to have completed at least a high school level course in statistics before taking this one. But if you haven’t, that’s no problem as we will go over the necessary statistical concepts.\nThe only thing you really need for this course is a modicum of motivation to learn R and RT2! But also a computer with enough memory to download R, R Studio, and maybe a few other things (~300 MB in total)."
  },
  {
    "objectID": "syllabus.html#why-should-i-take-this-course",
    "href": "syllabus.html#why-should-i-take-this-course",
    "title": "Syllabus",
    "section": "Why should I take this course?",
    "text": "Why should I take this course?\nMillions of people use the statistical programming language R to solve complex issues every day. If you have any interest in using data to understand the world, and visualize data in a way that communicates information succinctly and impactfully, this course is for you!\nNeed a programming skill to add to that resume to be more competitive on the job market? This course is for you!\nBesides being a highly marketable skill, learning R is incredibly useful to undertake quantitative research. If you have any interest in doing research, having R in your aRsenal can help you quickly and efficiently write reproducible code to import, clean, analyze, and interpret data in various forms. Finally, doing good research means doing ethical research. I will teach you how to avoid bad practices and malignant influences in order to conduct research in a reproducible, ethical, and rigorous manner.\n\n\n\n\n\n\nIf you want to impress potential romantic partners, R is also useful to have in your back pocket. I once went on a date where the subject of R came up (I brought it up), and my date asked how much it cost. When I said it was free, she was shocked and intrigued. I then explained all the things R can do. The relationship never went anywhere, but still…she was impressed."
  },
  {
    "objectID": "syllabus.html#how-are-you-handling-names-and-pronouns-in-the-course",
    "href": "syllabus.html#how-are-you-handling-names-and-pronouns-in-the-course",
    "title": "Syllabus",
    "section": "How are you handling names and pronouns in the course?",
    "text": "How are you handling names and pronouns in the course?\nIt is essential that we refer to everyone by their names and pronouns. Pronouns can be a way to affirm someone’s gender identity, but they can also be unrelated to a person’s identity. They are simply a public way in which people are referred to in place of their name (e.g. ‘he’ or ‘she’; ‘they’ or ‘ze’; or others). In my classroom, you are invited, but not required to, share the pronouns you use (or the pronouns you want to use in a particular space), and I ask that all of us commit to being attentive to and using each other’s pronouns. If you accidentally misgender someone or use an incorrect pronoun for them, please simply correct yourself to their pronoun. If during the course, you would like to change the pronoun that we are using for you, please let me know.\nIn support of the University of Pennsylvania’s commitment to providing an equitable and safe experience for students whose birth name and/or legal name does not reflect their gender identity and/or gender expression, Penn accepts requests from any student seeking to use a first and/or middle name that differs from the name currently in University records. A student’s requested name can and will be used where feasible in all University systems unless the student’s birth name and/or legal name use is required by law or the student’s requested name use if intent of misrepresentation.\nAny student, including transgender, genderfluid, genderqueer, gender diverse, non-cisgender and cisgender students who wish to designate a different name can do so by editing their Personal Information page in Path@Penn. For additional Preferred First Name, Pronouns, and Gender Identity Information, see this page: https://srfs.upenn.edu/student-records/update-data."
  },
  {
    "objectID": "datajoin.html",
    "href": "datajoin.html",
    "title": "7  Joining and Reshaping Data",
    "section": "",
    "text": "The data analyst’s task is often to combine multiple dataframes. We’ve already seen how to combine rows and columns with matrices in Section 4.2. Those of course are homogenous data structures (all elements have to be the same type). With dataframes we can have multiple types of data structures, and we commonly want to combine multiple dataframes. If two dataframes are already in the same order, and have the exact same number of rows, we can still use the cbind() function to append multiple columns. However, it’s safer to merge dataframes based on a key (usually an ID variable).\nUsually, there is some link such as an ID number for each observational unit that links the dataframes, but not always. Terminologically, combining data is often called merging, appending, or joining. The latter is commonly used with other programming languages like SQL, so we’ll stick to calling it joining or different types of data joins. Four commonly used types of data joins are presented in Figure 7.1.\n\n\n\nFigure 7.1: Four common data join types.\n\n\nLet’s look at an example of a left join using the left_join() function from the dplyr package. We’ll look at two dataframes which both have the same ID variable. So, what we want is a dataframe that takes the columns in one dataframe and adds them to the other dataframe, linked by a common ID variable.\n\nlibrary(dplyr)\nlibrary(readxl)\n\ndf1 &lt;- read_xlsx(\"Datasets/marketing1.xlsx\")\ndf2 &lt;- read_xlsx(\"Datasets/marketing2.xlsx\")\ndf3 &lt;- read_xlsx(\"Datasets/marketing3.xlsx\")\n\nhead(df1, n = 5)\n\n# A tibble: 5 × 5\n     ID Year_Birth Education  Marital_Status Income\n  &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;\n1  5524       1957 Graduation Single          58138\n2  2174       1954 Graduation Single          46344\n3  4141       1965 Graduation Together        71613\n4  6182       1984 Graduation Together        26646\n5  5324       1981 PhD        Married         58293\n\nhead(df2, n = 5)\n\n# A tibble: 5 × 3\n     ID MntWines MntFishProducts\n  &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;\n1  5524      635             172\n2  2174       11               2\n3  4141      426             111\n4  6182       11              10\n5  5324      173              46\n\ndf4 &lt;- df1 %&gt;%\n  left_join(df2,\n            by = c(\"ID\" = \"ID\")\n            )\n\nhead(df4, n = 5)\n\n# A tibble: 5 × 7\n     ID Year_Birth Education  Marital_Status Income MntWines MntFishProducts\n  &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;\n1  5524       1957 Graduation Single          58138      635             172\n2  2174       1954 Graduation Single          46344       11               2\n3  4141       1965 Graduation Together        71613      426             111\n4  6182       1984 Graduation Together        26646       11              10\n5  5324       1981 PhD        Married         58293      173              46\n\n\nThe left_join() function then returns all the rows from the first dataframe (df1 in this case) that match up with the rows in the second dataframe (df2 in this case). The right_join() function (also from the dplyr package) is the converse; it returns all the rows in the second dataframe (df2) that match up with the rows in first dataframe (df1). Functionally, it’s the same as the left_join() function, but notice how the rows in the second dataframe are now ordered first.\n\ndf5 &lt;- df2 %&gt;%\n  right_join(df1,\n             by = c(\"ID\" = \"ID\")\n             )\n\nhead(df5, n = 5)\n\n# A tibble: 5 × 7\n     ID MntWines MntFishProducts Year_Birth Education  Marital_Status Income\n  &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;\n1  5524      635             172       1957 Graduation Single          58138\n2  2174       11               2       1954 Graduation Single          46344\n3  4141      426             111       1965 Graduation Together        71613\n4  6182       11              10       1984 Graduation Together        26646\n5  5324      173              46       1981 PhD        Married         58293\n\n\nLet’s look at an example of an inner join, where the overlap between the two dataframes is retained. Using the inner_join() function from the dplyr package, we can merge two dataframes that have some common information (based on an ID variable). However, all rows in the second dataframe that do not correspond with the first will be dropped.\nTo create a new dataframe df6, we will join two dataframes (df3 and df2) that share the same nine rows (1:9), but df3 has an additional 11 rows (10:20) which df2 doesn’t have. df3 also has two columns that df2 does not have. Let’s see what happens when we merge them in Figure 7.2.\n\ndf6 &lt;- df3 %&gt;%\n  inner_join(df2,\n             by = c(\"ID\" = \"ID\")\n             )\n\n\n\n\nFigure 7.2: The merged dataframe df6 only retained the nine common rows between df2 and df3 (1:9). All other rows were deleted. Additionally, this function took all the columns in df2 and added them to df3, even if they are the same columns. In this case, R adds a little suffix to indicate the provenance of each column (.x and .y). This is not the most useful join, as you can see that it drops observations. However, it can be useful to examine the intersection of two dataframes."
  },
  {
    "objectID": "datatypes.html#sec-mats",
    "href": "datatypes.html#sec-mats",
    "title": "4  Data Types",
    "section": "4.2 Matrices",
    "text": "4.2 Matrices\nA matrix is a two-dimensional vector (i.e. rows and columns.) All columns in matrix should have the same type (e.g. logical, character, numeric) and same length. This means that matrices are homogeneous data structures in that all elements must be of the same type.\nYou can create a matrix from vectors using the rbind() function to combine rows of data, and using the cbind() function to combine columns of data.\n\n# row bind \n\na &lt;- c(.94, .92, .95) \nb &lt;- c(.25, .56, .82) \nc &lt;- c(.65, .45, .37) \nd &lt;- rbind(a, b, c) \n\nprint(d)  \n\n  [,1] [,2] [,3]\na 0.94 0.92 0.95\nb 0.25 0.56 0.82\nc 0.65 0.45 0.37\n\n# column bind  \ne &lt;- c(34.5, 23.6) \nf &lt;- c(22.3, 13.2) \ng &lt;- cbind(e,f) \n\nprint(g)\n\n        e    f\n[1,] 34.5 22.3\n[2,] 23.6 13.2\n\n\nYou can also create a matrix using the matrix() function in which the main arguments are the data vector, the number of rows, and the number of columns. Here is a simple matrix with the numbers 1:9 spread over three rows and three columns:\n\nmatrix(1:9, nrow = 3, ncol = 3)\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\nYou can also change the names of the rows and columns of the matrix using the rownames() and colnames() functions. Let’s try using the matrix() function to input a correlation matrix into R. A correlation matrix is a type of matrix which displays correlation coefficients between variables. Essentially, it shows you the strength of a relationship between pairs of variables ranging from -1 to 1. There are many types of correlation coefficients, but typically Pearson’s \\(r\\) is used. When you look at a correlation matrix, you will notice that 1s are on the diagonal, because a variable is always perfectly correlated with itself. The upper triangle (above the diagonal) is a mirror image of the bottom triangle (below the diagonal), so you only need to focus on one. Personally, I like focusing on the lower triangle, but it doesn’t much matter.\nImagine a study on exam performance and study habits with the following variables:\n\nHours spent studying\nExam Score\nIQ Score\nHours spent sleeping\nSchool Rating\n\nNow let us examine the correlation matrix of the variables of this hypothetical study illustrated in ?fig-cormat1.\n\nIt looks like there is a strong correlation (\\(r = 0.82\\)) between hours spent studying and exam score, which makes sense. We also have smaller correlations between IQ and exam score \\((r = 0.33)\\) and school rating and exam score \\((r = 0.23)\\). All other correlations with exam score are negligible. Let’s now input this correlation matrix into R.\n\n# Input correlations \n\ncorrs &lt;- c(1.00, 0.82, 0.08, -0.22, 0.36, 0.82, \n           1.00, 0.33, -0.04, 0.23, 0.48, 0.33, \n           1.00, 0.06, 0.02, -0.22, -0.04, 0.06, \n           1.00, 0.12, 0.36, 0.23, 0.02, 0.12, \n           1.00)  \n\n# Use matrix() function and specify appropriate rows and columns \n\ncorrmat1 &lt;- matrix(corrs, nrow = 5, ncol = 5)  \n\n# Rename rows and columns \n\ncolnames(corrmat1) &lt;- c(\"Hours studying\", \n                        \"Exam Score\", \n                        \"IQ Score\", \n                        \"Hours sleeping\", \n                        \"School rating\") \n\nrownames(corrmat1) &lt;- c(\"Hours studying\", \n                        \"Exam Score\", \n                        \"IQ Score\", \n                        \"Hours sleeping\", \n                        \"School rating\")  \n\nprint(corrmat1) \n\n               Hours studying Exam Score IQ Score Hours sleeping School rating\nHours studying           1.00       0.82     0.48          -0.22          0.36\nExam Score               0.82       1.00     0.33          -0.04          0.23\nIQ Score                 0.08       0.33     1.00           0.06          0.02\nHours sleeping          -0.22      -0.04     0.06           1.00          0.12\nSchool rating            0.36       0.23     0.02           0.12          1.00\n\n\nTo retrieve an element of a matrix, write the matrix name, followed by square brackets in which the first argument is the row and the second is the column that you want. For example, if we want to retrieve the correlation between \"Hours spent studying\" and \"Exam Score\", we can write the matrix name corrmat1 followed by row 2 and column 1:\n\ncorrmat1[2,1]\n\n[1] 0.82\n\n\nThis retrieves the appropriate correlation of 0.82. Finally, let’s say you want to see the values for an entire column or an entire row. In this case, simply leave the row or column argument blank:\n\n# See the values for column 3 only \ncorrmat1[, 3]  \n\nHours studying     Exam Score       IQ Score Hours sleeping  School rating \n          0.48           0.33           1.00           0.06           0.02 \n\n# See the values for row 4 only \ncorrmat1[4, ] \n\nHours studying     Exam Score       IQ Score Hours sleeping  School rating \n         -0.22          -0.04           0.06           1.00           0.12"
  },
  {
    "objectID": "datajoin.html#left-and-right-joins",
    "href": "datajoin.html#left-and-right-joins",
    "title": "7  Merging (Joining) and Reshaping Data",
    "section": "7.1 Left and Right Joins",
    "text": "7.1 Left and Right Joins\nLet’s look at an example of a left join using the left_join() function from the dplyr package. We’ll look at two dataframes which both have the same ID variable. So, what we want is a dataframe that takes the columns in one dataframe and adds them to the other dataframe, linked by a common ID variable.\n\nlibrary(dplyr)\nlibrary(readxl)\n\ndf1 &lt;- read_xlsx(\"Datasets/marketing1.xlsx\")\ndf2 &lt;- read_xlsx(\"Datasets/marketing2.xlsx\")\ndf3 &lt;- read_xlsx(\"Datasets/marketing3.xlsx\")\n\nhead(df1, n = 5)\n\n# A tibble: 5 × 5\n     ID Year_Birth Education  Marital_Status Income\n  &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;\n1  5524       1957 Graduation Single          58138\n2  2174       1954 Graduation Single          46344\n3  4141       1965 Graduation Together        71613\n4  6182       1984 Graduation Together        26646\n5  5324       1981 PhD        Married         58293\n\nhead(df2, n = 5)\n\n# A tibble: 5 × 3\n     ID MntWines MntFishProducts\n  &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;\n1  5524      635             172\n2  2174       11               2\n3  4141      426             111\n4  6182       11              10\n5  5324      173              46\n\ndf4 &lt;- df1 %&gt;%\n  left_join(df2,\n            by = c(\"ID\" = \"ID\")\n            )\n\nhead(df4, n = 5)\n\n# A tibble: 5 × 7\n     ID Year_Birth Education  Marital_Status Income MntWines MntFishProducts\n  &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;\n1  5524       1957 Graduation Single          58138      635             172\n2  2174       1954 Graduation Single          46344       11               2\n3  4141       1965 Graduation Together        71613      426             111\n4  6182       1984 Graduation Together        26646       11              10\n5  5324       1981 PhD        Married         58293      173              46\n\n\nThe left_join() function then returns all the rows from the first dataframe (df1 in this case) that match up with the rows in the second dataframe (df2 in this case). The right_join() function (also from the dplyr package) is the converse; it returns all the rows in the second dataframe (df2) that match up with the rows in first dataframe (df1). Functionally, it’s the same as the left_join() function, but notice how the rows in the second dataframe are now ordered first.\n\ndf5 &lt;- df2 %&gt;%\n  right_join(df1,\n             by = c(\"ID\" = \"ID\")\n             )\n\nhead(df5, n = 5)\n\n# A tibble: 5 × 7\n     ID MntWines MntFishProducts Year_Birth Education  Marital_Status Income\n  &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;\n1  5524      635             172       1957 Graduation Single          58138\n2  2174       11               2       1954 Graduation Single          46344\n3  4141      426             111       1965 Graduation Together        71613\n4  6182       11              10       1984 Graduation Together        26646\n5  5324      173              46       1981 PhD        Married         58293"
  },
  {
    "objectID": "datajoin.html#inner-and-full-joins",
    "href": "datajoin.html#inner-and-full-joins",
    "title": "7  Merging (Joining) and Reshaping Data",
    "section": "7.2 Inner and Full Joins",
    "text": "7.2 Inner and Full Joins\nLet’s look at an example of an inner join, where the overlap between the two dataframes is retained. Using the inner_join() function from the dplyr package, we can merge two dataframes that have some common information (based on an ID variable). However, all rows in the second dataframe that do not correspond with the first will be dropped.\nTo create a new dataframe df6, we will join two dataframes (df3 and df2) that share the same nine rows (1:9), but df3 has an additional 11 rows (10:20) which df2 doesn’t have. df3 also has two columns that df2 does not have. Let’s see what happens when we merge them.\n\ndf6 &lt;- df3 %&gt;%\n  inner_join(df2,\n             by = c(\"ID\" = \"ID\")\n             )\n\n\nThe merged dataframe df6 only retained the nine common rows between df2 and df3 (1:9). All other rows were deleted. Additionally, this function took all the columns in df2 and added them to df3, even if they are the same columns. In this case, R adds a little suffix to indicate the provenance of each column (.x and .y). This is not the most useful join, as you can see that it drops observations. However, it can be useful to examine the intersection of two dataframes.\nFinally let’s look at a full join which combines everything - the unique aspects of both dataframes as well as the overlap. We will make use of the full_join() function from the dplyr package.\n\ndf7 &lt;- df3 %&gt;%\n  full_join(df2,\n            by = c(\"ID\" = \"ID\")\n            )\n\n\nIn contrast with the inner join, the full join has added all rows across both dataframes. It also added all columns with the appropriate .x or .y suffix to columns shared between dataframes. Given that it added all rows, naturally there will be some columns for which there are no values. For example, rows 10:20 are not present in df2 and the variable MntFishProducts is present in df2. But after the merge, the values for MntFishProducts for rows 10:20 are missing NA."
  },
  {
    "objectID": "datajoin.html#two-common-merging-scenarios",
    "href": "datajoin.html#two-common-merging-scenarios",
    "title": "7  Merging (Joining) and Reshaping Data",
    "section": "7.3 Two Common Merging Scenarios",
    "text": "7.3 Two Common Merging Scenarios\nPractically, the data analyst typically faces two common data merging scenarios:\n\nWe want to add new columns to the same observational units.\nWe want to add new rows (also called appending) to the existing set of columns.\n\nLet’s have a look at how to deal with both these scenarios.\n\n# This is a case where we want to add new columns to the same observational units. This left join will add the columns from df2 to df1. Both dataframes have the same rows.\n\ndf8 &lt;- df1 %&gt;%\n  left_join(df2,\n            by = c(\"ID\" = \"ID\")\n            )\n\n\nNow, we have a merged dataframe where we have added two new columns (MntWines and MntFishProducts from df2) to df1 and matched them based on ID. This is a scenario where you have, for example, multiple dataframes for same individuals.\nNow, let’s look at a situation where we want to append new rows to an existing dataframe. This is a scenario where, for example, you have the same exact set of variables but data for different people across dataframes. First, we’ll modify our existing df3 so that it contains the exact same three variables (ID, MntWines, and MntFishProducts) as df2, but just has different rows. We will then use the base R rbind() function to append rows from df3 to df2.\n\n# This will make df3 have the same variables as df2, but with different rows corresponding to individuals. \n\ndf3 &lt;- df3[-c(1:9), -c(4:5)]\n\n# Now, we will merge df2 and df3 into a new dataframe df9. This will append rows from df3 to df2. \n\ndf9 &lt;- rbind(df2, df3)\n\n\nAs we see from the screenshot above, we have now appended rows 10:20 from df3 to rows 1:9 from df2."
  },
  {
    "objectID": "datajoin.html#reshaping-data",
    "href": "datajoin.html#reshaping-data",
    "title": "7  Merging (Joining) and Reshaping Data",
    "section": "7.4 Reshaping Data",
    "text": "7.4 Reshaping Data\nWhen we think about collecting data about some phenomenon over time, or of taking multiple measurements from the same observational unit, the a dataframe comes in one of two different formats: wide or long. A wide dataframe means that an observational unit’s repeated responses are found on a single row, whereas a long dataframe means that each row corresponds to just one time point for our observational unit. Wide format might be easier to read when looking at change over time, but long format is often useful for particular statistical analyses. Thus, it’s helpful to know how to reshape a dataframe from long to wide, and vice versa.\n\n7.4.1 From long to wide\nLet’s look at an example with a dataframe ncaabball which contains information on certain NCAA Basketball team over a three-year period from 2015 to 2017.\n\nncaabball  &lt;- read.csv(\"~/R Book/Datasets/ncaabball.csv\",\n                     header = TRUE)\n\nhead(ncaabball, n = 15)\n\n       Team Wins Year\n1  Illinois   19 2015\n2  Illinois   15 2016\n3  Illinois   20 2017\n4   Indiana   18 2017\n5   Indiana   20 2015\n6   Indiana   27 2016\n7      Iowa   19 2017\n8      Iowa   22 2015\n9      Iowa   22 2016\n10 Maryland   28 2015\n11 Maryland   24 2017\n12 Maryland   26 2016\n13 Michigan   15 2015\n14 Michigan   22 2016\n15 Michigan   26 2017\n\n\nCurrently, the dataframe is in long format, because the individual teams are listed on multiple rows. Let’s say we want a wide format in which each team only gets one row, and there become three columns corresponding to each of the three years with values in those cells corresponding to the number of wins per year. Let’s then reshape this dataframe from long to wide using the pivot_longer() function from the tidyr package, which is part of the tidyverse.\nThe names_from argument specifies which variable we want to make into separate columns. The values_from argument specifies what should go into the cells of our new set of columns. In this case, we want the values from the Wins column to be included in our new set of columns. Finally, we leverage the names_glue argument to specify the naming for our new columns. Here, we want the Year value to be followed by a space, and then the word Wins which corresponds to our values variable.\n\nlibrary(tidyverse)\n\n# Let's reshape from long to wide so each year is on one row.\n\nncaabball_wide &lt;- ncaabball %&gt;%\n  pivot_wider(\n    names_from = Year,\n    values_from = Wins,\n    names_glue = \"{Year} {.value}\")         ## This is saying \"use the existing column name for the column listed in the \"names_from\" argument, add a space, and then add the existing column name from the column listed in the \"values_from\" argument. \n\n## You can also customize the column names with string text outside of curly braces.\n\nncaabball_wide2 &lt;- ncaabball %&gt;%\n  pivot_wider(\n    names_from = Year,\n    values_from = Wins,\n    names_glue = \"Wins in {Year}\") \n\n# The dataframe is now in wide format with each year having its own column.\n\nhead(ncaabball_wide)\n\n# A tibble: 5 × 4\n  Team     `2015 Wins` `2016 Wins` `2017 Wins`\n  &lt;chr&gt;          &lt;int&gt;       &lt;int&gt;       &lt;int&gt;\n1 Illinois          19          15          20\n2 Indiana           20          27          18\n3 Iowa              22          22          19\n4 Maryland          28          26          24\n5 Michigan          15          22          26\n\nhead(ncaabball_wide2) ## Notice how the column names are different from ncaabball_wide.\n\n# A tibble: 5 × 4\n  Team     `Wins in 2015` `Wins in 2016` `Wins in 2017`\n  &lt;chr&gt;             &lt;int&gt;          &lt;int&gt;          &lt;int&gt;\n1 Illinois             19             15             20\n2 Indiana              20             27             18\n3 Iowa                 22             22             19\n4 Maryland             28             26             24\n5 Michigan             15             22             26\n\n\n\n\n7.4.2 From wide to long\nAlright, now let’s say we want to go back to long from wide. What this means is that our three columns 2015 (Wins), 2016 (Wins), and 2017 (Wins) will now become individual rows. They will now be in their own column, which we’ll have to name. Then the values under 2015 (Wins), 2016 (Wins), and 2017 (Wins) in the ncaabball_wide dataframe will have to be gathered and become rows in their own column, which we will also have to name. In summary, the wide dataframe with five rows and four columns will become a long datafame with 15 rows and three columns.\nTo accomplish this, we will use the pivot_longer() function also from the tidyr package. The first argument cols specifies the columns by name which will become individual rows. Next, the names_to argument specifies the name of the new column that will be created for our previous column variables. The values_to argument species the name of the new column under which we’ll gather all the values from our previous columns (these correspond to game wins).\n\n# Let's reshape from wide to long so each year gets its own row.\n\n## Because we no longer want the word 'Wins' listed after the year, we'll use a specific syntax in the names_pattern argument to only retain the first four characters, which corresponds to the year.\n\nncaabball_long &lt;- ncaabball_wide %&gt;%\n  pivot_longer(\n    cols = c(\"2015 Wins\", \"2016 Wins\", \"2017 Wins\"),\n    names_to = \"Year\",\n    names_pattern = \"(....)\",\n    values_to = \"Wins\"\n    )\n\nhead(ncaabball_long, n = 15) \n\n# A tibble: 15 × 3\n   Team     Year   Wins\n   &lt;chr&gt;    &lt;chr&gt; &lt;int&gt;\n 1 Illinois 2015     19\n 2 Illinois 2016     15\n 3 Illinois 2017     20\n 4 Indiana  2015     20\n 5 Indiana  2016     27\n 6 Indiana  2017     18\n 7 Iowa     2015     22\n 8 Iowa     2016     22\n 9 Iowa     2017     19\n10 Maryland 2015     28\n11 Maryland 2016     26\n12 Maryland 2017     24\n13 Michigan 2015     15\n14 Michigan 2016     22\n15 Michigan 2017     26\n\n\nAnd just like that, we are back to long format using the pivot_longer() function. Remember, reshaping is mainly about readability and preparing the data for particular statistical procedures. Some procedures require the data to be in long format, while others require the data to be in wide format. Therefore, it’s useful to know how to transition from one format to another."
  },
  {
    "objectID": "app7.html",
    "href": "app7.html",
    "title": "Appendix F — Answers for Section 7.5",
    "section": "",
    "text": "The following are answers to the exercises in Section 7.5.\n\nAssign the built-in dataframe Orange to an object named whatever you want. This dataframe relates to the age and circumference of orange trees. First, with respect to the specific tree, is the dataframe in long or wide format? Please explain your answer.\n\n\ntree &lt;- Orange\n\nhead(tree)\n\n  Tree  age circumference\n1    1  118            30\n2    1  484            58\n3    1  664            87\n4    1 1004           115\n5    1 1231           120\n6    1 1372           142\n\n# The dataframe appears to be in long format, as each tree has multiple rows.\n\n\nOnce again with respect to individual trees, reshape the dataframe. If you believe it is in long format, reshape to wide (hint: this is the correct answer). Assign the reshaped dataframe to a new object with a name that indicates the reshaped nature of the data (e.g., “tree_wide”). What you want to see is a dataframe where each row corresponds to age, and separate columns listing the circumference of each tree. Use an approach you have seen to name each column “Tree (number) circumference”. For example, the first tree column should be named “Tree 1 circumference”, the second should be called “Tree 2 circumference”, and so on. Once you have finished, use the head() function to show the first five rows of the reshaped dataframe.\n\n\nlibrary(tidyverse)\n\ntree_wide &lt;- tree %&gt;%\n   pivot_wider(\n    names_from = Tree,\n    values_from = circumference,\n    names_glue = \"Tree {Tree} {.value}\"\n    )\n\nhead(tree_wide, n = 5)\n\n# A tibble: 5 × 6\n    age `Tree 1 circumference` `Tree 2 circumference` `Tree 3 circumference`\n  &lt;dbl&gt;                  &lt;dbl&gt;                  &lt;dbl&gt;                  &lt;dbl&gt;\n1   118                     30                     33                     30\n2   484                     58                     69                     51\n3   664                     87                    111                     75\n4  1004                    115                    156                    108\n5  1231                    120                    172                    115\n# ℹ 2 more variables: `Tree 4 circumference` &lt;dbl&gt;,\n#   `Tree 5 circumference` &lt;dbl&gt;\n\n\n\nLet’s go backwards. Take the wide dataframe from Question 2, and reshape into long format, assigning this to a new object with a name noting that the new dataframe is in long format (e.g. “tree_long”). Here, we want each row to correspond to a tree, and separate columns for age and circumference, labelled as such. Then, use a command that you’ve seen before to make the new tree column come first in order in the dataframe. Finally, use the head() to show the first five rows of the new dataframe.\n\n\ntree_long &lt;- tree_wide %&gt;%\n  pivot_longer(\n    cols = c(\"Tree 1 circumference\", \n             \"Tree 2 circumference\", \n             \"Tree 3 circumference\",\n             \"Tree 4 circumference\",\n             \"Tree 5 circumference\"),\n    names_to = \"Tree\",\n    names_pattern = \"[^Tree](.)\",   ## This is basically saying take the former column labels (Tree 1 circumference, Tree 2 circumference, etc.), exclude the word Tree [^Tree], and keep the first character after the word 'Tree', and nothing else.\n    values_to = \"Circumference\"\n    ) %&gt;%\n  relocate(Tree, .before = everything()  ## This puts the Tree column first in order.\n           )\n\nhead(tree_long, n = 5)\n\n# A tibble: 5 × 3\n  Tree    age Circumference\n  &lt;chr&gt; &lt;dbl&gt;         &lt;dbl&gt;\n1 1       118            30\n2 2       118            33\n3 3       118            30\n4 4       118            32\n5 5       118            30\n\n\n\nLet’s go back to the original Orange dataframe, or the object to which you initially assigned it. Next, we’re going to create two new dataframes tree2 and tree3 using the code below. We’re also going to create an ID variable for the two dataframes with the same rows in order to have unique indices for each row.\n\n\ntree &lt;- Orange\n\nnewage &lt;- tree$circumference/2.5\ntreeid &lt;- tree$Tree\n\ntree2 &lt;- data.frame(Tree = treeid,\n                    \"Age_years\" = newage)\n\n\ntree3 &lt;- data.frame(Tree = c(rep(6, 7)),\n                    \"Age_years\" = c(10.4, \n                                      21.2, \n                                      30.4, \n                                      41.3, \n                                      55.8, \n                                      66.7,\n                                      71.4),\n                    ID = c(seq(from = 36, to = 42)\n                           )\n                    )\ntree3$Tree &lt;- ordered(tree3$Tree) ## This will make the Tree column in tree3 the same class as the Tree column in the original dataframe. This is needed for merging.\n\n## This creates an ID variable in the original dataframe and in tree2.\ntree$ID &lt;- 1:nrow(tree)\ntree2$ID &lt;- 1:nrow(tree2)\n\n\nCreate a new dataframe tree4 that merges tree2 with the Orange dataframe or whatever object you assigned it to. Note, what we want here is to add a new column Age (Years) to the same rows or observational units. The ID variable we created above should be used to link the dataframes. Once the merge is complete, print the first six rows of the merged dataframe to make sure it worked.\n\n\ntree4 &lt;- tree %&gt;%\n  left_join(tree2,\n            by = c(\"ID\" = \"ID\")\n            )\n\nhead(tree4, n = 6)\n\n  Tree.x  age circumference ID Tree.y Age_years\n1      1  118            30  1      1      12.0\n2      1  484            58  2      1      23.2\n3      1  664            87  3      1      34.8\n4      1 1004           115  4      1      46.0\n5      1 1231           120  5      1      48.0\n6      1 1372           142  6      1      56.8\n\n# While this is fine and links the data well, we do have duplicate Tree columns. Since the two dataframes are arranged in the same order (i.e. the ID variables are both sequentially sorted in each dataframe), we can also use the simpler cbind() function. This code takes the unique column from tree2 and binds it to the tree dataframe.\n\ntree4 &lt;- cbind(tree, tree2$Age_years)\n\n## Let's move ID to the front and clean up the Age_years variable name.\n\ntree4 &lt;- tree4 %&gt;%\n  relocate(ID, .before = everything()) %&gt;%\n  rename(\"Age (years)\" = `tree2$Age_years`)\n\nhead(tree4, n = 6)\n\n  ID Tree  age circumference Age (years)\n1  1    1  118            30        12.0\n2  2    1  484            58        23.2\n3  3    1  664            87        34.8\n4  4    1 1004           115        46.0\n5  5    1 1231           120        48.0\n6  6    1 1372           142        56.8\n\n\n\nNow create a new dataframe tree5 which appends rows from tree3 to the tree2 dataframe we created in Question 2. Once the merge is complete, print the last seven rows of the merged dataframe to make sure it worked.\n\n\ntree5 &lt;- rbind(tree2, tree3)\n\ntail(tree5, n = 7)\n\n   Tree Age_years ID\n36    6      10.4 36\n37    6      21.2 37\n38    6      30.4 38\n39    6      41.3 39\n40    6      55.8 40\n41    6      66.7 41\n42    6      71.4 42"
  },
  {
    "objectID": "datajoin.html#sec-joinex",
    "href": "datajoin.html#sec-joinex",
    "title": "7  Merging (Joining) and Reshaping Data",
    "section": "7.5 Exercises",
    "text": "7.5 Exercises\nAs always, it’s a good idea to attempt these while the material is still fresh. You can find the answers in Appendix F.\n\nAssign the built-in dataframe Orange to an object named whatever you want. This dataframe relates to the age and circumference of orange trees. First, with respect to the specific tree, is the dataframe in long or wide format? Please explain your answer.\nOnce again with respect to individual trees, reshape the dataframe. If you believe it is in long format, reshape to wide (hint: this is the correct answer). Assign the reshaped dataframe to a new object with a name that indicates the reshaped nature of the data (e.g., “tree_wide”). What you want to see is a dataframe where each row corresponds to age, and separate columns listing the circumference of each tree. Use an approach you have seen to name each column “Tree (number) circumference”. For example, the first tree column should be named “Tree 1 circumference”, the second should be called “Tree 2 circumference”, and so on. Once you have finished, use the head() function to show the first five rows of the reshaped dataframe.\nLet’s go backwards. Take the wide dataframe from Question 2, and reshape into long format, assigning this to a new object with a name noting that the new dataframe is in long format (e.g. “tree_long”). Here, we want each row to correspond to a tree, and separate columns for age and circumference, labelled as such. Then, use a command that you’ve seen before to make the new tree column come first in order in the dataframe. Finally, use the head() to show the first five rows of the new dataframe.\nLet’s go back to the original Orange dataframe, or the object to which you initially assigned it. Next, we’re going to create two new dataframes tree2 and tree3 using the code below. We’re also going to create an ID variable for the two dataframes with the same rows in order to have unique indices for each row.\n\n\n# I assign the Orange dataframe to an object called tree.\ntree &lt;- Orange\n\n# I then create a vector of values which will populate a new column.\nnewage &lt;- tree$circumference/2.5\n\n# I then create a vector corresponding to values listed in the Tree column in the original dataframe.\ntreeid &lt;- tree$Tree\n\n# I then create a new dataframe with two variables. It takes the original dataframe's Tree column, and adds a new age category using the newage vector created above.\ntree2 &lt;- data.frame(Tree = treeid,\n                    \"Age_years\" = newage)\n\n# I then create a new dataframe with three columns and seven rows. These will be appended to tree2 later.\ntree3 &lt;- data.frame(Tree = c(rep(6, 7)),\n                    \"Age_years\" = c(10.4, \n                                      21.2, \n                                      30.4, \n                                      41.3, \n                                      55.8, \n                                      66.7,\n                                      71.4),\n                    ID = c(seq(from = 36, to = 42)\n                           )\n                    )\n\n# I then make sure the Tree column in tree3 matches the class (ordered factor) of the Tree column in the original dataframe. This is needed for merging later.\n\ntree3$Tree &lt;- ordered(tree3$Tree) \n\n# I then create a unique ID variable in the original dataframe and in tree2 which will be used for merging later.\ntree$ID &lt;- 1:nrow(tree)\ntree2$ID &lt;- 1:nrow(tree2)\n\n\nCreate a new dataframe tree4 that merges tree2 with the Orange dataframe or whatever object you assigned it to. Note, what we want here is to add a new column Age (Years) to the same rows or observational units. The ID variable we created above should be used to link the dataframes. Once the merge is complete, print the first six rows of the merged dataframe to make sure it worked.\nNow create a new dataframe tree5 which appends rows from tree3 to the tree2 dataframe we created in Question 2. Once the merge is complete, print the last seven rows of the merged dataframe to make sure it worked."
  },
  {
    "objectID": "appintroex.html",
    "href": "appintroex.html",
    "title": "Appendix A — Answers for Section 3.8",
    "section": "",
    "text": "Calculate \\(89+9/(4*5)^2\\) and assign the value to the object solution1. Then print solution1.\n\n\nsolution1 &lt;- (89+9)/(4*5)^2 \nprint(solution1)\n\n\nWhat is the difference between R and R Studio?\n\nBase R refers to the statistical programming language and application installed on your computer to process the R programming language. RStudio is an Integrated Development Environment (IDE) that integrates with R to provide much more functionality. You can use base R without RStudio, but not the other way around.\n\nHow do you add a comment to a Script file?\n\n\n# Just add a hash/pound sign to the left. \n#### You can add more hashes for aesthetic purposes ####\n\n# Multi-line comments require a hash \n# starting on the left of each line.\n\n\nWhat are packages in R, and how do you install them?\n\nR packages are user-written collections of functions, compiled code, and sample data. There are over 9000+ packages in R and counting. We use packages for specific things we want to do that we cannot accomplish with the functions in base R, or to do things easier or more efficiently than base R functions.\nMost packages that have been vetted and checked are available on the Comprehensive R Archive Network (CRAN), which is the central R package repository.” “In most cases, installing a package in R is accomplished with the following code install.packages(\"name of package\").\n\nInstall and load the package rstudioapi.\n\n\ninstall.packages(\"rstudioapi\")\nlibrary(rstudioapi)\n\n\nHow do you modify the appearance of R Studio?\n\nTools –&gt; Global Options –&gt; Appearance –&gt; Editor Theme.\n\nAssign the value of 365 to an object called year. Then, create another object called months and assign it the value year * 0.032854884083862.\n\n\nyear &lt;- 365\nmonth &lt;- year * 0.032854884083862\n\n\nCreate three variables named Cowboys, Giants, and Commanders and assign them all the value \"Inferior Team\" using multiple assignment operators.\n\n\nCowboys &lt;- Giants &lt;- Commanders &lt;- \"Inferior Team\""
  },
  {
    "objectID": "appdatatypes.html",
    "href": "appdatatypes.html",
    "title": "Appendix B — Answers for Section 4.7",
    "section": "",
    "text": "Create a numeric vector called vec1 comprising the elements 3, -12, 532, 0, -100, 55, and -42. Then, find the median and lowest value in the vector.\n\n\nvec1 &lt;- c(3, -12, 532, 0, -100, 55, -42) \n\nmedian(vec1)\n\n[1] 0\n\nmin(vec1)\n\n[1] -100\n\n\n\nCreate a new vector vec2 which contains all the elements of vec1 that are positive integers. Then create a new vector vec3 that contains all the elements of vec1 that are negative integers. Then create a new vector vec4 which is the sum of vec2 and vec3.\n\n\nvec2 &lt;- vec1[vec1 &gt; 0]\nvec3 &lt;- vec1[vec1 &lt; 0]\nvec4 &lt;- vec2 + vec3\n\n\nAssign the built-in dataframe mtcars to an object named after your favorite animal. Then calculate and print the median of the variable mpg.\n\n\n# I'm a big donkey guy, but you can put your favorite animal for the name of the object.\ndonkey &lt;- mtcars \n\nmedian(donkey$mpg)\n\n[1] 19.2\n\n\n\nCreate a new variable and add to the object you created above (named after your favorite animal). This variable will take the variable Miles per Gallon mpg and convert it to Kilometers per Liter, which you will name kpl. This can be accomplished by taking mpg and dividing it by 2.352. Once you’ve created this new variable and added it to the object, calculate the median of kpl.\n\n\ndonkey$kpl &lt;- donkey$mpg / 2.352\nmedian(donkey$kpl)\n\n[1] 8.163265\n\n\n\nAssign the built-in dataframe OrchardSprays to an object name of your choice. Then, convert the variable treatment to an ordered factor variable, and change the existing names of factor levels from A:H to a list of sulphur levels from Sulpher_8 to Sulpher_1. Then, print the new treatment variable.\n\n\nlibrary(dplyr)\n\n# I'll assign the built-in OrchardSprays dataframe to an object called sprays.\nsprays &lt;- OrchardSprays\n\n# Next, I'll convert the treatment variable to an ordered factor variable.\n\nsprays$treatment &lt;- factor(sprays$treatment,\n                           ordered = TRUE)\n\nsummary(sprays$treatment)\n\nA B C D E F G H \n8 8 8 8 8 8 8 8 \n\n# Next, I'll replace the existing level names with new ones.\nsprays$treatment &lt;- recode(sprays$treatment,\n                           A = \"Sulphur_8\",\n                           B = \"Sulphur_7\",\n                           C = \"Sulphur_6\",\n                           D = \"Sulphur_5\",\n                           E = \"Sulphur_4\",\n                           F = \"Sulphur_3\",\n                           G = \"Sulphur_2\",\n                           H = \"Sulphur_1\")\n\nprint(sprays$treatment)\n\n [1] Sulphur_5 Sulphur_4 Sulphur_7 Sulphur_1 Sulphur_2 Sulphur_3 Sulphur_6\n [8] Sulphur_8 Sulphur_6 Sulphur_7 Sulphur_1 Sulphur_5 Sulphur_4 Sulphur_8\n[15] Sulphur_3 Sulphur_2 Sulphur_3 Sulphur_1 Sulphur_8 Sulphur_4 Sulphur_5\n[22] Sulphur_6 Sulphur_2 Sulphur_7 Sulphur_1 Sulphur_8 Sulphur_4 Sulphur_6\n[29] Sulphur_3 Sulphur_2 Sulphur_7 Sulphur_5 Sulphur_4 Sulphur_5 Sulphur_2\n[36] Sulphur_8 Sulphur_6 Sulphur_7 Sulphur_1 Sulphur_3 Sulphur_8 Sulphur_6\n[43] Sulphur_3 Sulphur_2 Sulphur_7 Sulphur_5 Sulphur_4 Sulphur_1 Sulphur_7\n[50] Sulphur_2 Sulphur_6 Sulphur_3 Sulphur_8 Sulphur_1 Sulphur_5 Sulphur_4\n[57] Sulphur_2 Sulphur_3 Sulphur_5 Sulphur_7 Sulphur_1 Sulphur_4 Sulphur_8\n[64] Sulphur_6\n8 Levels: Sulphur_8 &lt; Sulphur_7 &lt; Sulphur_6 &lt; Sulphur_5 &lt; ... &lt; Sulphur_1"
  },
  {
    "objectID": "appstringcomp.html",
    "href": "appstringcomp.html",
    "title": "Appendix C — Answers for Section 5.3",
    "section": "",
    "text": "Create a variable called MarySue1 with the value \"Dr Mary Sue Coleman, former president of the University of Michigan once said\". Then, create another variable called MarySue2 with the value \"For today, goodbye. For tomorrow, good luck. And Forever, Go Blue!\". Then find the number of characters in each variable using the nchar() function. Then, check if the letter ‘r’ is present in each variable, and report the results.\n\n\nMarySue1 &lt;- \"Dr Mary Sue Coleman, former president of the University of Michigan once said\"\n\nMarySue2 &lt;- \"For today, goodbye. For tomorrow, good luck. And Forever, Go Blue!\"\n\nnchar(MarySue1)\n\n[1] 77\n\nnchar(MarySue2)\n\n[1] 66\n\n# There are 77 characters in MarySue1 and 66 characters in MarySue2.\n\n\ngrepl('r', MarySue1)\n\n[1] TRUE\n\ngrepl('r', MarySue2)\n\n[1] TRUE\n\n# The letter r is present in both variables MarySue1 and MarySue2.\n\n\nCreate a variable called MarySue3 whose value is a concatenation (combination) of MarySue1 and MarySue2. Then, print the value for MarySue3.\n\n\nMarySue3 &lt;- paste(MarySue1, MarySue2)\nprint(MarySue3)\n\n[1] \"Dr Mary Sue Coleman, former president of the University of Michigan once said For today, goodbye. For tomorrow, good luck. And Forever, Go Blue!\"\n\n\n\nCreate a string vector called basho and assign it the value \"An old silent pond. A frog jumps into the pond—Splash! Silence again.\" Then create and print another variable called basho2 in which the word ‘frog’ has been replaced by ‘buffalo’, and the word ‘Splash!’ has been replaced by ‘Yikes!’.\n\n\nbasho &lt;- \"An old silent pond. A frog jumps into the pond—Splash! Silence again.\"\n\nbasho2 &lt;- basho\n\ngsub('frog', 'buffalo', basho2)\n\n[1] \"An old silent pond. A buffalo jumps into the pond—Splash! Silence again.\"\n\ngsub('Splash!', 'Yikes!', basho2)\n\n[1] \"An old silent pond. A frog jumps into the pond—Yikes! Silence again.\"\n\nprint(basho2)\n\n[1] \"An old silent pond. A frog jumps into the pond—Splash! Silence again.\"\n\n\n\nLet’s do a variation of Mad Libs I will call Mad Sentences. Install and load the keyToEnglish package (be mindful of the capitalization in this package’s name). Then create three variables named after your three favorite cuisines. For each variable, assign the value generate_random_sentences(n = 2, punctuate = TRUE) to generate two random sentences per variable. This will produce a total of six sentences (two per variable). Finally, create a variable called madsentences whose value combines (pastes) all three variables. Print madsentences. If it sounds nonsensical, then it worked!\n\n\nlibrary(keyToEnglish)\n\nindian &lt;- generate_random_sentences(n = 2, punctuate = TRUE)\nchinese &lt;- generate_random_sentences(n = 2, punctuate = TRUE)\nkorean &lt;- generate_random_sentences(n = 2, punctuate = TRUE)\n\nmadsentence &lt;- paste(indian, chinese, korean)\nprint(madsentence)\n\n[1] \"Ephemeral thorny rivet invests bronze thespians. Nautical bumpy pork classifies tiny mezzanines. Vain shaggy astronaut scorches tweedy hobgoblins.\"            \n[2] \"Mercurial camoflauged kitchen perturbs metallic cakes. Rare irridescent barterer dries bipedal novels. Offbeat electrum limburger overloads miniscule sponges.\"\n\n\n\nLet’s compare the returns from simple vs compound interest after five years. First, define p as 1000, r as .07 and t as 5. Then Create a variable called simple with the value p * r * t. Next, create a variable compound with the value p * (1 + r)^t - p. Then, perform a logical test to see if simple is equal to compound, and write out the results of the test in one sentence.\n\n\np &lt;- 1000\nr &lt;- 7 / 100\nt &lt;- 5\n\nsimple &lt;- p * r * t\n\ncompound &lt;- p * (1 + r)^t - p\n\nsimple == compound\n\n[1] FALSE\n\n# Simple interest is not equal to compound interest in five years at a principal of 1000 (dollars) and a rate of seven percent.\n\n\nRetain the variables you created above and write a series of conditional (If else/Else If) statements according to the following rules: 1) If simple is less than compound, print the statement “Simple interest is less than compound interest.”; 2) If simple is greater compound, print the statement “Simple interest is greater than compound interest.”; 3) If simple is equal to compound, print the statement “Simple interest is equal to compound interest.”\n\n\nif (simple &lt; compound) {\n  print(\"Simple interest is less than compound interest\")\n} else if (simple &gt; compound) {\n  print(\"Simple interest is greater than compound interest\")\n} else {                             # could also be else if (simple == compound)\n  print(\"Simple interest is equal to compound interest\")\n}\n\n[1] \"Simple interest is less than compound interest\""
  },
  {
    "objectID": "appdataclean.html",
    "href": "appdataclean.html",
    "title": "Appendix D — Answers for Section 8.7",
    "section": "",
    "text": "The following are answers to the exercises in Section 8.7.\nLet’s create a dataframe with the following code:\n\nindianfood &lt;- data.frame(name = c(\"Boondi\",\n                                  \"Gajar ka halwa\",\n                                  \"Ghevar\",\n                                  \"Kalakand\",\n                                  \"Misti Doi\",\n                                  \"Aloo tikki\",\n                                  \"Chicken tikka masala\"),\n                         ingredients = c(\"Maida flour, yogurt, oil, sugar\",\n                                         \"Carots, milk, sugar, ghee, cashews, raisins\",\n                                         \"Flour, ghee, kewra, milk, clarified butter, sugar, almonds, pistachio, saffron, green cardimom\",\n                                         \"Milk, cottage cheese, sugar\",\n                                         \"Milk, jaggery\",\n                                         \"Rice flour, potatoe, bread crumbs, garam masalaaaa, salt\",\n                                         \"Naan bread, tomato sauce, skinless chicken breasts, heavy cream, garam masala\"),\n                         prep_time = c(45,\n                                       15,\n                                       15,\n                                       20,\n                                       480,\n                                       5,\n                                       15020240),\n                         state = c(\"West Bengal\",\n                                   \"Punjab\",\n                                   \"Rajasthan\",\n                                   \"West Bengal\",\n                                   \"West Bengal\",\n                                   \"Punjab\",\n                                   \"Punjab\")\n                         )\n\nNext, let’s fix some errors and make some changes!\n\nFirst, let’s create some more descriptive variable names. Change the existing variables names to Name, Ingredients, Preparation Time (mins), and Origin (state). Then, print the names of the dataframe.\n\n\nnewnames &lt;- c(\"Name\", \"Ingredients\", \"Preparation Time (mins)\", \"Origin (state)\")\n\nnames(indianfood) &lt;- newnames\nnames(indianfood)\n\n[1] \"Name\"                    \"Ingredients\"            \n[3] \"Preparation Time (mins)\" \"Origin (state)\"         \n\n\n\nNext, having read through the ingredients, we can see various typos that should be fixed. Go ahead and fix the spelling mistakes (typos) you see in the Ingredients column. Hint: there are four spelling mistakes in the Ingredients column, and we are using standard American English for spelling.\n\n\n# I see typos in Rows 2, 3, and 6. In row 2, carrot is spelled incorrectly. In row 3, cardamom is spelled incorrectly. In row 6, potato and masala are spelled incorrectly. \n\nindianfood[2,2] &lt;- \"Carrots, milk, sugar, ghee, cashews, raisins\"\nindianfood[3,2] &lt;- \"Flour, ghee, kewra, milk, clarified butter, sugar, almonds, pistachio, saffron, green cardamom\"\nindianfood[6,2] &lt;- \"Rice flour, potato, bread crumbs, garam masala, salt\"\n\n\nI see that the ingredients list has the terms “cottage cheese”, “clarified butter”, and “naan bread.” Substitute paneer for “cottage cheese” and remove “clarified butter” since there is already ‘ghee’ in the same row, which is pretty much the same thing. Also, remove ‘bread’ from “Naan bread”, since that is unnecessary. Then print the Ingredients column.\n\n\nindianfood$Ingredients &lt;- gsub(\"cottage cheese\", \"paneer\", indianfood$Ingredients)\nindianfood$Ingredients &lt;- gsub(\" clarified butter,\", \" \", indianfood$Ingredients)\nindianfood$Ingredients &lt;- gsub(\"Naan bread\", \"Naan\", indianfood$Ingredients)\n\n\nprint(indianfood$Ingredients)\n\n[1] \"Maida flour, yogurt, oil, sugar\"                                              \n[2] \"Carrots, milk, sugar, ghee, cashews, raisins\"                                 \n[3] \"Flour, ghee, kewra, milk,  sugar, almonds, pistachio, saffron, green cardamom\"\n[4] \"Milk, paneer, sugar\"                                                          \n[5] \"Milk, jaggery\"                                                                \n[6] \"Rice flour, potato, bread crumbs, garam masala, salt\"                         \n[7] \"Naan, tomato sauce, skinless chicken breasts, heavy cream, garam masala\"      \n\n\n\nLet’s check the class of the Origin (state) variable. If it is not already in factor class, go ahead and convert it to factor. Make sure to save the new factor-classed variable back in the dataset. Then, check the class of the variable again to make sure it is factor.\n\n\n# Check the class of the Origin (state) variable.\nclass(indianfood$`Origin (state)`)\n\n[1] \"character\"\n\n# It's currently in character class, so let's make it factor.\nindianfood$`Origin (state)` &lt;- as.factor(indianfood$`Origin (state)`)\n\n# Let's check the new class.\nclass(indianfood$`Origin (state)`)\n\n[1] \"factor\"\n\n\n\nNext, let’s add some variable labels to each variable. The variable labels can say whatever you think might be helpful. In general, think of some description that will provide context if you are looking at these data for the first time. Then, do something to check if it worked. Hint: you’ve seen two ways to do this.\n\n\nlibrary(expss)\n\nindianfood &lt;- apply_labels(indianfood,\n                           Name = \"Name of the Indian dish\",\n                           Ingredients = \"Required components to make the Indian dish\",\n                           \"Preparation Time (mins)\" = \"The time in minutes it takes to prepare to cook the Indian dish\",\n                           \"Origin (state)\" = \"The provenance of the Indian dish by state.\")\n\nstr(indianfood) # you can also View(indianfood) to see the variable labels under the variable names.\n\n'data.frame':   7 obs. of  4 variables:\n $ Name                   :Class 'labelled' chr  \"Boondi\" \"Gajar ka halwa\" \"Ghevar\" \"Kalakand\" ...\n   .. .. LABEL: Name of the Indian dish \n $ Ingredients            :Class 'labelled' chr  \"Maida flour, yogurt, oil, sugar\" \"Carrots, milk, sugar, ghee, cashews, raisins\" \"Flour, ghee, kewra, milk,  sugar, almonds, pistachio, saffron, green cardamom\" \"Milk, paneer, sugar\" ...\n   .. .. LABEL: Required components to make the Indian dish \n $ Preparation Time (mins):Class 'labelled' num  45 15 15 20 480 ...\n   .. .. LABEL: The time in minutes it takes to prepare to cook the Indian dish \n $ Origin (state)         :Class 'labelled' Factor w/ 3 levels \"Punjab\",\"Rajasthan\",..: 3 1 2 3 3 1 1\n   .. .. LABEL: The provenance of the Indian dish by state. \n\n\n\nFinally, let’s have a look at the Preparation Time (mins) variable to detect any outliers. Print the range of values and produce a histogram of this variable. Are any values seemingly outliers from the histogram?\n\n\n# Look at the range of the values.\nrange(indianfood$`Preparation Time (mins)`)\n\n[1]        5 15020240\n\n# Alright, the highest value is definitely suspect. Let's look at the histogram.\nhist(indianfood$`Preparation Time (mins)`)\n\n\n\n# The value 15020240 definitely seems like an outlier because it is SO far away from the other values.\n\n\nIf you detected an outlier in the previous question (and I hope that you did!), explain your decision as to remove it or retain it in the dataframe. If you choose to remove the outlier value, replace it with a more plausible value derived from a quick Google search. Then, print the variable.\n\n\n# Though you don't need to do this since we already see that 15020240 is way beyond the IQR, you can look at a boxplot or using the rstatix's package's identify_outliers function to check this as well. \n\nlibrary(rstatix)\nlibrary(dplyr)\n\nindianfoodoutliers &lt;- indianfood %&gt;%\n  identify_outliers(`Preparation Time (mins)`)\n\n# If you want to see the actual row with the outlier, you can run View(indianfoodoutliers).\n\n# Does a value of 15020240 minutes make sense as the preparation time for chicken tikka masala. First, how long is that value in hours?\n15020240 / 60\n\n[1] 250337.3\n\n## Ok, so it equates to 250,337.3 hours. This seems obviously false. Just to be sure, I Googled chicken tikka masala recipes and see that the preparation time is 20 minutes. Thus, this value is an outlier in that it is illogical and likely a data entry error. I will replace it with 20 minutes.\n\n# Find the row number corresponding to the outlying value.\nwhich(indianfood$`Preparation Time (mins)` == 15020240)\n\n[1] 7\n\n# Now, change the value for Preparation Time (mins) in row 7 to 20, and check that it worked.\nindianfood[7, 3] &lt;- 20\n\nprint(indianfood$`Preparation Time (mins)`)\n\nLABEL: The time in minutes it takes to prepare to cook the Indian dish \nVALUES:\n45, 15, 15, 20, 480, 5, 20\n\n\n\nFinally, save your cleaned indianfood dataframe as an R data file as well as a CSV file using the file name of your choice.\n\n\n# Save as an R Data file\nsave(indianfood, file = \"indianfood.Rdata\")\n\n# Save as a CSV file\nwrite.csv(indianfood, \"indianfood.csv\", row.names = FALSE)"
  },
  {
    "objectID": "apptidyex.html",
    "href": "apptidyex.html",
    "title": "Appendix E — Answers for Section 9.3",
    "section": "",
    "text": "The following are answers to the exercises in Section 9.3.\n\nLoad the tidyverse package. Then assign the built-in dataframe starwars to an object named whatever you want. Then subset the dataframe by human species only. Save the subsetted dataframe as an object called swhuman. Then calculate and report the mean and median height in swhuman. Also report an NAs (missing data) in the height variable. Note: the units for the height variable are centimeters.\n\n\nlibrary(tidyverse)\n\nstardf &lt;- starwars\n\nswhuman &lt;- stardf %&gt;%\n  filter(species == \"Human\")\n\nswhuman %&gt;%\n  select(height) %&gt;%\n  summary()\n\n     height     \n Min.   :150.0  \n 1st Qu.:168.5  \n Median :180.0  \n Mean   :176.6  \n 3rd Qu.:184.0  \n Max.   :202.0  \n NA's   :4      \n\n# The mean height is 176.6 centimeters, and the median height is 180 centimeters.There are 4 NAs.\n\n\nHopefully you noticed that there are indeed some NAs in the swhuman dataframe! Detect which rows have NAs for the height variable, and write the names of the characters that have this. Next, let’s fix these errors. Perform an internet search and populate those NAs with plausible values. If you need to convert from feet to centimeters, multiply the value in feet by 30.48. If you absolutely cannot find the height of any character substitute the median height from Question 1 for their height.\n\n\n# Which rows have NA for the height variable?\n\nwhich(is.na(swhuman$height))\n\n[1] 18 32 33 34\n\nswhuman %&gt;%\n  slice(18,\n        32,\n        33,\n        34) %&gt;%\n  print()\n\n# A tibble: 4 × 14\n  name      height  mass hair_color skin_color eye_color birth_year sex   gender\n  &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 Arvel Cr…     NA    NA brown      fair       brown             NA male  mascu…\n2 Finn          NA    NA black      dark       dark              NA male  mascu…\n3 Rey           NA    NA brown      light      hazel             NA fema… femin…\n4 Poe Dame…     NA    NA brown      light      brown             NA male  mascu…\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n## It looks like Arvel Crynyd, Finn, Rey, and Poe Dameron all have NA values for height. From a quick Google search, I found heights of Finn, Rey, and Poe Dameron but not for Arvel Crynyd. Thus, I will enter a value of 180 (median from Question 1) for Arvel Crynyd. For Finn, Rey, and Poe Dameron, I will enter 176.8, 170.7, and 172, respectively. \n\nswhuman$height[18] &lt;- 180\n\nswhuman$height[32:34] &lt;- c(176.8, 170.7, 172)\n\n\nOnce you have filled in this missing data, calculate the new mean and median for the height variable. Comment on how much of a difference the additional values made on the mean and median compared with the values you calculated in Question 1. Then determine the three shortest characters, and three tallest characters.\n\n\nsummary(swhuman$height)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  150.0   170.0   178.0   176.4   183.0   202.0 \n\n# The new mean is 176.4. This is 0.2 centimeters less than the mean from Question 1.\n# The new median is 178. This is 2 centimeters less than the median from Question 1.\n\n# Next, let's print the three shortest and three tallest characters.\n\nswhuman %&gt;%\n  slice_min(height, n = 3)\n\n# A tibble: 3 × 14\n  name      height  mass hair_color skin_color eye_color birth_year sex   gender\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 Leia Org…    150    49 brown      light      brown             19 fema… femin…\n2 Mon Moth…    150    NA auburn     fair       blue              48 fema… femin…\n3 Cordé        157    NA brown      light      brown             NA fema… femin…\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\nswhuman %&gt;%\n  slice_max(height, n = 3)\n\n# A tibble: 3 × 14\n  name      height  mass hair_color skin_color eye_color birth_year sex   gender\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 Darth Va…    202   136 none       white      yellow          41.9 male  mascu…\n2 Qui-Gon …    193    89 brown      fair       blue            92   male  mascu…\n3 Dooku        193    80 white      fair       brown          102   male  mascu…\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n## The three shortest characters are Leia Organa, Mon Mothma, and Cordé.\n## The three tallest characters are Darth Vader, Qui-Gon Jinn, and Dooku.\n\n\nReturn to the larger starwars dataframe or whatever object to which you assigned it. Determine which characters have NA for height. If there are any characters with NA for height (hint: there are), enter plausible values for their heights using the approach taken in Question 2. Then report the mean and median height across everyone in this dataframe.\n\n\n# Both approaches tell you the rows with NA in height. Use the approach you like.\n\nwhich(is.na(stardf$height))\n\n[1] 28 82 83 84 85 86\n\nstardf$height %&gt;%\n  is.na() %&gt;%\n  which()\n\n[1] 28 82 83 84 85 86\n\n# Determine which characters have NA for height.\n\nstardf %&gt;%\n  slice(28,\n        82:86) %&gt;%\n  print()\n\n# A tibble: 6 × 14\n  name      height  mass hair_color skin_color eye_color birth_year sex   gender\n  &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 Arvel Cr…     NA    NA brown      fair       brown             NA male  mascu…\n2 Finn          NA    NA black      dark       dark              NA male  mascu…\n3 Rey           NA    NA brown      light      hazel             NA fema… femin…\n4 Poe Dame…     NA    NA brown      light      brown             NA male  mascu…\n5 BB8           NA    NA none       none       black             NA none  mascu…\n6 Captain …     NA    NA unknown    unknown    unknown           NA &lt;NA&gt;  &lt;NA&gt;  \n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n# I already have heights for the first four. BB8's height is 67.1, and Captain Phasma's height is 200.1.\n\nstardf$height[28] &lt;- 180\n\nstardf$height[82:86] &lt;- c(176.8, 170.7, 172, 67.1, 200.1)\n\n# Now calculate mean and median height.\n\nsummary(stardf$height)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   66.0   167.0   180.0   173.4   191.0   264.0 \n\n## The mean height is 173.4 cm and the median height is 180 cm.\n\n\nStill working with the starwars dataframe, convert the species variable to factor. Then, group and summarise the mean height by species, and print this in descending order. Report which species is the tallest, on average. Then, rearrange and report the species which is the shortest, on average.\n\n\n# Convert species to factor\n\nstardf$species &lt;- as.factor(stardf$species)\n\n# Which is the tallest species, on average?\n\nstardf %&gt;%\n  group_by(species) %&gt;%\n  summarise(mean_height = mean(height)) %&gt;%\n  arrange(desc(mean_height))\n\n# A tibble: 38 × 2\n   species  mean_height\n   &lt;fct&gt;          &lt;dbl&gt;\n 1 Quermian        264 \n 2 Wookiee         231 \n 3 Kaminoan        221 \n 4 Kaleesh         216 \n 5 Gungan          209.\n 6 Pau'an          206 \n 7 Besalisk        198 \n 8 Cerean          198 \n 9 Chagrian        196 \n10 Nautolan        196 \n# ℹ 28 more rows\n\n## The Quermian species is the tallest on average.\n\n# Which is the shortest species on average?\n\nstardf %&gt;%\n  group_by(species) %&gt;%\n  summarise(mean_height = mean(height)) %&gt;%\n  arrange(mean_height)\n\n# A tibble: 38 × 2\n   species        mean_height\n   &lt;fct&gt;                &lt;dbl&gt;\n 1 Yoda's species         66 \n 2 Aleena                 79 \n 3 Ewok                   88 \n 4 Vulptereen             94 \n 5 Dug                   112 \n 6 Droid                 121.\n 7 Xexto                 122 \n 8 Toydarian             137 \n 9 Sullustan             160 \n10 Toong                 163 \n# ℹ 28 more rows\n\n## Yoda's species is the shortest on average."
  },
  {
    "objectID": "appdatajoin.html",
    "href": "appdatajoin.html",
    "title": "Appendix F — Answers for Section 7.5",
    "section": "",
    "text": "The following are answers to the exercises in Section 7.5.\n\nAssign the built-in dataframe Orange to an object named whatever you want. This dataframe relates to the age and circumference of orange trees. First, with respect to the specific tree, is the dataframe in long or wide format? Please explain your answer.\n\n\ntree &lt;- Orange\n\nhead(tree)\n\n  Tree  age circumference\n1    1  118            30\n2    1  484            58\n3    1  664            87\n4    1 1004           115\n5    1 1231           120\n6    1 1372           142\n\n# The dataframe appears to be in long format, as each tree has multiple rows.\n\n\nOnce again with respect to individual trees, reshape the dataframe. If you believe it is in long format, reshape to wide (hint: this is the correct answer). Assign the reshaped dataframe to a new object with a name that indicates the reshaped nature of the data (e.g., “tree_wide”). What you want to see is a dataframe where each row corresponds to age, and separate columns listing the circumference of each tree. Use an approach you have seen to name each column “Tree (number) circumference”. For example, the first tree column should be named “Tree 1 circumference”, the second should be called “Tree 2 circumference”, and so on. Once you have finished, use the head() function to show the first five rows of the reshaped dataframe.\n\n\nlibrary(tidyverse)\n\ntree_wide &lt;- tree %&gt;%\n   pivot_wider(\n    names_from = Tree,\n    values_from = circumference,\n    names_glue = \"Tree {Tree} {.value}\"\n    )\n\nhead(tree_wide, n = 5)\n\n# A tibble: 5 × 6\n    age `Tree 1 circumference` `Tree 2 circumference` `Tree 3 circumference`\n  &lt;dbl&gt;                  &lt;dbl&gt;                  &lt;dbl&gt;                  &lt;dbl&gt;\n1   118                     30                     33                     30\n2   484                     58                     69                     51\n3   664                     87                    111                     75\n4  1004                    115                    156                    108\n5  1231                    120                    172                    115\n# ℹ 2 more variables: `Tree 4 circumference` &lt;dbl&gt;,\n#   `Tree 5 circumference` &lt;dbl&gt;\n\n\n\nLet’s go backwards. Take the wide dataframe from Question 2, and reshape into long format, assigning this to a new object with a name noting that the new dataframe is in long format (e.g. “tree_long”). Here, we want each row to correspond to a tree, and separate columns for age and circumference, labelled as such. Then, use a command that you’ve seen before to make the new tree column come first in order in the dataframe. Finally, use the head() to show the first five rows of the new dataframe.\n\n\ntree_long &lt;- tree_wide %&gt;%\n  pivot_longer(\n    cols = c(\"Tree 1 circumference\", \n             \"Tree 2 circumference\", \n             \"Tree 3 circumference\",\n             \"Tree 4 circumference\",\n             \"Tree 5 circumference\"),\n    names_to = \"Tree\",\n    names_pattern = \"[^Tree](.)\",   ## This is basically saying take the former column labels (Tree 1 circumference, Tree 2 circumference, etc.), exclude the word Tree [^Tree], and keep the first character after the word 'Tree', and nothing else.\n    values_to = \"Circumference\"\n    ) %&gt;%\n  relocate(Tree, .before = everything()  ## This puts the Tree column first in order.\n           )\n\nhead(tree_long, n = 5)\n\n# A tibble: 5 × 3\n  Tree    age Circumference\n  &lt;chr&gt; &lt;dbl&gt;         &lt;dbl&gt;\n1 1       118            30\n2 2       118            33\n3 3       118            30\n4 4       118            32\n5 5       118            30\n\n\n\nLet’s go back to the original Orange dataframe, or the object to which you initially assigned it. Next, we’re going to create two new dataframes tree2 and tree3 using the code below. We’re also going to create an ID variable for the two dataframes with the same rows in order to have unique indices for each row.\n\n\ntree &lt;- Orange\n\nnewage &lt;- tree$circumference/2.5\ntreeid &lt;- tree$Tree\n\ntree2 &lt;- data.frame(Tree = treeid,\n                    \"Age_years\" = newage)\n\n\ntree3 &lt;- data.frame(Tree = c(rep(6, 7)),\n                    \"Age_years\" = c(10.4, \n                                      21.2, \n                                      30.4, \n                                      41.3, \n                                      55.8, \n                                      66.7,\n                                      71.4),\n                    ID = c(seq(from = 36, to = 42)\n                           )\n                    )\ntree3$Tree &lt;- ordered(tree3$Tree) ## This will make the Tree column in tree3 the same class as the Tree column in the original dataframe. This is needed for merging.\n\n## This creates an ID variable in the original dataframe and in tree2.\ntree$ID &lt;- 1:nrow(tree)\ntree2$ID &lt;- 1:nrow(tree2)\n\n\nCreate a new dataframe tree4 that merges tree2 with the Orange dataframe or whatever object you assigned it to. Note, what we want here is to add a new column Age (Years) to the same rows or observational units. The ID variable we created above should be used to link the dataframes. Once the merge is complete, print the first six rows of the merged dataframe to make sure it worked.\n\n\ntree4 &lt;- tree %&gt;%\n  left_join(tree2,\n            by = c(\"ID\" = \"ID\")\n            )\n\nhead(tree4, n = 6)\n\n  Tree.x  age circumference ID Tree.y Age_years\n1      1  118            30  1      1      12.0\n2      1  484            58  2      1      23.2\n3      1  664            87  3      1      34.8\n4      1 1004           115  4      1      46.0\n5      1 1231           120  5      1      48.0\n6      1 1372           142  6      1      56.8\n\n# While this is fine and links the data well, we do have duplicate Tree columns. Since the two dataframes are arranged in the same order (i.e. the ID variables are both sequentially sorted in each dataframe), we can also use the simpler cbind() function. This code takes the unique column from tree2 and binds it to the tree dataframe.\n\ntree4 &lt;- cbind(tree, tree2$Age_years)\n\n## Let's move ID to the front and clean up the Age_years variable name.\n\ntree4 &lt;- tree4 %&gt;%\n  relocate(ID, .before = everything()) %&gt;%\n  rename(\"Age (years)\" = `tree2$Age_years`)\n\nhead(tree4, n = 6)\n\n  ID Tree  age circumference Age (years)\n1  1    1  118            30        12.0\n2  2    1  484            58        23.2\n3  3    1  664            87        34.8\n4  4    1 1004           115        46.0\n5  5    1 1231           120        48.0\n6  6    1 1372           142        56.8\n\n\n\nNow create a new dataframe tree5 which appends rows from tree3 to the tree2 dataframe we created in Question 2. Once the merge is complete, print the last seven rows of the merged dataframe to make sure it worked.\n\n\ntree5 &lt;- rbind(tree2, tree3)\n\ntail(tree5, n = 7)\n\n   Tree Age_years ID\n36    6      10.4 36\n37    6      21.2 37\n38    6      30.4 38\n39    6      41.3 39\n40    6      55.8 40\n41    6      66.7 41\n42    6      71.4 42"
  },
  {
    "objectID": "appkahneman.html#background",
    "href": "appkahneman.html#background",
    "title": "Appendix G — Kahneman’s Open Letter to Priming Researchers",
    "section": "G.1 Background",
    "text": "G.1 Background\nThis letter has been archived by Nature, and can be currently found here. If the link stops working, please email me and I’ll fix it.\nAs mentioned in Chapter 13 , this letter was sent by psychologist Daniel Kahneman to psychologist John Bargh and others in the field of priming research. The letter was sent at a time where the entire field of not only priming, but psychology in general came under intense scrutiny in the form of a Replication Crisis, which since encouraged other fields to deal with their own crises of reproducibility, transparency, and replication. Today, the STEM fields as well are dealing with their own similar crises, particularly around fabrication or manipulation of images, which have become easier to detect with image recognition software and keen scientific integrity consultants like the formidable Dr Elisabeth Bik.\nThe letter sent by Kahneman is instructive and prescient. He correctly predicted the train wreck that would become the Replication Crisis, and while his daisy chain proposal may not have been realized among labs studying priming, it was reflected in similar large replication efforts such as Many Labs and Many Economists that continue to this day. Below, you will find the letter in full."
  },
  {
    "objectID": "appkahneman.html#the-letter-in-full",
    "href": "appkahneman.html#the-letter-in-full",
    "title": "Appendix G — Kahneman’s Open Letter to Priming Researchers",
    "section": "G.2 The Letter in Full",
    "text": "G.2 The Letter in Full\nFrom: Daniel Kahneman\nSent: Wednesday, September 26, 2012 9:32 AM\nSubject: A proposal to deal with questions about priming effects\nDear colleagues,\nI write this letter to a collection of people who were described to me (mostly by John Bargh) as students of social priming. There were names on the list that I could not match to an email. Please pass it on to anyone else you think might be relevant.\nAs all of you know, of course, questions have been raised about the robustness of priming results. The storm of doubts is fed by several sources, including the recent exposure of fraudulent researchers, general concerns with replicability that affect many disciplines, multiple reported failures to replicate salient results in the priming literature, and the growing belief in the existence of a pervasive file drawer problem that undermines two methodological pillars of your field: the preference for conceptual over literal replication and the use of meta-analysis. Objective observers will point out that the problem could well be more severe in your field than in other branches of experimental psychology, because every priming study involves the invention of a new experimental situation.\nFor all these reasons, right or wrong, your field is now the poster child for doubts about the integrity of psychological research. Your problem is not with the few people who have actively challenged the validity of some priming results. It is with the much larger population of colleagues who in the past accepted your surprising results as facts when they were published. These people have now attached a question mark to the field, and it is your responsibility to remove it.\nI am not a member of your community, and all I have personally at stake is that I recently wrote a book that emphasizes priming research as a new approach to the study of associative memory – the core of what dual-system theorists call System 1. Count me as a general believer. I also believe in a point that John Bargh made in his response to Cleeremans, that priming effects are subtle and that their design requires high-level skills. I am skeptical about replications by investigators new to priming research, who may not be attuned to the subtlety of the conditions under which priming effects are observed, or to the ease with which these effects can be undermined.\nMy reason for writing this letter is that I see a train wreck looming. I expect the first victims to be young people on the job market. Being associated with a controversial and suspicious field will put them at a severe disadvantage in the competition for positions. Because of the high visibility of the issue, you may already expect the coming crop of graduates to encounter problems. Another reason for writing is that I am old enough to remember two fields that went into a prolonged eclipse after similar outsider attacks on the replicability of findings: subliminal perception and dissonance reduction.\nI believe that you should collectively do something about this mess. To deal effectively with the doubts you should acknowledge their existence and confront them straight on, because a posture of defiant denial is self-defeating. Specifically, I believe that you should have an association, with a board that might include prominent social psychologists from other field. The first mission of the board would be to organize an effort to examine the replicability of priming results, following a protocol that avoids the questions that have been raised and guarantees credibility among colleagues outside the field.\nThe following is just an example of such a protocol:\n\nAssemble a group of five labs, where the leading investigators have an established reputation (tenure should perhaps be a requirement). Substantial labs with several students are the most desirable participants.\nEach lab selects a recent demonstration of a priming effect, which they consider robust and most likely to replicate.\nThe board makes a public commitment to these five specific effects\nSet up a daisy chain of labs A-B-C-D-E-A, where each lab will replicate the study selected by its neighbor: B replicates A, C replicates B etc.\nHave the replicating lab send someone to see how subjects are run (hence the emphasis on recency – the experiments should be in the active repertoire of the original lab, so that additional subjects can be run with confidence that the same procedure is followed).\nHave the replicated lab send someone to vet the procedure of the replicating lab as it starts its work\nRun enough subjects to guarantee power (probably more than in the original study)\nUse technology (e.g. video) to ensure that every detail of the method is documented and can be copied by others.\nPre-commit to publish the results, letting the chips fall where they may, and make all data available for analysis by others.\n\nThis is something you could do quickly, and relatively cheaply. The main costs are 10 trips, and funds to cover these costs would be easy to get (I have checked). You would have to be careful in selecting laboratories and results to maximize credibility, and every step of the procedure should be open and documented. The unusually high openness to scrutiny may be annoying and even offensive, but it is a small price to pay for the big prize of restored credibility.\nSuccess (say, replication of four of the five positive priming results) would immediately rehabilitate the field. Importantly, success would also provide an effective challenge to the adequacy of outsiders’ replications. A publicly announced and open effort would be credible among colleagues at large, because it would show that you are sufficiently confident in your results to take a risk.\nMore ambiguous results would be painful, of course, but they would still protect the reputations of scholars who sincerely believe in their work – even if they are sometimes wrong.\nThe protocol I outlined is just an example of something you might do. The main point of my letter is that you should do something, and that you must do it collectively. No single individual will be able to overcome the doubts, but if you act as a group and avoid defensiveness you will be credible.\nAll best,"
  },
  {
    "objectID": "datatypes.html#factors",
    "href": "datatypes.html#factors",
    "title": "4  Data Types",
    "section": "4.6 Factors",
    "text": "4.6 Factors\nFactors are data structures used to store categorical variables in R. Categorical variables have a fixed set of possible values. Factors are not vectors, so they’re a pretty unique class. They have their own unique class, and can be ordered if we have an ordered categorical variable (i.e. a categorical variable with a logical and explicit order, such as finishes in a race that correspond to ‘first’, ‘second’, etc.).\nFor example, let’s say we have a categorical variable related to the temperature of cooked meat called done which has five possible values (or levels) - rare, medium rare, medium, medium well, and well done. Often, we are faced with a character vector which we want to convert to factor, which can be accomplished with the factor() function, in which the first argument is the variable or vector to be converted. Remember, you can always use the class() function to check if you converted the vector or variable to the correct class.\n\n# Let's create the done variable with five levels.\ndone &lt;- c(\"rare\", \"medium rare\", \"medium\", \"medium well\", \"well done\")\n\nclass(done)\n\n[1] \"character\"\n\n# Now, let's convert this character vector to factor.\n\ndone2 &lt;- factor(done)\n\nclass(done2)\n\n[1] \"factor\"\n\n\nIf we want to look at the levels in our factor variable, we can simply put the variable or vector name in parentheses within the levels() function. If we don’t like the order in which the levels are sorted, we can reorder them by inputting a vector of strings corresponding to the order we want as the second argument of the factor() function.\n\n# Let's examine the levels of our factor variable.\nlevels(done2)\n\n[1] \"medium\"      \"medium rare\" \"medium well\" \"rare\"        \"well done\"  \n\n# That's not the right order! Let's reorder the levels so they go in ascending order to temperature.\n\ndonelevels &lt;- c(\"rare\", \"medium rare\", \"medium\", \"medium well\", \"well done\")\n\ndone2 &lt;- factor(done2, levels = donelevels)\n\n# Let's check if it worked.\nlevels(done2)\n\n[1] \"rare\"        \"medium rare\" \"medium\"      \"medium well\" \"well done\"  \n\n\nFactor levels like these have a natural order to them. In this case, the natural order is governed by increasing temperature of the meat. So we might want to properly cast them as ordered factors using the ordered argument in factor, which takes a logical value (i.e. TRUE if you want it ordered and FALSE if you don’t).\n\n# Let's create an ordered factor variable and check that it worked.\ndone3 &lt;- factor(done2,\n                levels = donelevels,\n                ordered = TRUE)\n\nclass(done3)\n\n[1] \"ordered\" \"factor\" \n\n# We can also check that the levels are correctly ordered with the levels() function. We can also use the print() function, which even illustrates the order of the levels.\nprint(done3)\n\n[1] rare        medium rare medium      medium well well done  \nLevels: rare &lt; medium rare &lt; medium &lt; medium well &lt; well done\n\n\nNow, let’s say we want to change the levels of our factor variable. Perhaps, I want the factor levels to reflect my personal meat temperature preference of medium rare or not. So I’ll code medium rare as Delicious and everything else as Gross. We can use the recode() function from the dplyr package to accomplish this (more on dplyr later in Chapter 9).\n\nlibrary(dplyr)\n\ndone4 &lt;- recode(done3,\n                rare = \"Gross\",\n                \"medium rare\" = \"Delicious\",\n                medium = \"Gross\",\n                \"medium well\" = \"Gross\",\n                \"well done\" = \"Gross\")\n\nprint(done4)\n\n[1] Gross     Delicious Gross     Gross     Gross    \nLevels: Gross &lt; Delicious"
  },
  {
    "objectID": "cleaning.html#the-pipe-operator",
    "href": "cleaning.html#the-pipe-operator",
    "title": "8  Data Cleaning",
    "section": "8.2 The Pipe Operator (%>%)",
    "text": "8.2 The Pipe Operator (%&gt;%)\nThis is a probably a good time to introduce one of the most useful collections of packages in R - the tidyverse!\n\n\n\nSource: www.tidyverse.org\n\n\nAccording to its website:\n\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.\n\nIt currently has about 30 packages which can all be installed simultaneously with install.packages(\"tidyverse\"). Once you load the tidyverse, you can see a full list of its packages with the function tidyverse_packages().\n\nlibrary(tidyverse)  \n\ntidyverse_packages()\n\n [1] \"broom\"         \"conflicted\"    \"cli\"           \"dbplyr\"       \n [5] \"dplyr\"         \"dtplyr\"        \"forcats\"       \"ggplot2\"      \n [9] \"googledrive\"   \"googlesheets4\" \"haven\"         \"hms\"          \n[13] \"httr\"          \"jsonlite\"      \"lubridate\"     \"magrittr\"     \n[17] \"modelr\"        \"pillar\"        \"purrr\"         \"ragg\"         \n[21] \"readr\"         \"readxl\"        \"reprex\"        \"rlang\"        \n[25] \"rstudioapi\"    \"rvest\"         \"stringr\"       \"tibble\"       \n[29] \"tidyr\"         \"xml2\"          \"tidyverse\"    \n\n\nThe tidyverse is a collection of useful tools that are commonly used by R users. In particular, the packages dplyr, and ggplot2 are the stars of the show, and very useful for data wrangling and visualization, which is what we will focus on mainly in this book.\nThe pipe operator %&gt;% is used to combine functions in a way that sets up a chain of operations. It was originally part of the magrittr package, and now comes standard with the dplyr package as well. The pipe is essential telling R “Take the output from the left side and pass it into the right side as the first argument.” Put more simply, the pipe is saying “Take this [stuff on left] and put it through that [stuff on right]”. This is important because it can make our code more efficient when we want to multiple things. Let’s look at a simple example.\n\nlibrary(dplyr)\n\n# First, let's randomly sample 10 numbers from a normal distribution.\n\nx &lt;- rnorm(10)\nprint(x)\n\n [1]  0.20402713  0.97692700 -0.37022870  0.97379657  0.02481611  0.52615723\n [7]  1.11562815 -0.02706712 -0.81295051  0.76899704\n\n# Here's the slow way to 1) Add five to every number in the vector; 2) round the numbers to one decimal place; 3) take the log of the numbers; and 4) take the mean of the resulting numbers. \n\n## First, we add five to every number in the vector\nx &lt;- x + 5\nprint(x)\n\n [1] 5.204027 5.976927 4.629771 5.973797 5.024816 5.526157 6.115628 4.972933\n [9] 4.187049 5.768997\n\n## Second, we round the vector to one decimal place.\nx &lt;- round(x, digits = 1)\nprint(x)\n\n [1] 5.2 6.0 4.6 6.0 5.0 5.5 6.1 5.0 4.2 5.8\n\n## Third, we take the log of each number.\nx &lt;- log(x)\nprint(x)\n\n [1] 1.648659 1.791759 1.526056 1.791759 1.609438 1.704748 1.808289 1.609438\n [9] 1.435085 1.757858\n\n## Take the mean of the resulting vector. \nprint(mean(x))\n\n[1] 1.668309\n\n# Let's now accomplish this the more efficient way with pipes!\n\ny &lt;- rnorm(10)\nprint(y)\n\n [1] -1.943097647 -0.494817527  0.924527308 -0.003174683  1.025257662\n [6] -3.639611705 -0.687618213  1.448598335 -0.927727940 -0.859825387\n\ny %&gt;%\n  `+`(5) %&gt;%              # Notice that arithmetic operators are surrounded by backticks `` and scalar values are surrounded by parentheses.\n  round(digits = 2) %&gt;%   # Only the second argument is needed, since the first argument is the vector being passed by the pipes.\n  log() %&gt;%\n  mean() %&gt;%\n  print()\n\n[1] 1.426607\n\n# The summary() function can also provide us with useful information very easily. This information is the interquartile range, including the median and mean.\ny %&gt;%\n  summary()\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-3.6396 -0.9108 -0.5912 -0.5157  0.6926  1.4486 \n\n\nWe can also use the pipe operator to manipulate dataframes, including operations with string vectors or columns. Importantly, note that the pipe operator puts the output from the left side as the first argument on the right side. However, this becomes a problem when we use a function whose first argument is not a dataframe or vector, such as the gsub() function. In such cases, you can put a period . in the place where the vector or dataframe should go. Let’s look at an example.\n\n# Let's create a small dataframe.\n\nbball &lt;- data.frame(team = c(\"San Antonio\",\n                             \"San Antonio\",\n                             \"New Orleans\",\n                             \"Washington\",\n                             \"Utah\",\n                             \"Milwaukee\",\n                             \"Cleveland\"),\n                    player_name = c(\"Romeo Langford\",\n                                    \"Jakov Poeltl\",\n                                    \"Dyson Daniels\",\n                                    \"Will Barton\",\n                                    \"Mike Conley\",\n                                    \"Joe Ingles\",\n                                    \"Raul Neto\"),\n                    points = c(2,\n                               23,\n                               35,\n                               23,\n                               8,\n                               33,\n                               28)\n                    )\n\n# Let's use the pipe operator to chain some functions together! The functions below change the team names and finally print the unique entries in the vector using the unique() function.\n\nbball$team %&gt;%\n  gsub(\"San Antonio\", \"San Antonio Spurs\", .) %&gt;%\n  gsub(\"New Orleans\", \"New Orleans Pelicans\", .) %&gt;%\n  gsub(\"Washington\", \"Washington Commanders\", .) %&gt;%\n  gsub(\"Utah\", \"Utah Jazz\", .) %&gt;%\n  gsub(\"Milwaukee\", \"Milwaukee Bucks\", .) %&gt;%\n  gsub(\"Cleveland\", \"Cleveland Cavaliers\", .) %&gt;%\n  unique()\n\n[1] \"San Antonio Spurs\"     \"New Orleans Pelicans\"  \"Washington Commanders\"\n[4] \"Utah Jazz\"             \"Milwaukee Bucks\"       \"Cleveland Cavaliers\""
  },
  {
    "objectID": "dataexp.html#filter-arrange",
    "href": "dataexp.html#filter-arrange",
    "title": "9  Data Exploration with dplyr",
    "section": "9.1 Filter() & Arrange()",
    "text": "9.1 Filter() & Arrange()\n\nlibrary(dplyr)\n\n# Load mtcars dataframe. Assign it to an object.\ndf &lt;- mtcars\n\n# Let's look at cars with only eight cylinders.\ndf %&gt;%\n  filter(cyl == 8)\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\n\n# Let's add to that, and order the dataframe by mpg in descending order (highest to lowest). To do this, we embed the desc() function within arrange(). For ascending order, no need for the desc() function.\n\ndf %&gt;%\n  filter(cyl == 8) %&gt;%\n  arrange(desc(mpg))\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4"
  },
  {
    "objectID": "dataexp.html#mutate-rename",
    "href": "dataexp.html#mutate-rename",
    "title": "9  Data Exploration with dplyr",
    "section": "9.2 Mutate() & Rename()",
    "text": "9.2 Mutate() & Rename()\n\n# Now let's add a new column called kpl (kilometers per litre) using mutate(). A quick Google search tells me that going from mpg to kpl involves dividing mpg by 2.352.  \n\ndf %&gt;%\n  filter(cyl == 8) %&gt;%\n  arrange(desc(mpg)) %&gt;%\n  mutate(kpl = mpg / 2.352) %&gt;%\n  head()\n\n                   mpg cyl  disp  hp drat    wt  qsec vs am gear carb      kpl\nPontiac Firebird  19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2 8.163265\nHornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2 7.950680\nMerc 450SL        17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3 7.355442\nMerc 450SE        16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3 6.972789\nFord Pantera L    15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4 6.717687\nDodge Challenger  15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2 6.590136\n\n# Let's add a new name for the wt variable called weight.\ndf %&gt;%\n  filter(cyl == 8) %&gt;%\n  arrange(desc(mpg)) %&gt;%\n  mutate(kpl = mpg / 2.352) %&gt;%\n  rename(weight = wt) %&gt;%\n  head()\n\n                   mpg cyl  disp  hp drat weight  qsec vs am gear carb      kpl\nPontiac Firebird  19.2   8 400.0 175 3.08  3.845 17.05  0  0    3    2 8.163265\nHornet Sportabout 18.7   8 360.0 175 3.15  3.440 17.02  0  0    3    2 7.950680\nMerc 450SL        17.3   8 275.8 180 3.07  3.730 17.60  0  0    3    3 7.355442\nMerc 450SE        16.4   8 275.8 180 3.07  4.070 17.40  0  0    3    3 6.972789\nFord Pantera L    15.8   8 351.0 264 4.22  3.170 14.50  0  1    5    4 6.717687\nDodge Challenger  15.5   8 318.0 150 2.76  3.520 16.87  0  0    3    2 6.590136"
  },
  {
    "objectID": "dataexp.html#select-slice",
    "href": "dataexp.html#select-slice",
    "title": "9  Data Exploration with dplyr",
    "section": "9.3 Select() & Slice()",
    "text": "9.3 Select() & Slice()\n\n# Let's now only select a few variables using select()\n\ndf %&gt;%\n  select(mpg, cyl, wt) %&gt;%\n  head()\n\n                   mpg cyl    wt\nMazda RX4         21.0   6 2.620\nMazda RX4 Wag     21.0   6 2.875\nDatsun 710        22.8   4 2.320\nHornet 4 Drive    21.4   6 3.215\nHornet Sportabout 18.7   8 3.440\nValiant           18.1   6 3.460\n\n## If we want to select all but a few variables, we can still use select. Let's say I want all variables except mpg, cyl, and wt. I just need to add a minus before each variable name.\n\ndf %&gt;%\n  select(-mpg, -cyl, -wt)\n\n                     disp  hp drat  qsec vs am gear carb\nMazda RX4           160.0 110 3.90 16.46  0  1    4    4\nMazda RX4 Wag       160.0 110 3.90 17.02  0  1    4    4\nDatsun 710          108.0  93 3.85 18.61  1  1    4    1\nHornet 4 Drive      258.0 110 3.08 19.44  1  0    3    1\nHornet Sportabout   360.0 175 3.15 17.02  0  0    3    2\nValiant             225.0 105 2.76 20.22  1  0    3    1\nDuster 360          360.0 245 3.21 15.84  0  0    3    4\nMerc 240D           146.7  62 3.69 20.00  1  0    4    2\nMerc 230            140.8  95 3.92 22.90  1  0    4    2\nMerc 280            167.6 123 3.92 18.30  1  0    4    4\nMerc 280C           167.6 123 3.92 18.90  1  0    4    4\nMerc 450SE          275.8 180 3.07 17.40  0  0    3    3\nMerc 450SL          275.8 180 3.07 17.60  0  0    3    3\nMerc 450SLC         275.8 180 3.07 18.00  0  0    3    3\nCadillac Fleetwood  472.0 205 2.93 17.98  0  0    3    4\nLincoln Continental 460.0 215 3.00 17.82  0  0    3    4\nChrysler Imperial   440.0 230 3.23 17.42  0  0    3    4\nFiat 128             78.7  66 4.08 19.47  1  1    4    1\nHonda Civic          75.7  52 4.93 18.52  1  1    4    2\nToyota Corolla       71.1  65 4.22 19.90  1  1    4    1\nToyota Corona       120.1  97 3.70 20.01  1  0    3    1\nDodge Challenger    318.0 150 2.76 16.87  0  0    3    2\nAMC Javelin         304.0 150 3.15 17.30  0  0    3    2\nCamaro Z28          350.0 245 3.73 15.41  0  0    3    4\nPontiac Firebird    400.0 175 3.08 17.05  0  0    3    2\nFiat X1-9            79.0  66 4.08 18.90  1  1    4    1\nPorsche 914-2       120.3  91 4.43 16.70  0  1    5    2\nLotus Europa         95.1 113 3.77 16.90  1  1    5    2\nFord Pantera L      351.0 264 4.22 14.50  0  1    5    4\nFerrari Dino        145.0 175 3.62 15.50  0  1    5    6\nMaserati Bora       301.0 335 3.54 14.60  0  1    5    8\nVolvo 142E          121.0 109 4.11 18.60  1  1    4    2\n\n# If we want to select certain rows of a dataframe, we can do this with slice() by mentioning the index number of the columns. If we want to know the row number based on a column value (e.g. mpg &gt; 20), we can use the which() function where you can write the column, relational operator, and value.\n\nwhich(df$mpg &gt; 20)\n\n [1]  1  2  3  4  8  9 18 19 20 21 26 27 28 32\n\ndf %&gt;%\n  slice(1:4,\n        8,\n        9,\n        18:21,\n        26:28,\n        32) %&gt;%\n  print()\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nMerc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n# Similarly, if you want to know which rows have the value 'NA' which indicates missing data for a particular column, we can wrap the is.na() function within the which() function like this.\n\n## Let's add some NAs to the disp variable first.\n\ndf[c(5,8,21:22), 3] &lt;- NA\n\n## Now we will see which rows in disp have NA. \n\nwhich(is.na(df$disp))\n\n[1]  5  8 21 22\n\n## Often we want to know the highest and lowest values of a variable. We can use slice_min() for the lowest values and slice_max() for the highest values. The first argument is the column and the second is how many rows you want (e.g. n = 5).\n\n## The five cars with the lowest mpg.\n\ndf %&gt;%\n  slice_min(mpg, n = 5)\n\n                     mpg cyl disp  hp drat    wt  qsec vs am gear carb\nCadillac Fleetwood  10.4   8  472 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8  460 215 3.00 5.424 17.82  0  0    3    4\nCamaro Z28          13.3   8  350 245 3.73 3.840 15.41  0  0    3    4\nDuster 360          14.3   8  360 245 3.21 3.570 15.84  0  0    3    4\nChrysler Imperial   14.7   8  440 230 3.23 5.345 17.42  0  0    3    4\n\n## The five cars with the highest mpg.\n\ndf %&gt;%\n  slice_max(mpg, n = 5)\n\n                mpg cyl disp  hp drat    wt  qsec vs am gear carb\nToyota Corolla 33.9   4 71.1  65 4.22 1.835 19.90  1  1    4    1\nFiat 128       32.4   4 78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4 75.7  52 4.93 1.615 18.52  1  1    4    2\nLotus Europa   30.4   4 95.1 113 3.77 1.513 16.90  1  1    5    2\nFiat X1-9      27.3   4 79.0  66 4.08 1.935 18.90  1  1    4    1"
  },
  {
    "objectID": "dataexp.html#relocate-and-summarise",
    "href": "dataexp.html#relocate-and-summarise",
    "title": "9  Data Exploration with dplyr",
    "section": "9.4 Relocate() and Summarise()",
    "text": "9.4 Relocate() and Summarise()\n\n# If I want to change the order of how some columns appear in the dataframe, I can do so with relocate() where the first argument is the column(s) to move, and the second argument specifies the intended new location with either .before or .after to indicate where you want to place the columns. \n\n## Let's move wt to the front.\n\ndf %&gt;%\n  relocate(wt, .before = everything()\n           )\n\n                       wt  mpg cyl  disp  hp drat  qsec vs am gear carb\nMazda RX4           2.620 21.0   6 160.0 110 3.90 16.46  0  1    4    4\nMazda RX4 Wag       2.875 21.0   6 160.0 110 3.90 17.02  0  1    4    4\nDatsun 710          2.320 22.8   4 108.0  93 3.85 18.61  1  1    4    1\nHornet 4 Drive      3.215 21.4   6 258.0 110 3.08 19.44  1  0    3    1\nHornet Sportabout   3.440 18.7   8    NA 175 3.15 17.02  0  0    3    2\nValiant             3.460 18.1   6 225.0 105 2.76 20.22  1  0    3    1\nDuster 360          3.570 14.3   8 360.0 245 3.21 15.84  0  0    3    4\nMerc 240D           3.190 24.4   4    NA  62 3.69 20.00  1  0    4    2\nMerc 230            3.150 22.8   4 140.8  95 3.92 22.90  1  0    4    2\nMerc 280            3.440 19.2   6 167.6 123 3.92 18.30  1  0    4    4\nMerc 280C           3.440 17.8   6 167.6 123 3.92 18.90  1  0    4    4\nMerc 450SE          4.070 16.4   8 275.8 180 3.07 17.40  0  0    3    3\nMerc 450SL          3.730 17.3   8 275.8 180 3.07 17.60  0  0    3    3\nMerc 450SLC         3.780 15.2   8 275.8 180 3.07 18.00  0  0    3    3\nCadillac Fleetwood  5.250 10.4   8 472.0 205 2.93 17.98  0  0    3    4\nLincoln Continental 5.424 10.4   8 460.0 215 3.00 17.82  0  0    3    4\nChrysler Imperial   5.345 14.7   8 440.0 230 3.23 17.42  0  0    3    4\nFiat 128            2.200 32.4   4  78.7  66 4.08 19.47  1  1    4    1\nHonda Civic         1.615 30.4   4  75.7  52 4.93 18.52  1  1    4    2\nToyota Corolla      1.835 33.9   4  71.1  65 4.22 19.90  1  1    4    1\nToyota Corona       2.465 21.5   4    NA  97 3.70 20.01  1  0    3    1\nDodge Challenger    3.520 15.5   8    NA 150 2.76 16.87  0  0    3    2\nAMC Javelin         3.435 15.2   8 304.0 150 3.15 17.30  0  0    3    2\nCamaro Z28          3.840 13.3   8 350.0 245 3.73 15.41  0  0    3    4\nPontiac Firebird    3.845 19.2   8 400.0 175 3.08 17.05  0  0    3    2\nFiat X1-9           1.935 27.3   4  79.0  66 4.08 18.90  1  1    4    1\nPorsche 914-2       2.140 26.0   4 120.3  91 4.43 16.70  0  1    5    2\nLotus Europa        1.513 30.4   4  95.1 113 3.77 16.90  1  1    5    2\nFord Pantera L      3.170 15.8   8 351.0 264 4.22 14.50  0  1    5    4\nFerrari Dino        2.770 19.7   6 145.0 175 3.62 15.50  0  1    5    6\nMaserati Bora       3.570 15.0   8 301.0 335 3.54 14.60  0  1    5    8\nVolvo 142E          2.780 21.4   4 121.0 109 4.11 18.60  1  1    4    2\n\n## Let's move wt to before disp.\n\ndf %&gt;%\n  relocate(wt, .before = disp)\n\n                     mpg cyl    wt  disp  hp drat  qsec vs am gear carb\nMazda RX4           21.0   6 2.620 160.0 110 3.90 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 2.875 160.0 110 3.90 17.02  0  1    4    4\nDatsun 710          22.8   4 2.320 108.0  93 3.85 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 3.215 258.0 110 3.08 19.44  1  0    3    1\nHornet Sportabout   18.7   8 3.440    NA 175 3.15 17.02  0  0    3    2\nValiant             18.1   6 3.460 225.0 105 2.76 20.22  1  0    3    1\nDuster 360          14.3   8 3.570 360.0 245 3.21 15.84  0  0    3    4\nMerc 240D           24.4   4 3.190    NA  62 3.69 20.00  1  0    4    2\nMerc 230            22.8   4 3.150 140.8  95 3.92 22.90  1  0    4    2\nMerc 280            19.2   6 3.440 167.6 123 3.92 18.30  1  0    4    4\nMerc 280C           17.8   6 3.440 167.6 123 3.92 18.90  1  0    4    4\nMerc 450SE          16.4   8 4.070 275.8 180 3.07 17.40  0  0    3    3\nMerc 450SL          17.3   8 3.730 275.8 180 3.07 17.60  0  0    3    3\nMerc 450SLC         15.2   8 3.780 275.8 180 3.07 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 5.250 472.0 205 2.93 17.98  0  0    3    4\nLincoln Continental 10.4   8 5.424 460.0 215 3.00 17.82  0  0    3    4\nChrysler Imperial   14.7   8 5.345 440.0 230 3.23 17.42  0  0    3    4\nFiat 128            32.4   4 2.200  78.7  66 4.08 19.47  1  1    4    1\nHonda Civic         30.4   4 1.615  75.7  52 4.93 18.52  1  1    4    2\nToyota Corolla      33.9   4 1.835  71.1  65 4.22 19.90  1  1    4    1\nToyota Corona       21.5   4 2.465    NA  97 3.70 20.01  1  0    3    1\nDodge Challenger    15.5   8 3.520    NA 150 2.76 16.87  0  0    3    2\nAMC Javelin         15.2   8 3.435 304.0 150 3.15 17.30  0  0    3    2\nCamaro Z28          13.3   8 3.840 350.0 245 3.73 15.41  0  0    3    4\nPontiac Firebird    19.2   8 3.845 400.0 175 3.08 17.05  0  0    3    2\nFiat X1-9           27.3   4 1.935  79.0  66 4.08 18.90  1  1    4    1\nPorsche 914-2       26.0   4 2.140 120.3  91 4.43 16.70  0  1    5    2\nLotus Europa        30.4   4 1.513  95.1 113 3.77 16.90  1  1    5    2\nFord Pantera L      15.8   8 3.170 351.0 264 4.22 14.50  0  1    5    4\nFerrari Dino        19.7   6 2.770 145.0 175 3.62 15.50  0  1    5    6\nMaserati Bora       15.0   8 3.570 301.0 335 3.54 14.60  0  1    5    8\nVolvo 142E          21.4   4 2.780 121.0 109 4.11 18.60  1  1    4    2\n\n## Let's move wt to after qsec\n\ndf %&gt;%\n  relocate(wt, .after = qsec)\n\n                     mpg cyl  disp  hp drat  qsec    wt vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 16.46 2.620  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 17.02 2.875  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 18.61 2.320  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 19.44 3.215  1  0    3    1\nHornet Sportabout   18.7   8    NA 175 3.15 17.02 3.440  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 20.22 3.460  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 15.84 3.570  0  0    3    4\nMerc 240D           24.4   4    NA  62 3.69 20.00 3.190  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 22.90 3.150  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 18.30 3.440  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 18.90 3.440  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 17.40 4.070  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 17.60 3.730  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 18.00 3.780  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 17.98 5.250  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 17.82 5.424  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 17.42 5.345  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 19.47 2.200  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 18.52 1.615  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 19.90 1.835  1  1    4    1\nToyota Corona       21.5   4    NA  97 3.70 20.01 2.465  1  0    3    1\nDodge Challenger    15.5   8    NA 150 2.76 16.87 3.520  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 17.30 3.435  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 15.41 3.840  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 17.05 3.845  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 18.90 1.935  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 16.70 2.140  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 16.90 1.513  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 14.50 3.170  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 15.50 2.770  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 14.60 3.570  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 18.60 2.780  1  1    4    2\n\n# Let's use the summarise() and group_by() functions to get a summary of the weight of each car (wt) grouped by cylinders (cyl), and rounded to two decimal places.\n\ndf %&gt;%\n  group_by(cyl) %&gt;%\n  summarise(mean_weight = mean(wt)) %&gt;%\n  round(digits = 2)\n\n# A tibble: 3 × 2\n    cyl mean_weight\n  &lt;dbl&gt;       &lt;dbl&gt;\n1     4        2.29\n2     6        3.12\n3     8        4   \n\n\nThus, we can see that the dplyr package offers several useful functions to manage our data. Remember, that if you want any changes to be reflected in your dataframe, such as renaming a variable, remember to assign your dplyr code to your dataframe. For example, if I want the name change for wt to weight to stick, I would assign that to the dataframe like this:\n\ndf &lt;- df %&gt;%\n  rename(weight = wt)"
  },
  {
    "objectID": "dataviz.html#visualizations-in-base-r",
    "href": "dataviz.html#visualizations-in-base-r",
    "title": "10  Data Visualization with base R and ggplot2",
    "section": "10.1 Visualizations in Base R",
    "text": "10.1 Visualizations in Base R\n\n10.1.1 Bar graphs\nBar graphs are commonly used to illustrate differences between groups on some numeric measure. Usually the levels of a categorical variable are on one axis, and some sort of numeric measure on the other axis. The latter can be another variable, but can also be a summary measures such as counts, sums, means, or standard deviations. In base R, we can create a bar graph using the barplot() function. Let’s say I want to compare the total population for the five continents in the gapminder dataframe for the year 1952.\n\nlibrary(gapminder)\nlibrary(tidyverse)\n\n# First, I'll create a small dataframe which filters the dataframe by the year 1952. It then groups the dataframe by continent, and creates a new variable which adds up the total population by continent. It then divides this variable by a million, rounds to the nearest integer, and renames the columns.\n\nsumpop &lt;- gapminder %&gt;%\n  filter(year == \"1952\") %&gt;%\n  group_by(continent) %&gt;%\n  summarise(sum_pop = sum(pop)) %&gt;%\n  mutate(sum_pop = round((sum_pop/1000000), digits = 0)) %&gt;%\n  rename(\"Total Population (Millions)\" = sum_pop,\n         \"Continent\" = continent) \n\n\nhead(sumpop)\n\n# A tibble: 5 × 2\n  Continent `Total Population (Millions)`\n  &lt;fct&gt;                             &lt;dbl&gt;\n1 Africa                              238\n2 Americas                            345\n3 Asia                               1395\n4 Europe                              418\n5 Oceania                              11\n\n# Now, let's make a basic barplot.\n\nbarplot(sumpop$`Total Population (Millions)`,\n        names.arg = sumpop$Continent)\n\n\n\n\nOk, that’s not the prettiest bar graph in the world. Also the y-axis doesn’t extend to the maximum value of the data. Let’s change the y-axis limits with the ylim argument. Let’s also add some color to the bars, add a title, and label the y-axis.\n\n\n\n\n\n\nColors in R\n\n\n\nBefore we proceed, let’s take a moment to review how to use colors for our visualizations in R. There are a number of ways to use colors in R. The first way is to simple write the name of the color like \"red\", \"beige\", \"cyan\" or more exotic color names such as \"darkorchid2\", \"goldenrod\", and \"mistyrose\". In fact, there are 657 colors which you can currently call by name. The full list can be seen by running the colors() function. Of course, seeing a list of color names is not useful unless we already know how the colors look. To see a full list of all 657 colors along with their names, check out this Data Novia page.\nYou might want to choose colors based on a palette, often to illustrate progress on a gradient. Or you might just find it easier to look at a palette and see which colors work well together. In such cases, I recommend choosing an established color palette from the RColorBrewer package. You can load this library and use the display.brewer.all() function to get a list of color palettes. The palettes are listed in Figure 10.1 below. You can then use the brewer.pal() function from the RColorBrewer package to add some colors to our visualizations. The first argument is the number of colors you want, and the second argument is the name of the palette.\n\n\n\nFigure 10.1: The RColorBrewer package has a number of great color palettes.\n\n\nThere are also great color packages available through other packages. One of my favorites is the wesanderson package created by Karthik Ram, which contain color palettes from the movies of famous director Wes Anderson.\nFinally, you can also invoke colors by hexadecimal number. The hexadecimal system is a numerical system of base-16. It uses 16 symbols to represent numbers (10 to 15 are represented with the letters A through F). A list of hexadecimal numbers can be found in Figure 10.2.\n\n\n\nFigure 10.2: Hexadecimal color codes. Source: VisiBone.\n\n\nWhen using the hexadecimal (or ‘hex’) number in the col = argument, make sure to add a hash # in front of the code (e.g. #00CCFF, #0033CC).\n\n\nOk, back to our barplot. Let’s create one with named colors, labels, and a title.\n\nbarplot(sumpop$`Total Population (Millions)`,  #this is the y-axis variable.\n        names.arg = sumpop$Continent,          # this is the x-axis variable.\n        col = c(\"forestgreen\", \"darkblue\", \"brown2\", \"gold1\", \"magenta4\"), # the color of the bars\n        ylim = c(0, 1400), # y-axis limit\n        ylab = \"Population (millions)\",\n        main = \"Total Population by Continent in 1952\"  # Title of graph.\n        )\n\n\n\n\nLooks good, but how about some colors from the Wes Anderson movie The Royal Tenenbaums? Why not!\n\nlibrary(wesanderson)\n\n# Let's look at the second The Royal Tenenbaums palette.The first one only has four colors, but the second one has five.\n\nwes_palette(\"Royal2\")\n\n\n\npal1 &lt;- wes_palette(\"Royal2\")\n\nbarplot(sumpop$`Total Population (Millions)`,  #this is the y-axis variable.\n        names.arg = sumpop$Continent,          # this is the x-axis variable.\n        col = pal1, # the color of the bars\n        ylim = c(0, 1400), # y-axis limit\n        ylab = \"Population (millions)\",\n        main = \"Total Population by Continent in 1952\"  # Title of graph.\n        )\n\n\n\n\n\n\n10.1.2 Histograms\nIf you have two numeric variables, you can examine the distribution of a variable based on values of another variable with a histogram. Even if you only want to examine the distribution of values in one variable, a histogram is a great approach. I commonly check the distribution of a variable quickly with the hist() function, where the first argument is the variable of interest. Other arguments can be used to add axis labels, titles, and axis limits. With a histogram, the number of bars or bins can be increased when you have many different values. This can be adjusted with the breaks = argument. Let’s look at the distribution of the mpg variable in the mtcars dataframe.\n\n# A quick and basic histogram\nhist(mtcars$mpg)\n\n\n\n# Let's add some color, labels, and axis limits.\nhist(mtcars$mpg, col = \"#FF3333\",\n     xlab = \"Miles Per Gallon\",\n     ylab = \"Count\",\n     main = \"Distribution of Miles Per Gallon\",\n     xlim = c(0, 40),\n     ylim = c(0, 15)\n     )\n\n\n\n# Let's now increase the number of bins. If we do this with many unique values, we may need to adjust the y-axis limits since the frequency is more diffuse.\n\nhist(mtcars$mpg, col = \"#FF3333\",\n     xlab = \"Miles Per Gallon\",\n     ylab = \"Count\",\n     main = \"Distribution of Miles Per Gallon\",\n     xlim = c(0, 40),\n     ylim = c(0, 5),\n     breaks = 20\n     )\n\n\n\n\n\n\n10.1.3 Scatterplot\nWhen we have two numeric variables and want to see the relationship between them, a scatterplot is ideal. This visualization is a series of dots on graph that shows the corresponding value of y for all values of x in the dataframe. This can be constructed using the plot() function.\n\n# Let's create a scatterplot of miles per gallon and horsepower.\n\nplot(mtcars$mpg, mtcars$hp,\n     main = \"Scatterplot of miles per gallon and horesepower\",\n     xlab = \"Miles per Gallon\",\n     ylab = \"Horsepower\",\n     col = \"#FF00CC\",\n     ylim = c(30,350)\n     )\n\n\n\n\nWe can also change the shape of the points to other things, listed in Figure 10.3 using the pch = argument. The size of the shape can be adjusted by specifying a number relative to the current size. For example cex = 2 would double the default size of the shape, and cex = 0.5 would reduce the size of the shape by 50%.\n\n\n\nFigure 10.3: Different points that can be used on scatterplots.\n\n\nFor instance, let’s change the shape of the dots to the filled-in triangle (#17), and increase the size of the shapes by 50%.\n\nplot(mtcars$mpg, mtcars$hp,\n     main = \"Scatterplot of miles per gallon and horesepower\",\n     xlab = \"Miles per Gallon\",\n     ylab = \"Horsepower\",\n     col = \"#FF00CC\",\n     ylim = c(30,350), \n     cex = 1.5,\n     pch = 17\n     )\n\n\n\n\nYou can export the images to certain formats using the corresponding functions - jpeg(), png(), svg(), and pdf(), where the first argument is the file path including the file name, and additional arguments that can be used to adjust the width and height of the image. After this command is run, we then create the plot, and then close the plot using the dev.off() function.\n\n# Step 1: Create export file.\n\npng(\"Scatterplot1.png\", \n    width = 750, \n    height = 500)\n\n# Step 2: Create plot.\nplot(mtcars$mpg, mtcars$hp,\n     main = \"Scatterplot of miles per gallon and horesepower\",\n     xlab = \"Miles per Gallon\",\n     ylab = \"Horsepower\",\n     col = \"LightSkyBlue\",\n     ylim = c(30,350),\n     pch = 19\n     )\n\n# Close file.\ndev.off()"
  },
  {
    "objectID": "dataviz.html#visualizations-in-ggplot2",
    "href": "dataviz.html#visualizations-in-ggplot2",
    "title": "10  Data Visualization with base R and ggplot2",
    "section": "10.2 Visualizations in ggplot2",
    "text": "10.2 Visualizations in ggplot2\nOne of the best parts of the tidyverse is the ggplot2 package for data visualization. The ‘gg’ in ggplot2 stands for grammar of graphics, the namesake of a famous book by Leland Wilkinson. The books sets out a framework for layering elements to construct visualizations. If you’ve ever used graphic design software like Photoshop or Canva, you will note a similar idea behind layering different elements together. The creator of ggplot2 - Hadley Wickham - was influenced by this approach, and extended this thinking in his paper A Layered Grammar of Graphics.1\nOne of the main ideas behind the grammar of graphics influence on ggplot2 is that each graphic can contain many layers of different things, but they all need at least three basic layers satisfied:\n\nSome data that you want to visualize. This is commonly one or more variables from a dataframe.\nParticular aesthetics (aes for short) which specify the axes and dimensions of the plot.\nA geometric object (geom for short) which refer to the type of plot we want (scatterplot, bar plot, etc).\n\nThere are other potential layers we can add to customize labels, add relevant statistics, use a different coordinate system, specify subplots, and more. Some of the main layers are presented in Figure 10.4. Note that the bottom three are the most crucial as these form the basic building blocks of a ggplot.\n\n\n\nFigure 10.4: Some of the main layers of ggplot2. At a minimum, you need the bottom three layers to create a ggplot.\n\n\n\n10.2.1 Bar Graphs\nLet’s return to the gapminder dataframe from the package of the same name. In Section 10.1.1 we created bar graphs for population by continent for the year 1952. Let’s do the same thing, but for the year 2007, and using ggplot() function to create our bar graph. Note that with bar graphs, we need the argument stat = \"identity\" within the geom_bar() layer.\n\n# First, let's create a smaller dataframe which filters the dataframe by the year 2007 It then groups the dataframe by continent, and creates a new variable which adds up the total population by continent. It then divides this variable by a million, and rounds to the nearest integer.\n\nsumpop2 &lt;- gapminder %&gt;%\n  filter(year == \"2007\") %&gt;%\n  group_by(continent) %&gt;%\n  summarise(sum_pop = sum(pop)) %&gt;%\n  mutate(sum_pop = round((sum_pop/1000000), digits = 0))\n\nhead(sumpop2)\n\n# A tibble: 5 × 2\n  continent sum_pop\n  &lt;fct&gt;       &lt;dbl&gt;\n1 Africa        930\n2 Americas      899\n3 Asia         3812\n4 Europe        586\n5 Oceania        25\n\n# Let's now create our bar graph to examine population by continent for the year 2007.\n\nggplot(sumpop2,                             ## Data layer\n       aes(x = continent, y = sum_pop)) +   ## Aesthetics layer\n  geom_bar(stat = \"identity\")               ## Geometry layer\n\n\n\n\nThat’s the basic bar graph. Let’s add some colors using the fill = argument in the aes layer. We can also modify the colors by name or hex digit using the scale_fill_manual() function. We can also use a palette from the RColorBrewer package using the scale_fill_brewer() function. Additionally, we can also modify the legend position (including to omit it entirely) using the theme(legend.position = )argument. If we want to reorder the x-axis categories, we can do that using the scale_x_discrete(limits = function. We can also modify axis labels and the title of the plot using the labs() layer. Additionally, the grey background of the graph can seem pretty drab. To change the overall appearance of the plot, we can select one of a number of themes (see a list of themes on the ggplot page). Personally, I like the theme called minimal.\n\n# Using the \"fill =\" argument, we enter the variable we want to color.\nggplot(sumpop2,\n       aes(x = continent, y = sum_pop, fill = continent)) +\n  geom_bar(stat = \"identity\") \n\n\n\n# We can change the colors by name or hex digit using scale_fill_manual().\nggplot(sumpop2,\n       aes(x = continent, y = sum_pop, fill = continent)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = c(\"#FF0000\", \"#FF00CC\", \"#3300FF\", \n                               \"#33FF00\", \"#CC9900\"))\n\n\n\n# Let's use a palette from the RColorBrewer package, reorder the continents on the x-axis in descending order, get rid of the legend, change the axis labels, add a title for the plot, and use the minimal theme.\n\nggplot(sumpop2,\n       aes(x = continent, y = sum_pop, fill = continent)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme(legend.position = \"none\") +\n  scale_x_discrete(limits = c(\"Asia\", \"Africa\", \"Americas\",\n                              \"Europe\", \"Oceania\")) +\n  labs(title = \"Total Population by Continent in 2007\",\n       y = \"Total Population (Millions)\",\n       x = \"Continent\") +\n  theme_minimal()\n\n\n\n\nIt’s starting to look nicer, right? There’s plenty more we can do to customize this ggplot, including adding value labels right above each bar, which can improve the interpretability of the plot. We can also another variable in there to compare changes. For instance, let’s say we want to look at the population in 1952 vs 2007 for each continent.\n\n# First, let's create a dataframe with population for the years 1952 and 2007 by continent. \nsumpop3 &lt;- gapminder %&gt;%\n  select(country, continent, year, pop) %&gt;%\n  filter(year == \"1952\" | year == \"2007\") %&gt;%\n  group_by(continent, year) %&gt;%\n  summarise(total_pop = sum(pop)) %&gt;%\n  mutate(total_pop = round((total_pop/1000000), digits = 0))\n  \nsumpop3$year &lt;- as.factor(sumpop3$year)\n\nhead(sumpop3)\n\n# A tibble: 6 × 3\n# Groups:   continent [3]\n  continent year  total_pop\n  &lt;fct&gt;     &lt;fct&gt;     &lt;dbl&gt;\n1 Africa    1952        238\n2 Africa    2007        930\n3 Americas  1952        345\n4 Americas  2007        899\n5 Asia      1952       1395\n6 Asia      2007       3812\n\n# Now let's create side-by-side bar graphs (known as a dodged barplot) using the position = position_dodge() argument in geom_bar(). \n\nggplot(sumpop3,\n       aes(x = continent, y = total_pop, fill = year)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme(legend.position = \"none\") +\n  scale_x_discrete(limits = c(\"Asia\", \"Africa\", \"Americas\",\n                              \"Europe\", \"Oceania\")) +\n  labs(title = \"Total Population by Continent: 1952 vs 2007\",\n       y = \"Total Population (Millions)\",\n       x = \"Continent\") +\n  theme_minimal()\n\n\n\n# Now let's add value labels above each bar using the geom_text() layer.\n\nggplot(sumpop3,\n       aes(x = continent, y = total_pop, fill = year)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme(legend.position = \"none\") +\n  scale_x_discrete(limits = c(\"Asia\", \"Africa\", \"Americas\",\n                              \"Europe\", \"Oceania\")) +\n  labs(title = \"Total Population by Continent: 1952 vs 2007\",\n       y = \"Total Population (Millions)\",\n       x = \"Continent\") +\n  theme_minimal() +\n  geom_text(aes(label = total_pop),\n            vjust = 1.6,\n            color = \"white\",\n            position = position_dodge(0.9), size = 3.5)\n\n\n\n\nOk, that looks good but because the bars for Oceania are so small, we can’t see the value labels if they are placed inside the bar. So let’s place them above the bars.\n\nggplot(sumpop3,\n       aes(x = continent, y = total_pop, fill = year)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme(legend.position = \"none\") +\n  scale_x_discrete(limits = c(\"Asia\", \"Africa\", \"Americas\",\n                              \"Europe\", \"Oceania\")) +\n  labs(title = \"Total Population by Continent: 1952 vs 2007\",\n       y = \"Total Population (Millions)\",\n       x = \"Continent\") +\n  theme_minimal() +\n  geom_text(aes(label = total_pop),\n            vjust = -0.3,\n            color = \"black\",\n            position = position_dodge(0.9), size = 3.5)\n\n\n\n\n\n\n10.2.2 Histograms\nLet’s say we want to look at the distribution for a numeric variable such as the Gross Domestic Product per capita variable in the original gapminder dataframe for the year 2007.\n\n# Let's create a dataframe with GDP across all countries for the year 2007.\n\nsumpop4 &lt;- gapminder %&gt;%\n  filter(year == \"2007\") \n\n# Now let's create a histogram.\nggplot(sumpop4,\n       aes(x = gdpPercap)) +\n  geom_histogram(fill = \"darkgreen\") +\n  theme_minimal() +\n  labs(x = \"GDP Per Capita ($US)\") \n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nOk, that’s not bad. Notice how R gave us the message `stat_bin()` using `bins = 30`. Pick better value with `binwidth` ? This means that the default number of bins (or bars) for a histogram in ggplot is 30. This message shows up because we didn’t specify the exact number of bins. We can get rid of this message by specifying bins = x in the geom_histogram() layer, where x is some number of bins. We can also specify how data is covered by a single bin using the binwidth = argument.\n\n\n\n\n\n\nHow many bins? How wide a bin?\n\n\n\nWhat is the optimal number of bins you should select for a histogram? There is no universally agreed-upon standard. Generally, there is a trade-off between too much and too little detail with histograms, so people often play around by trying different numbers of bins. Some folks have specified some rules of thumb, which we might consider as well. Neither of them are perfect, but they might be a useful starting point.\n\n10.2.3 Sturge’s Rule\n\nN_{bins}= 1 + 3.322 (log_x)\n\nwhere the number of bins N_{bins} is equal to one plus 3.322 times the log of the number of observations in the data log_x. For our histogram above, we have 142 observations of countries’ GDP per capita for the year 2007 from the gapminder dataframe. Thus, the number of bins we should use according to Sturge’s Rule would be:\n\nN_{bins} = 1 + 3.322(log_{142}) \\\\\nlog(142) = 4.955827 \\\\\n3.322 * 4.955827 = 16.46326 \\\\\nN_{bins} = 1 + 16.46326 \\\\\nN_{bins} = 17.16 \\approx 17\n\n\n\n10.2.4 Freedman-Diaconis Rule\n\nbinwidth = 2 \\frac{IQR(x)}{\\sqrt[3]{n}}\n\nwhere the width of a bin binwidth is equal to 2 times the interquartile range of the variable IQR(x) divided by the cubed root of the number of observations \\sqrt[3]{n}. For our histogram above, the IQR is 16,383.99. How did I know that? I just used the function IQR() with the variable sumpop4$gdpPercap as the argument. Thus, the width of each bin we should specify according to the Freedman-Diaconis Rule would be:\n\nbinwidth = 2 \\frac{16383.99}{\\sqrt[3]{142}} \\\\\n\\sqrt[3]{142} = 5.217103 \\\\\n16383.99 / 5.217103 = 3140.438 \\\\\nbinwidth = 3140.438 * 2 = 6280.876\n\n\n\n10.2.5 Rice’s Rule\n\nN_{bins} = 2 * \\sqrt[3]{n}\n\nwhere the number of bins N_{bins} is equal to twice the cubed root of the number of observations 2 * \\sqrt[3]{n}. Thus, the number of bins for our histogram above according to Rice’s Rule would be:\n\nN_{bins} = 2 * 5.217103 = 10.43 \\approx 10\n\n\n\n\nLet’s plot and compare histograms using the three rules.\n\n# Histogram using N_bins based on Sturge's Rule.\nggplot(sumpop4,\n       aes(x = gdpPercap)) +\n  geom_histogram(fill = \"darkgreen\", bins = 17) +\n  theme_minimal() +\n  labs(x = \"GDP Per Capita ($US)\") \n\n\n\n# Histogram using bin width based on the Freedman-Diaconis Rule.\nggplot(sumpop4,\n       aes(x = gdpPercap)) +\n  geom_histogram(fill = \"darkgreen\", binwidth = 6280.876) +\n  theme_minimal() +\n  labs(x = \"GDP Per Capita ($US)\") \n\n\n\n# Histogram using N_bins based on Rice's Rule.\nggplot(sumpop4,\n       aes(x = gdpPercap)) +\n  geom_histogram(fill = \"darkgreen\", bins = 10) +\n  theme_minimal() +\n  labs(x = \"GDP Per Capita ($US)\") \n\n\n\n\nIt looks like the histograms based on Rice’s Rule and the Freedman-Diaconis Rule are fairly similar. You should try different combinations until you find one that looks right to you.\nWe can also add a vertical line in the histogram corresponding to the mean or median, which can be helpful to see the central tendency of the distribution using the geom_vline() layer.\n\nggplot(sumpop4,\n       aes(x = gdpPercap)) +\n  geom_histogram(fill = \"darkgreen\", \n                 bins = 15, \n                 color = \"white\") +\n  theme_minimal() +\n  labs(x = \"GDP Per Capita ($US)\") +\n  geom_vline(aes(xintercept = median(gdpPercap)), \n             color = \"black\",\n             linetype = \"dashed\") \n\n\n\n\n\n\n10.2.6 Scatterplots\nI quite enjoy the scatterplots produced by ggplot2 as they can be very visually appealing. As we know from Section 10.1.3, scatterplots are used to examine the relationship between two numeric variables. Let’s say I want to examine the life expectancy over time in the country of Oman from the gapminder dataframe.\n\n# First, let's create a dataframe just for Oman\nlifexpoman &lt;- gapminder %&gt;%\n  filter(country == \"Oman\") \n\n# Next, let's create a basic scatterplot.\n\nggplot(data = lifexpoman,\n       aes(x = year,\n           y = lifeExp)) +\n  geom_point() +\n  theme_minimal()\n\n\n\n\nAlright, that’s not bad. Let’s modify our axis labels, add a title, modify our axes so they show the highest values, increase the size of the points, change the shape of the points, increase the size of the points, and add a loess smoother to the data. Loess refers to locally weighted smoothing, which provides a trend line to see patterns in our data more easily. We can add one using the geom_smooth() layer.\nTo change the type of shape, recall that you can specify a shape number from the list seen in Figure 10.3. BUT, You can also use a unicode character within parentheses! This opens up a lot of possibilities, as there are about 149,186 unicode characters available currently, covering different scripts, symbols, and even emojis. Wikipedia has a nice list of unicode characters. Let’s get especially wild, and select a unicode character corresponding to the Arabic letter ح (‘Hah’), which is first letter of the Arabic word حياة (Hayat), which means Life. Given that we are examining life expectancy in Oman, where the official language is Arabic, using the first letter of this Arabic word seems appropriate.\nTo make things even more interesting, instead of a point, let’s also create a histogram where the shape corresponds to the flag of Oman. We can do this using the ggimage package’s geom_flag() function which plays well with ggplot2. In this function, there is an aes argument that takes a country’stwo-letter ISO Alpha-2 code. For Oman, the ISO code is ‘OM’.\nYou’ve already seen how to change axis labels and the title using the labs() layer. To change the y-axis of a numeric variable, we can use the scale_y_continuous() function, and to change the x-axis, we can use the scale_x_continuous() function. These two functions have four important arguments:\n\nbreaks which specifies a vector of numbers to display on the axis.\nn.breaks which specifies a number of total breaks on the axis.\nlabels which specifies a vector of labels to use on the axis.\nlimits which specifies the range of the axis.\n\nLet’s now use these arguments to modify our scatterplot.\n\n# Histogram with our changes and using a unicode symbol for the shape.\nggplot(data = lifexpoman,\n       aes(x = year,\n           y = lifeExp)) +\n  geom_point(size = 7,\n             shape = \"\\u062D\") +   # Unicode symbol for Arabic letter ح ('Hah') from Wikipedia\n  theme_minimal() +\n  labs(title = \"Oman's Increasing Life Expectancy Over Time\",  # title and axis labels\n       y = \"Life Expectancy (Years)\",\n       x = \"Year\") +\n  scale_x_continuous(breaks = seq(1952, 2007, 5)) +   # modifying x-axis\n  scale_y_continuous(limits = c(30, 80)) +            # modifying y-axis\n  geom_smooth()                                       # Loess smoother\n\n\n\n# Histogram with the flag of Oman for the shape.\nlibrary(ggimage) \n\nggplot(data = lifexpoman,\n       aes(x = year,\n           y = lifeExp)) +\n  geom_flag(aes(image = \"OM\"), size = 0.10) +       # This replaces our geom_point layer\n  theme_minimal() +\n  labs(title = \"Oman's Increasing Life Expectancy Over Time\",  # title and axis labels\n       y = \"Life Expectancy (Years)\",\n       x = \"Year\") +\n  scale_x_continuous(breaks = seq(1952, 2007, 5)) +   # modifying x-axis\n  scale_y_continuous(limits = c(30, 80)) +            # modifying y-axis\n  geom_smooth()                                       # Loess smoother\n\n\n\n\nAlright, that looks interesting! We can see a clear trend toward higher life expectancy in Oman over time. We’ve also seen how to input unicode and use a companion package to modify the shape. Let’s illustrate a final few things with the scatterplot in ggplot2.\nIf we want to examine the relationship between two numeric variables by levels of a categorical variable, we can add this as color or shape dimension. In fact, we can add two categorical variables to the scatterplot (as the shape and color dimensions), but this can be make the plot too busy or overwhelming. Let’s examine a scatterplot of life expectancy and GDP per capita among all countries and time points in the gapminder dataframe. We’ll add a color dimension to examine the relationship by continent. Let’s also specify the title for the legend using the guides() function. If we want to re-order the categories of the legend, we can simply re-order the categories of the factor variable first.\nAdditionally, we can create multiple subplots based on a categorical variable. For instance, let’s create multiple scatterplots by year to look at the relationship between GDP per capita and life expectancy, with a color dimension for continent. We can create multiple subplots using the facet_wrap() function which takes a variable name preceded by a tilda ~ as its main argument. You can also facet on more than one variable, but this can be overwhelming so we’ll stick to one variable for now. Finally, let’s use theme_bw() to see the different subplots clearly.\n\n# First, let's re-order the factor variable continent in a particular order. Here, I choose to order the levels by population size.\ngapminder$continent &lt;- factor(gapminder$continent,\n                              levels = c(\"Asia\",\n                            \"Africa\",\n                            \"Americas\",\n                            \"Europe\",\n                            \"Oceania\"))\n\n# Next, can make our scatterplot and add continent as the color dimension.\nggplot(data = gapminder,\n       aes(x = lifeExp,\n           y = gdpPercap,\n           color = continent)) +\n    geom_point(size = 2) +            # Makes the points bigger\n    theme_minimal() +\n  labs(title = \"Scatterplot of Life Expectancy and GDP Over Time\",  \n       y = \"GDP Per Capita (US $)\",\n       x = \"Life Expectancy (Years)\") +\n  scale_x_continuous(breaks = seq(20, 90, 10), # Displays ages 20 to 90 by intervals of 10.\n                     limits = c(20, 90)) +     # Bound the x-axis from 20 to 90.\n  guides(color = guide_legend(title = \"Continent\")) +  # Capitalize Continent in the legend\n  facet_wrap(~year)        # Create multiple subplots\n\n\n\n\nAlright, that doesn’t look too bad. A couple of last things though. First, notice how there’s an outlying point in the plots for 1952, 1957, 1962, 1967, 1972, and 1977? It’s an outlier in terms of GDP per capita. Let’s say we want to label that outlier on our plots. In this case, we can use the ggrepel package’s geom_text_repel() function, which allows us to use tidyverse-style commands to identify outliers based on a rule. This will look clearer once we illustrate this. Second, we can also transform the scale of our axes should we wish to do so. A common transformation is to take the base 10 logarithm (or other logarithm) of axis values. Transformation can sometimes make patterns clearer. So, let’s apply a base 10 log transformation to our y-axis using the trans = 'log10' argument within the scale_y_continuous function.\n\nlibrary(ggrepel)\n\nggplot(gapminder,\n       aes(x = lifeExp,\n           y = gdpPercap,\n           color = continent)) +\n  geom_point() +\n  theme_bw() +\n  labs(title = \"Scatterplot of Life Expectancy and GDP Over Time\",  \n       y = \"GDP Per Capita (US $)\",\n       x = \"Life Expectancy (Years)\") +\n  scale_x_continuous(breaks = seq(20, 90, 10),\n                     limits = c(20, 90)) +\n  scale_y_continuous(trans = \"log10\") +\n  facet_wrap(~year) +\n  guides(color = guide_legend(title = \"Continent\")) +\n  geom_text_repel(data = . %&gt;% \n                    filter(gdpPercap&gt; 50000), \n                  aes(label = country), \n                  size = 3.5)\n\n\n\n\nHere we see scatterplots for all the years in the dataframe where the outlier is labelled (it’s Kuwait!) thanks to ggrepel. In general, one should always aim for simplicity and clarity with such visualizations. Aggregate trends (such as combining all countries) can also mask smaller trends, so one should interpret such visualizations cautiously, and ideally paired with statistical analysis.\n\n\n10.2.7 Violin Plots\nRemember how discussed using boxplots to visualize outliers and potentially illogical values in Section 8.5 ? You might be wondering why I didn’t discuss boxplots in this section. The main reason is that boxplots can be misleading because they do not show the underlying distribution. Consider Figure 10.5 from a blog post on boxplots written by the Data Visualization Society.\n\n\n\nFigure 10.5: Boxplots give you some useful information but also conceal other important information.desbarats_ive_2021?\n\n\nThe blog post I’ve cited in the image caption is definitely worth a read, and explain other pitfalls with boxplots. However, if you would like the information provided by a boxplot AND want to know the underlying distribution, the violin plot is an ideal option. They illustrate the distribution of a numeric variable as a mirror image on either side of a line. They can also have a box plot embedded them. To accomplish this, we can use the geom_violin() and geom_boxplot functions. Let’s create a violin plot to examine the distribution of life expectancy in 1952 and 2007 across all continents, and then separately by continent.\n\nlibrary(RColorBrewer)   # Let's use some nice palettes.\n\n# First, let's create a dataframe for the years 1952 and 2007.\nlifeexp &lt;- gapminder %&gt;%\n  filter(year == \"1952\" |\n           year == \"2007\") \n\n# We need to make sure the x-axis variable is factor.\nlifeexp$year &lt;- as.factor(lifeexp$year)\n\n# Let's then create violin plots for the years 1952 and 2007.\nggplot(lifeexp,\n       aes(y = lifeExp,\n           x = year,\n           fill = year)) +\n  geom_violin(trim = FALSE) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(title = \"Life Expectancy in 1952 vs 2007\",  \n       y = \"Life Expectancy (Years)\",\n       x = \"Year\") +\n  scale_y_continuous(breaks = seq(20, 90, 10),  \n                     limits = c(20, 90))  \n\n\n\n# Then we'll do two more things - add a boxplot within the violin plot and create subplots by continent. Again, we'll use the bw theme to more easily distinguish between subplots.\nggplot(lifeexp,\n       aes(y = lifeExp,\n           x = year,\n           fill = year)) +\n  geom_violin(trim = FALSE) +\n  geom_boxplot(width = 0.1) +\n  theme_bw() +\n  theme(legend.position = \"none\") +\n  labs(title = \"Life Expectancy in 1952 vs 2007\",  \n       y = \"Life Expectancy (Years)\",\n       x = \"Year\") +\n  facet_wrap(~continent) +\n  scale_fill_brewer(palette = \"Set1\") +\n  scale_y_continuous(breaks = seq(20, 90, 10),  \n                     limits = c(20, 90))  \n\n\n\n\nThe violin plots show how the life expectancy distribution across all continents looked quite different in 1952 compared with 2007. In 1952, we can see a clear mode around 40 years, and in 2007 the mode shifted to 75. When broken down by continent, we can see a similar pattern in Asia, and different patterns in other continents.\nIn this way, the violin plot is a great tool to visualize distributions between groups, and you can think of them as the ideal complement to a box plot.\n\n\n\n\n1. Wickham H. A Layered Grammar of Graphics. Journal of Computational and Graphical Statistics. 2010;19(1):3-28. doi:10.1198/jcgs.2009.07098"
  },
  {
    "objectID": "dataviz.html#freedman-diaconis-rule",
    "href": "dataviz.html#freedman-diaconis-rule",
    "title": "10  Data Visualization with base R and ggplot2",
    "section": "10.3 Freedman-Diaconis Rule",
    "text": "10.3 Freedman-Diaconis Rule\n\nbinwidth = 2 \\frac{IQR(x)}{\\sqrt[3]{n}}\n\nwhere the width of a bin binwidth is equal to 2 times the interquartile range of the variable IQR(x) divided by the cubed root of the number of observations \\sqrt[3]{n}. For our histogram above, the IQR is 16,383.99. How did I know that? I just used the function IQR() with the variable sumpop4$gdpPercap as the argument. Thus, the width of each bin we should specify according to the Freedman-Diaconis Rule would be:\n\nbinwidth = 2 \\frac{16383.99}{\\sqrt[3]{142}} \\\\\n\\sqrt[3]{142} = 5.217103 \\\\\n16383.99 / 5.217103 = 3140.438 \\\\\nbinwidth = 3140.438 * 2 = 6280.876"
  },
  {
    "objectID": "dataviz.html#rices-rule",
    "href": "dataviz.html#rices-rule",
    "title": "10  Data Visualization with base R and ggplot2",
    "section": "10.4 Rice’s Rule",
    "text": "10.4 Rice’s Rule\n\nN_{bins} = 2 * \\sqrt[3]{n}\n\nwhere the number of bins N_{bins} is equal to twice the cubed root of the number of observations 2 * \\sqrt[3]{n}. Thus, the number of bins for our histogram above according to Rice’s Rule would be:\n\nN_{bins} = 2 * 5.217103 = 10.43 \\approx 10"
  },
  {
    "objectID": "cleaning.html#sec-illexs",
    "href": "cleaning.html#sec-illexs",
    "title": "8  Data Cleaning",
    "section": "8.5 Detecting Illogical or Extreme Values",
    "text": "8.5 Detecting Illogical or Extreme Values\nOne simple method of determining whether there exist illogical values in a numeric variable is to look at the maximum and minimum values (ensuring that they correspond to logical values for that variable) using the range() function. Similarly, boxplots boxplot() and histograms hist() are also useful. Let’s look at the built-in mtcars data frame.\n\n# Assign the mtcars dataframe to an object named df1\ndf1 &lt;- mtcars\n\n# I add a few outliers to the 'mpg' column\ndf1[c(2,5,7,9:11,22), 1] &lt;- 500\n\n# Let's examine the range of values in the 'mpg' column starting with the range.\nrange(df1$mpg)\n\n[1]  10.4 500.0\n\n\nImmediately, I see that the minimum value of 10 miles per gallon appears logical (think bigger SUVs which are less fuel-efficient than smaller sedan cars). However, the maximum value of 500 miles per gallon is not logical at all. How do I know? Well, for one I put in these entries. But if I didn’t know, a quick Google search tells me that the most fuel efficient gasoline car achieves 42 miles per gallon on the highway, and the most fuel efficient electric car achieves an 84 miles per gallon-equivalent. Thus, 500 miles per gallon is more of a distant dream rather than an accurate value.\nLet’s then visualize the variable with a histogram and a boxplot.\n\n# Next, let's look at a histogram of values\nhist(df1$mpg)\n\n\n\n# Next, let's look at a boxplot of the mpg variable.\nboxplot(df1$mpg)\n\n\n\n\nThe histogram shows us that the majority of values are less than 100, and a small subset of values lie around 500. This suggests that 500 might be an outlier value. The boxplot suggests that same, but provides us the Interquartile Range (IQR), which refers to the 25th percentile to 75th percentile of the data. The IQR is used to build box plots, which also display ‘whiskers’ or lines extending above and below the box, corresponding to 1.5 times the 75th percentile and 1.5 times the 25th percentile. If data points are outside the whiskers, these can be outliers. The boxplot above is super squished, and we can see a dot at 500. This indicates that a value of 500 might be an outlier.\nThe rstatix package also has some useful functions for detecting outliers. The identify_outliers() function in particular returns you a data frame with two new columns indicating whether a specific value is an outlier (defined in this package as 1.5 times the IQR) or an extreme value (defined in this package is 1.5 times the IQR).\n\nlibrary(rstatix)\nlibrary(dplyr)\n\ndf2 &lt;- df1 %&gt;%\n  identify_outliers(mpg)\n\n\nThe resulting data frame shows the seven unique values that are 1.5 and 3 times the IQR (see the two columns added at the end). For our purposes, this helps identify the specific outliers or extreme values. In this case, we have already determined that 500 is an illogical value for the fuel efficiency (miles per gallon) of cars. Thus, we can delete the values and replace them with NA to indicate missing values. To accomplish this, let’s use the which() function and %in% operator to identify the rows with a value of 500 for mpg from our original dataset. The which() function gives us the position or index which satisfies a given condition. Recall that the %in% operator checks whether the values in the first argument are present in the second argument, and returns a logical value. When we combine which() with the %in% operator, and add those into the first argument in square brackets after a dataframe, this will give the specific rows that satisfy a particular condition.\n\n# Identify rows with mpg == 500\ndf1[which(df1$mpg %in% 500),]\n\n                  mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4 Wag     500   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nHornet Sportabout 500   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nDuster 360        500   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 230          500   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280          500   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C         500   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nDodge Challenger  500   8 318.0 150 2.76 3.520 16.87  0  0    3    2\n\n# Replace 500 with NA\ndf1[which(df1$mpg %in% 500), 1] &lt;- NA\n\nIf we view the resulting dataframe, we can see that the mpg variable for the rows in question have all been replaced with NA to indicate missing data (indicated with red squares)."
  },
  {
    "objectID": "replication5.html",
    "href": "replication5.html",
    "title": "15  Common Problems that Hamper Reproducibility",
    "section": "",
    "text": "We’ve now seen the issues stemming from crises of reproducibility across multiple academic fields. Before we can discuss potential solutions to the many problems, let’s take stock of some common problems that hamper reproducibility in Figure 15.1.\n\n\n\nFigure 15.1: Figure taken from the report of the symposium, 'Reproducibility and reliability of biomedical research', organised by the Academy of Medical Sciences, BBSRC, MRC and Wellcome Trust in April 2015. Full report here.\n\n\nNotice how the issues are arranged in a circle. This is not a coincidence, as such researcher degrees of freedom are present at all steps in the research cycle. Consider the image in Figure 15.2, which comes from a 2017 paper in Nature.1\n\n\n\nFigure 15.2: Researcher degrees of freedom can happen at any stage of the research process.\n\n\nBesides illustrating the many possibilities for researcher degrees of freedom to introduce bias into the research process, Figure 15.2 it also points to the pernicious problem of HARKing (Hypothesizing After Results are Known), which is tantamount to circular reasoning. In practical terms, this process often looks like this:\n\nThe researcher conducts a hypothesis test based on a hunch H.\nThey don’t find what they expect, but find something statistically significant U.\nThe researcher writes up the results as H = Uusing the same data.\n\nYou may wonder, what’s the big deal? If you found something unexpected and interesting, isn’t that the point of science? To be clear, if the researcher starts with a hunch H and finds something unexpected and statistically significant U, it’s absolutely ok and even a good idea to explore U in a new study with different data. But using the same data for H and U and pretending like H never existed is a problem.\nFor one, if hypotheses that are falsified are never reported, that distorts the evidence base for what hypotheses are supported and which hypotheses are not supported. Imagine 10 different researchers all investigate the same hypothesis H, and find that the hypothesis is not supported by the data, but don’t report it, then other researchers may continue to pursue H. This is a waste of resources. Instead, if all researchers reported that H is not supported by their data, this tells us that H might not be the right avenue to pursue.\nAdditionally, if the unexpected hypothesis U is reported as the a priori hypothesis, this reduces reproducibility as U may be unique to the sample, and not generalizable. It may represent a false positive or false negative. This is especially bad if the researcher is cherry-picking their results to support unexpected finding U because it is a straight-forward distortion of their findings. Reporting only statistically significant results and concealing non-significant results leads to a distortion in the evidence base and potentially wastes resources, as described above.\n\n\n\n\n1. Munafò MR, Nosek BA, Bishop DVM, et al. A manifesto for reproducible science. Nature Human Behaviour. 2017;1(1):1-9. doi:10.1038/s41562-016-0021"
  },
  {
    "objectID": "replication5.html#summary-of-some-common-problems",
    "href": "replication5.html#summary-of-some-common-problems",
    "title": "15  Common Problems that Hamper Reproducibility",
    "section": "15.1 Summary of Some Common Problems",
    "text": "15.1 Summary of Some Common Problems\nWe’ve now seen the issues stemming from crises of reproducibility across multiple academic fields. Before we can discuss potential solutions to the many problems, let’s take stock of some common problems that hamper reproducibility in Figure 15.1.\n\n\n\nFigure 15.1: Figure taken from the report of the symposium, ‘Reproducibility and reliability of biomedical research’, organised by the Academy of Medical Sciences, BBSRC, MRC and Wellcome Trust in April 2015. Full report here.\n\n\nNotice how the issues are arranged in a circle. This is not a coincidence, as such researcher degrees of freedom are present at all steps in the research cycle. Consider the image in Figure 15.2, which comes from a 2017 paper in Nature.1\n\n\n\nFigure 15.2: Researcher degrees of freedom can happen at any stage of the research process."
  },
  {
    "objectID": "replication5.html#harking",
    "href": "replication5.html#harking",
    "title": "15  Common Problems that Hamper Reproducibility",
    "section": "15.2 HARKing",
    "text": "15.2 HARKing\nBesides illustrating the many possibilities for researcher degrees of freedom to introduce bias into the research process, Figure 15.2 it also points to the pernicious problem of HARKing (Hypothesizing After Results are Known),2 which is tantamount to circular reasoning. In practical terms, this process often looks like this:\n\nThe researcher conducts a hypothesis test based on a hunch H.\nThey don’t find what they expect, but find something statistically significant U.\nThe researcher writes up the results as H = Uusing the same data.\n\nYou may wonder, what’s the big deal? If you found something unexpected and interesting, isn’t that the point of science? To be clear, if the researcher starts with a hunch H and finds something unexpected and statistically significant U, it’s absolutely OK and even a good idea to explore U in a new study with different data. In fact, some suggest that this form of transparent HARKing (also known as THARKing) in which authors present new hypotheses derived from the data in the Discussion section of an article can be beneficial for knowledge creation.3 I agree with this position, as transparently stating your thoughts on potential new hypotheses can generate new ideas for future research, and also clearly bounds the discussion in an exploratory sense. But using the same data for H and U and pretending like H never existed is a problem.\nFor one, if hypotheses that are falsified are never reported, that distorts the evidence base for what hypotheses are supported and which hypotheses are not supported. Imagine 10 different researchers all investigate the same hypothesis H, and find that the hypothesis is not supported by the data, but don’t report it, then other researchers may continue to pursue H. This is a waste of resources. Instead, if all researchers reported that H is not supported by their data, this tells us that H might not be the right avenue to pursue.\nAdditionally, if the unexpected hypothesis U is reported as the a priori hypothesis, this reduces reproducibility as U may be unique to the sample, and not generalizable. It may represent a false positive or false negative. This is especially bad if the researcher is cherry-picking their results to support unexpected finding U because it is a straight-forward distortion of their findings. Reporting only statistically significant results and concealing non-significant results leads to a distortion in the evidence base and potentially wastes resources, as described above.\nThere are always chance variations in effect size due to sample size, the underlying odds in the effect being true before the study is even conducted, the composition of the sample, the variation in population effects, and more. Therefore, reporting U as H can be a case of a researcher being fooled by randomness, and reporting spurious effects as real ones.\n\nIt’s also worth noting that authors have since noting some variants of HARKing, which are shown in Table 15.1.4\n\n\nTable 15.1: Variations of HARKing.\n\n\n\n\n\n\n\nAcronym\nFull Form\nDescription\n\n\n\n\nCHARKing\nConstructing Hypotheses After Results are Known\nThis is like the description of HARKing above. Creating hypotheses after the fact and presenting them as having been constructed a priori.\n\n\nRHARKing\nRetrieving Hypotheses After Results are Known\nLet’s say the researchers had previously proposed some hypotheses but then dropped them. After the fact, they might revive those hypotheses and present them as having been known a priori.\n\n\nSHARKing\nSuppressing Hypotheses After Results are Known\nNot reporting a priori hypotheses that were not supported by the results. This could also be not reporting a priori hypotheses supported by results just because the researchers didn’t like these hypotheses.\n\n\n\n\nA key problem of all variations of HARKing is that they can lead to theories that are overfit by the results.4 This means that the theories can become too bound up in the results of individual sample results, and thus are less useful at more general explanations of a phenomenon when applied to other samples and contexts."
  },
  {
    "objectID": "replication5.html#publication-bias",
    "href": "replication5.html#publication-bias",
    "title": "15  Common Problems that Hamper Reproducibility",
    "section": "15.3 Publication Bias",
    "text": "15.3 Publication Bias\nYou may be wondering what motivates researcher degrees of freedom such as HARKing? One important consideration is the incentives that researchers have for engaging in researcher degrees of freedom. A key incentive is to be able to publish one’s results. In a perfect world perhaps, all results would have an equal opportunity to be published regardless of statistical significance.\n\n\n\n\n1. Munafò MR, Nosek BA, Bishop DVM, et al. A manifesto for reproducible science. Nature Human Behaviour. 2017;1(1):1-9. doi:10.1038/s41562-016-0021\n\n\n2. Kerr NL. HARKing: Hypothesizing after the results are known. Personality and social psychology review. 1998;2(3):196-217.\n\n\n3. Hollenbeck JR, Wright PM. Harking, sharking, and tharking: Making the case for post hoc analysis of scientific data. Journal of Management. 2017;43(1):5-18.\n\n\n4. Lishner DA. HARKing: Conceptualizations, harms, and two fundamental remedies. Journal of Theoretical and Philosophical Psychology. 2021;41(4):248."
  },
  {
    "objectID": "replication5.html#publication-bias-the-file-drawer-problem",
    "href": "replication5.html#publication-bias-the-file-drawer-problem",
    "title": "15  Common Problems that Hamper Reproducibility",
    "section": "15.3 Publication Bias & the File Drawer Problem",
    "text": "15.3 Publication Bias & the File Drawer Problem\n\nYou may be wondering what motivates researcher degrees of freedom such as HARKing? One important consideration is the incentives that researchers have for engaging in researcher degrees of freedom. A key incentive is to be able to publish one’s results. In a perfect world perhaps, all results would have an equal opportunity to be published regardless of statistical significance. Unfortunately, this is not the case.\nStudies with statistically significant results are more likely to be written up and published than studies with non-statistically significant results (i.e. null results as they are often called).5 67 8 Publication bias can be defined as An editorial predilection for publishing particular findings, e.g., positive results, which leads to the failure of authors to submit negative findings for publication.9 The editors of journals want to publish ground-breaking research so that the prestige, relevance, and citation frequency for their journal is increased. Sadly, null or negative results are not as attractive to publish than statistically significant results, despite being as important for providing a more complete picture of the evidence base for a given phenomenon. This can lead to substantial distortions of the evidence base.\nConsider, for example, the findings of a 2021 meta-analytic study published in the prestigious journal Proceedings of the National Academy of Sciences which suggested that nudge interventions (light-touch behavioral interventions which sway an individual toward a particular choice without restricting their ability to make a different choice) have an average effect size of d= 0.43.10 This is a pretty large effect in the social science literature! It’s also a remarkable headline-worthy finding. However, even meta-analyses can be prone to publication bias, because any evidence syntheses is only as good as the evidence it synthesizes. Thus, when another research team applied a novel technique to account for publication bias, they found that the average effect size of nudge interventions dropped to about d = 0.04 - 0.11, which the authors concluded was suggestive of no evidence of such interventions.11 While the final word on nudge interventions is far from being written, this back-and-forth shows how the severity of publication bias can lead to drastically different estimates of overall effect.\nPublication bias thus naturally leads researchers to suppress null or non-statistically significant findings. Many researchers don’t even write up their null result studies, and simply abandon them forever. This is known as the file drawer problem, where null results are relegated to a virtual or actual file drawer, never to see the light of day.12 The extreme case of the problem is that 5% of studies that have false positives (Type I errors) are published in journals, and 95% of studies with non-statistically significant results are relegated to the file drawer.12 However, consider that p-hacking increases the likelihood of false positives in the literature as well.\nWhen examining the problem empirically based on studies supported by a National Science Foundation program, a group of authors found some troubling results,7 seen in Figure 15.3.\n\n\n\nFigure 15.3: Publication bias results in fewer results seeing the light of day!\n\n\nPublication bias is such an extensive problem (and having been recognized in the literature for over 40 years), that researchers are continually developing new methods to detect publication bias in evidence syntheses.13 1415 Of course, there are other potential solutions to the problem which we’ll discuss in the next chapter, but suffice it to say, we must always be cognizant of publication bias when evaluating a body of evidence."
  },
  {
    "objectID": "replication5.html#cognitive-biases",
    "href": "replication5.html#cognitive-biases",
    "title": "15  Common Problems that Hamper Reproducibility",
    "section": "15.4 Cognitive Biases",
    "text": "15.4 Cognitive Biases\n\nYou might be thinking at this point, Why do researchers engage in researcher degrees of freedom if they know the consequences to the evidence base? That’s just the thing. They may willfully engage in unethical research practices, but they just as well be fooled by various cognitive biases to which we can all fall prey. For one, researchers have a strong incentive to publish papers, as their tenure and promotion is often heavily dependent on their publication output (hence the phrase Publish or Perish). Then given that journals are more likely to publish studies with statistically significant findings, researchers have a stake in the results of their quantitative research. Additionally, the salaries of researchers in many fields is also partly (and sometimes entirely) contingent on acquiring research funding, where a strong publication record is expected.\nThese conditions work synergistically with our inherent cognitive biases. For one, consider confirmation bias, or our tendency to look for evidence and conduct research in a way that aligns with our existing beliefs and hypotheses.16 In essence, confirmation bias pushes us to find what we want to find, rather than what is actually there. Though the term confirmation bias is relatively new, the phenomenon has been observed in research for quite some time. Consider this quote from 1869 by journalist Charles Mackay:\n\nWhen men wish to construct or support a theory, how they torture facts into their service!17\n\nWe should be clear that confirmation bias need not be a conscious, malicious choice, but rather a subconscious motive which can affect even the most principled open scientists. A key consideration is that a research question differs from a hypothesis.18 The differences lies in how variables within a study are measured and the relationships between them postulated by hypotheses. Think of the research question as the big picture investigation of the phenomenon, and the hypotheses as being specific relationships of interest. If a researcher conducts a study and does not find evidence consistent with their hypothesis, a natural question would be if they constructed (or operationalized) their conceptual constructs in the right way. They may ask, Is this a problem with the implementation, measurement, or underlying theory? This is a valid question, and there are often no clear answers. However, there are solutions we will discuss in the next chapter that can mitigate biases such as this.\nSimilar to confirmation bias is hindsight bias, or the tendency to make a prediction after the fact (a postdiction) where a person thinks they knew it all along.19 For example, a researcher makes some vague prediction in their mind before conducting a study, conducts the study, and then selects one of many outcomes as something they predicted all along. This leads to overconfidence in the obtained results, and conflates prediction with postdiction.20 This can go hand-in-hand with HARKing, whereby the researcher creates hypotheses after obtaining results, and espouses the view that they knew this would happen all along.\nAnother important cognitive bias which influences reproducibility and research transparency is apophenia, or the human tendency to see patterns in randomness. A common example of this is seeing a face among a collection of inanimate objects, or seeing animal shapes in clouds. Consider the image in Figure 15.4.\n\n\n\nFigure 15.4: The “Face on Mars” (left) photographed in 1976 by the NASA Viking 1 orbiter. On the right is the same image taken in 2001 by the NASA Mars Global Surveyor. Source: NASA/JPL.\n\n\nThe image shows the same mesa on Mars - one captured in 1976 (on the left) and another captured in 2001 (on the right). The picture on the right has an image resolution 10 times higher than the image on the left. However, when NASA released the image on the left to the public in 1976, they thought the apparent ‘face,’ resembling an ancient Egyptian planet, would drum up public interest. What they didn’t realize is that it led to a lot of speculation about ancient civilizations on Mars, and conspiracy theories that NASA was hiding the facts.21 Even after the 2001 image was released, it didn’t satisfy everyone. Some still believed that this ‘face’ is evidence of a civilization on Mars. This reflects a natural tendency to see patterns where there are none.\nWhen applied to research, apophenia can lead researchers to infer meaningful patterns based on noisy (or meaningless) data. For instance, running many hypothesis tests increases the likelihood of observing a false-positive, which researchers may perceive as a valid finding. Then, one or more cognitive biases can result in hypotheses and theories based on noise and spurious findings.\nHave you ever heard the expression, Correlation does not necessarily equal causation? This is because just because two things are statistically related or similar does not mean that there is any meaningful relationship between them. However, our tendency toward apophenia can lead us to conclude a meaningful correlation when there isn’t one. Consider the image in Figure 15.5 from the website Spurious Correlations created by Tyler Vigen. The image shows a remarkably strong (and by chance) correlation between the number of letters in the winning word of the Scripps National Spelling Bee and the number of people killed by venomous spiders each year.\n\n\n\nFigure 15.5: Apophenia leads us to infer patterns within random noise. Credit: Tylervigen.com.\n\n\nIn sum, we should be mindful of the many cognitive biases that can shape our conduct of research. While the situation may seem hopeless, know that there is hope! In the next chapter, we’ll discuss many advances in Open Science that can help mitigate these biases in the conduct of research.\n\n\n\n\n1. Munafò MR, Nosek BA, Bishop DVM, et al. A manifesto for reproducible science. Nature Human Behaviour. 2017;1(1):1-9. doi:10.1038/s41562-016-0021\n\n\n2. Kerr NL. HARKing: Hypothesizing after the results are known. Personality and social psychology review. 1998;2(3):196-217.\n\n\n3. Hollenbeck JR, Wright PM. Harking, sharking, and tharking: Making the case for post hoc analysis of scientific data. Journal of Management. 2017;43(1):5-18.\n\n\n4. Lishner DA. HARKing: Conceptualizations, harms, and two fundamental remedies. Journal of Theoretical and Philosophical Psychology. 2021;41(4):248.\n\n\n5. Møller AP, Jennions MD. Testing and adjusting for publication bias. Trends in Ecology & Evolution. 2001;16(10):580-586. doi:10.1016/S0169-5347(01)02235-2\n\n\n6. Mlinarić A, Horvat M, Šupak Smolčić V. Dealing with the positive publication bias: Why you should really publish your negative results. Biochemia Medica. 2017;27(3):447-452. doi:10.11613/BM.2017.030201\n\n\n7. Franco A, Malhotra N, Simonovits G. Publication bias in the social sciences: Unlocking the file drawer. Science. 2014;345(6203):1502-1505.\n\n\n8. Thornton A, Lee P. Publication bias in meta-analysis: Its causes and consequences. Journal of Clinical Epidemiology. 2000;53(2):207-216. doi:10.1016/S0895-4356(99)00161-4\n\n\n9. Porta M. A Dictionary of Epidemiology. Oxford university press; 2014.\n\n\n10. Mertens S, Herberz M, Hahnel UJJ, Brosch T. The effectiveness of nudging: A meta-analysis of choice architecture interventions across behavioral domains. Proceedings of the National Academy of Sciences. 2022;119(1):e2107346118. doi:10.1073/pnas.2107346118\n\n\n11. Maier M, Bartoš F, Stanley TD, Shanks DR, Harris AJL, Wagenmakers EJ. No evidence for nudging after adjusting for publication bias. Proceedings of the National Academy of Sciences. 2022;119(31):e2200300119. doi:10.1073/pnas.2200300119\n\n\n12. Rosenthal R. The file drawer problem and tolerance for null results. Psychological Bulletin. 1979;86(3):638-641. doi:10.1037/0033-2909.86.3.638\n\n\n13. Shi L, Lin L. The trim-and-fill method for publication bias: Practical guidelines and recommendations based on a large database of meta-analyses. Medicine. 2019;98(23):e15987. doi:10.1097/MD.0000000000015987\n\n\n14. Maier M, Bartoš F, Wagenmakers EJ. Robust Bayesian meta-analysis: Addressing publication bias with model-averaging. Psychological Methods. 2023;28(1):107-122. doi:10.1037/met0000405\n\n\n15. Rodgers MA, Pustejovsky JE. Evaluating meta-analytic methods to detect selective reporting in the presence of dependent effect sizes. Psychological methods. 2021;26(2):141.\n\n\n16. Nickerson RS. Confirmation bias: A ubiquitous phenomenon in many guises. Review of general psychology. 1998;2(2):175-220.\n\n\n17. Mackay C. Memoirs of Extraordinary Popular Delusions and the Madness of Crowds. George Routledge; sons; 1869.\n\n\n18. Pusztai L, Hatzis C, Andre F. Reproducibility of research and preclinical validation: Problems and solutions. Nature Reviews Clinical Oncology. 2013;10(12):720-724.\n\n\n19. Fischhoff B. Hindsight is not equal to foresight: The effect of outcome knowledge on judgment under uncertainty. Journal of Experimental Psychology: Human perception and performance. 1975;1(3):288.\n\n\n20. Nosek BA, Lindsay DS. Preregistration becoming the norm in psychological science. APS Observer. 2018;31.\n\n\n21. NASA Science. Unmasking the Face on Mars  Science Mission Directorate. Unmasking the Face on Mars. Published online May 2001. Accessed August 29, 2023. https://science.nasa.gov/science-news/science-at-nasa/2001/ast24may_1"
  },
  {
    "objectID": "replication6.html",
    "href": "replication6.html",
    "title": "17  Potential Solutions & the Way Forward",
    "section": "",
    "text": "Wise capybaras feel motivated and encouraged by advances in Open Science, and I agree with them!\n\n\nIf you’ve made it this far having read all the previous chapters, I hope you are not dismayed at the state of research transparency and reproducibility (RT2) in science. If you are, I don’t blame you. It’s easy to feel helpless at the state of research. However, there are many advances that have been made, and that will be made to improve things. Consider this, about 15 years ago, RT2 was not really a thing. Most folks had no idea about problems with replicating and reproducing results of empirical studies. Sure, some folks like John Ioannidis were already sounding the alarms for a while, but I think it’s safe to say most researchers either didn’t know or didn’t care about RT2 problems (many still don’t).\nSince that time, there have been a number of changes to the RT2 landscape, and a push toward Open Science has been increasing. Let’s review some of the potential solutions to the problems we’ve seen."
  },
  {
    "objectID": "replication6.html#a-partial-list",
    "href": "replication6.html#a-partial-list",
    "title": "17  Potential Solutions & the Way Forward",
    "section": "17.1 A Partial List",
    "text": "17.1 A Partial List\nFirst, let’s look at a table of potential solutions advanced in an excellent Nature article entitled A manifesto for reproducible science1 in Table 17.1.\n\n\nTable 17.1: A partial list of potential solutions. Adapted from Table 1 in the original article by Munafò et al.1\n\n\n\n\n\n\nPotential Solution Area\nExample(s) of Potential Solutions\n\n\n\n\nProtecting against cognitive biases\nBlinding\n\n\nImproved methodological training\nRigorous training in statistics and research methods for future researchers.\nRigorous continuing education in statistics and methods for current researchers.\n\n\nIndependent methodological support\nInvolvement of methodologists in research.\nIndependent oversight.\n\n\nCollaboration and team science\nMulti-site studies/distributed data collection.\nTeam-science consortia.\n\n\nPromoting study pre-registration\nRegistered Reports.\nOpen Science Framework use.\n\n\nImproving the quality of reporting\nUsing reporting checklists.\nProtocol checklists.\n\n\nProtecting against conflicts of interest\nDisclosure of conflicts of interest.\nExclusion/containment of financial and non-financial conflicts of interest.\n\n\nEncouraging transparency and open science\nOpen data, code, and related materials.\nPre-registration.\n\n\nDiversifying peer review\nPreprints.\nPre- and post-publication peer review such as Publons and PubMed Commons.\n\n\nRewarding open and reproducible practices\nBadges.\nTransparency and Openness Promotion Guidelines.\nFunding replication studies.\nRegistered Reports.\nOpen science practices in hiring and promotion.\n\n\n\n\nThat’s a pretty good list. It’s not complete, nor could it be, as advances are continually being made, but it’s summarizes potential solutions across a number of areas nicely. Let’s now explore some potential solutions in detail."
  },
  {
    "objectID": "replication6.html#pre-registration",
    "href": "replication6.html#pre-registration",
    "title": "17  Potential Solutions & the Way Forward",
    "section": "17.2 Pre-registration",
    "text": "17.2 Pre-registration\n\n17.2.1 General Description\nPre-registration refers to specifying your research plan in advance of your study and submitting it to a registry.2 It’s basically saying what you plan to do and how you plan to do it, and then publishing those details in the public domain before you conduct your study. In this way, pre-registration separates hypothesis-testing (confirmatory) research from hypothesis-generating (exploratory) research as shown in Figure 17.1.\n\n\n\nFigure 17.1: Pre-registration is not a straitjacket; you can still conduct exploratory analyses and try things out. It’s just that you should be clear which analyses will be confirmatory and which will be exploratory.\n\n\nAs we saw in Chapter 15, humans are prone to several cognitive biases which can affect our ability to separate a priori from a posteriori thinking, even if we do not mean to conflate the two. Pre-registration can mitigate these biases by clearly stating at the outset what analyses will be used to confirm or falsify hypotheses, and which analyses will be exploratory. Then, the reader can judge the merit of the work for themselves. Ideally before a researcher collects any data, they should pre-register their confirmatory hypotheses and related analyses in the public domain. There are several benefits of doing so in terms of reducing bias due to:\n\nSelective reporting of outcomes.\nDifferent analytic decisions that can lead to different results.\nPublication bias, because it corrects the denominator of studies and hypotheses out there.\nCognitive biases like apophenia, hindsight bias, and confirmation bias.\n\n\n\n17.2.2 Common Apprehensions\nNow, let’s say you followed my advice above and pre-registered your study in the public domain prior to data collection. Then, in the course of the research process, you discover that something you said you would do in your pre-registration isn’t working, and you have to pivot to a different strategy. Is that OK? Yes! As long as you clearly state any deviations from your pre-registered plan, it’s totally fine if you need to adjust something. It’s all about being transparent about what you did. Additionally, even if you already have a dataframe you want to analyze, you can still pre-register your analysis plan prior to analyzing the data. This is not the ideal scenario as it still leaves a researcher degree of freedom to cheat and peek at the data, but at least it shows people exactly what you did and readers can judge the veracity of the pre-registration for themselves.\nYou may wonder, Won’t someone steal my ideas if I pre-register my ideas? This is a valid concern, and though I don’t believe this happens frequently, the point is that it can happen. The solution is therefore to embargo the pre-registration, which keeps it private for a certain amount of time (up to four years on some platforms) before it is made public. This allows researchers plenty of time to conduct a pre-registered study without worrying about their ideas being stolen (also known as getting scooped). Additionally, once the researcher’s ideas are pre-registered and public, that provides a timestamp to lay claim to their ideas. That is, if someone else steals your exact set of analyses, data, and code, you have a record of having registered these in the public domain prior to their study being published. In my experience, getting your ideas stolen due to preregistration doesn’t really happen. In fact, some believe that the fear of being scooped is not a legitimate drawback against pre-registration as papers are unlikely to be rejected due to scooping alone, no two scooped studies can be the same, and if someone uses your methods on a different sample, that’s actually a good thing for science!3 Of course, if one is still afraid, the embargo period is an elegant solution.\n\n\n17.2.3 What Should Be Included & Where to Pre-register\nYou can be as detailed as you want in a pre-registration, but there are a few elements that should be described as a minimum. For instance, you should describe the hypotheses, planned analyses, and criteria for falsifying or supporting the hypotheses. In general, a reader should be able to replicate the methodology used, and the pre-registration should minimize as many researcher degrees of freedom as possible for confirmatory analyses. Personally, I treat the pre-registration as the bulk of my Methods section that will eventually end up in the published manuscript (save any deviations from the pre-registration which I include later in the final manuscript). This includes information about the variables being measured, a detailed analysis plan, and several back-up plans if what I want to do fails for some reason. If possible, the actual code that will be used to conduct the analyses should be included as well.\nThe pre-registration platform aspredicted.org provides perhaps the simplest template for create a pre-registration, wherein researchers need to answer the following eight questions:\n\nHave any data been collected for this study already?\nWhat’s the main question being asked or hypothesis being tested in this study?\nDescribe the key dependent variable(s) specifying how they will be measured.\nHow many and which conditions will participants be assigned to?\nSpecify exactly which analyses you will conduct to examine the main question/hypothesis.\nAny secondary analyses?\nHow many observations will be collected or what will determine sample size?\nAnything else you would like to pre-register (e.g., data exclusions, variables collected for exploratory purposes, unusual analyses planned)?\n\nMy favorite platform for pre-registration and posting all research materials in the public domain is the Open Science Framework(OSF). You can pre-register any study on the OSF, and it’s a great place to also put all your de-identified data, code, and materials used to generate your analyses. Besides having an excellent repository of study materials and pre-registrations, the site also provides several resources for learning more about pre-registration and Open Science. It also has an embargo period of four years should you wish to maintain privacy of the pre-registration for a certain period of time.\nThere are also study design-specific registries. For instance, the American Economic Association has a registry for Randomized Controlled Trials only. The PROSPERO registry hosted by York University in the UK is THE go-to place to register systematic reviews, meta-analyses, rapid reviews, and umbrella reviews. Clinical trials in the US have to registered on clinicaltrials.gov, but this registry is also applicable to any social science randomized controlled trial. The International Initiative for Impact Evaluation (3ie) hosts the Registry for International Development Impact Evaluations. Thus, there are many places to pre-register a study! If you’re at a loss for where to register, I suggest trying the Open Science Framework.\n\n\n\n\n1. Munafò MR, Nosek BA, Bishop DVM, et al. A manifesto for reproducible science. Nature Human Behaviour. 2017;1(1):1-9. doi:10.1038/s41562-016-0021\n\n\n2. Center for Open Science. Preregistration. Accessed August 29, 2023. https://www.cos.io/initiatives/prereg\n\n\n3. Schwarzkopf S. It’s not the end of the world if your research gets “scooped.” Times Higher Education (THE). Published online April 2016. Accessed August 29, 2023. https://www.timeshighereducation.com/blog/its-not-end-world-if-your-research-gets-scooped"
  }
]